<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>triton + chatglm4 部署 - zeon&#039;s blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="zeon&#039;s blog"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="zeon&#039;s blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="简单介绍triton的推理架构由两部分组成，一部分由client，一部分由k8s cluster组成。"><meta property="og:type" content="blog"><meta property="og:title" content="triton + chatglm4 部署"><meta property="og:url" content="http://example.com/2024/06/17/ai%E5%B7%A5%E7%A8%8B/triton%20+%20chatglm4%20(finish)/"><meta property="og:site_name" content="zeon&#039;s blog"><meta property="og:description" content="简单介绍triton的推理架构由两部分组成，一部分由client，一部分由k8s cluster组成。"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:published_time" content="2024-06-17T15:22:02.589Z"><meta property="article:modified_time" content="2024-04-18T16:00:00.000Z"><meta property="article:author" content="Sam"><meta property="article:tag" content="AI(人工智能)"><meta property="article:tag" content="Triton"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://example.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com/2024/06/17/ai%E5%B7%A5%E7%A8%8B/triton%20+%20chatglm4%20(finish)/"},"headline":"triton + chatglm4 部署","image":["http://example.com/img/og_image.png"],"datePublished":"2024-06-17T15:22:02.589Z","dateModified":"2024-04-18T16:00:00.000Z","author":{"@type":"Person","name":"Sam"},"publisher":{"@type":"Organization","name":"zeon's blog","logo":{"@type":"ImageObject","url":"http://example.com/img/mylogo.png"}},"description":"简单介绍triton的推理架构由两部分组成，一部分由client，一部分由k8s cluster组成。"}</script><link rel="canonical" href="http://example.com/2024/06/17/ai%E5%B7%A5%E7%A8%8B/triton%20+%20chatglm4%20(finish)/"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/mylogo.png" alt="zeon&#039;s blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-06-17T15:22:02.589Z" title="6/17/2024, 11:22:02 PM">2024-06-17</time></span><span class="level-item">40 minutes read (About 5930 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">triton + chatglm4 部署</h1><div class="content"><h2 id="简单介绍"><a href="#简单介绍" class="headerlink" title="简单介绍"></a>简单介绍</h2><p>triton的推理架构由两部分组成，一部分由client，一部分由k8s cluster组成。</p>
<span id="more"></span>

<ul>
<li>集群：总体上是一个k8s集群，要不然启动，生命周期维护，弹性扩容，需要一个管理系统会自动化运维，编排。</li>
<li>前端均衡器：前排有Load Balancer负载均衡器，把用户请求分配各个推理容器。推理容器会启动多个，如何分担推理容器。</li>
<li>模型仓库：把模型文件管理起来。</li>
<li>指标监控系统：运维系统正常监控都有。</li>
<li>triton推理服务: 启动多个节点，多个模型框架的支持，因为有的模型onnx，有的pytorch，有的tensorflow，有的tensorRT，<br>资料：</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/tutorials">https://github.com/triton-inference-server/tutorials</a></p>
<h3 id="trinton的层数"><a href="#trinton的层数" class="headerlink" title="trinton的层数"></a>trinton的层数</h3><p>k8s层，管理多个节点</p>
<p>一个triton节点都是一个模型（一个gpu，或多个gpu）</p>
<p>tensorRt（对NN模型的一个加速库）</p>
<h3 id="trinton具备的功能"><a href="#trinton具备的功能" class="headerlink" title="trinton具备的功能"></a>trinton具备的功能</h3><ul>
<li>要支持多模型的框架（tensorflow， pytorch主流，tensorRT，ONNX RT，自定义）</li>
<li>异构支持，cpu，gpu，多gpu</li>
<li>并发模型执行（cpu级别的优化）</li>
<li>支持http，rest，grpc(http2)，apis</li>
<li>k8s集群的融入和指标监控系统</li>
<li>模型的管理加载更新</li>
<li>重点就是推理服务队列的分发</li>
</ul>
<h3 id="如何设计triton？"><a href="#如何设计triton？" class="headerlink" title="如何设计triton？"></a>如何设计triton？</h3><ul>
<li>（从推理框架解耦）推理从请求到结束的生命周期，当收到一个调用qwen的一个请求，如何调用？比如说写了些python代码（pytorch）去实现推理，但是推理代码到底是由人员开发，还是交给pytorch本身？所以需要做的是要让它和推理框架解耦化。</li>
<li>（后端管理）还有常见的后端代理管理。</li>
<li>（并发）模型的并发支持，多线程。</li>
<li>（队列）请求队列的调度管理</li>
<li>（回复管理）推理结束不单是简单的转发，信息整合回复管理，也就是推理结果管理。</li>
<li>（GPRC &amp; HTTP）GPRC和HTTP服务<br>单发模型场景：据个例子如cv，输入一个图像返回一个字符串</li>
</ul>
<p>piepline场景：就是过程涉及多个模型的串联</p>
<p>状态模型场景 ：比如llm以及其上下文</p>
<h3 id="并发场景"><a href="#并发场景" class="headerlink" title="并发场景"></a>并发场景</h3><p>最常见的并发场景1：单一模型多个线程进行推理。</p>
<p>最常见的并发场景2：多模型多线程。</p>
<p>主要triton抓住了3种模型特征，无状态模型，有状态的模型，和集成模型。</p>
<p>几个组件介绍</p>
<ul>
<li>dynamic batching scheduler：先打个batch，再送去dynamic batcher里去，把requests进行grouping到一个batch，提升gpu的吞吐量。</li>
<li>streaming inferece request：进来一个片段就要处理，语音片段，同一语音片段会进去同一个线程的batch。</li>
<li>后端解耦：backend api需要和triton进行解耦，需要些custom backend 也就是C API，同时利用到了dynamic batcher之类的。</li>
<li>model analyzer: triton一个附加功能，是一个client，对推理请求扫描，扫描延迟和吞吐量，扫描gpu的显存footprints。</li>
</ul>
<h2 id="基本5个模块（一个简单的例子）"><a href="#基本5个模块（一个简单的例子）" class="headerlink" title="基本5个模块（一个简单的例子）"></a>基本5个模块（一个简单的例子）</h2><p>最基本的5个模块：</p>
<ul>
<li>model repository</li>
<li>配置served model</li>
<li>登录triton server</li>
<li>配置集成模型</li>
<li>发送request去triton server</li>
</ul>
<h3 id="目录结构"><a href="#目录结构" class="headerlink" title="目录结构"></a>目录结构</h3><p>在model_repository目录下装4个二级目录（densenet_onnx, inception_graphdef, resenet50）</p>
<p>第三级目录比如，在desenet_onnx目录下放版本号，放config.pbtxt配置，可能还有带有label.txt。</p>
<h3 id="文件格式"><a href="#文件格式" class="headerlink" title="文件格式"></a>文件格式</h3><p>版本目录下可以放很多种格式</p>
<p>model.py （python)</p>
<p>dali (model.dali)</p>
<p>openvino (model.xml, model.bin)</p>
<p>custom (model.so)</p>
<p>torch（model.pt)</p>
<p>onnx (model.onnx)</p>
<p>tensorRT (model.plan)</p>
<p>模型版本和版本目录名一致，推理指定版本号找到模型文件。</p>
<h3 id="config文件的作用"><a href="#config文件的作用" class="headerlink" title="config文件的作用"></a>config文件的作用</h3><p>config定义了scheduling策略，batchsize之类的，input，output。还有另外的一个label.txt, 是对于分类模型，用来直接转化为字符串。</p>
<h3 id="启动命令nvi"><a href="#启动命令nvi" class="headerlink" title="启动命令nvi"></a>启动命令nvi</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tritonserver --model-repository=/triton/models_repository</span><br></pre></td></tr></table></figure>
<p>不想要编译triton，直接下载triton的镜像，在镜像里去运行。<br>启动后triton模型库加载成功，看到一个model ready，甚至看到了监听0.0.0.0: 8000端口。</p>
<p>使用 python image_client.py -m resnet50_trt -c 1 -b 1 -a  ..&#x2F;pics&#x2F; 这样执行推理。</p>
<h3 id="config文件编写"><a href="#config文件编写" class="headerlink" title="config文件编写"></a>config文件编写</h3><ul>
<li>指定模型在平台和backend选择。</li>
<li>还有max_batch_size指定模型推理的batch_size, 别超过gpu显存。</li>
<li>输入tensor叫啥，输出tensor叫啥。<br>在server tensorRT，onnx，tensorflow不需要指定config文件，但是pytorch必须指定config文件。对于platform和backend如何填写。</li>
</ul>
<h4 id="指定platform和backend"><a href="#指定platform和backend" class="headerlink" title="指定platform和backend"></a>指定platform和backend</h4><p>tensorTR可以指定tensorrt_plan,或者tensorrt。对于 pytorch指定pytorch_libtorch或者pytorch。</p>
<p>20.1.05必须指定backend是<backend_name></p>
<h4 id="指定input-output"><a href="#指定input-output" class="headerlink" title="指定input&amp;output"></a>指定input&amp;output</h4><p>需要指定input的name,data_type,dims, pytroch必须INPUT__0等等。如果支持可变维度，可以把-1设置到可变维度那一栏。max_batch_size为0，这个时候可以使用reshape{shape:[1,3,224,244]}。 </p>
<h4 id="指定policy"><a href="#指定policy" class="headerlink" title="指定policy"></a>指定policy</h4><p>也可以指定version_policy: {all{}} 所有版本都serve</p>
<p>可以指定version_policy: latest { num_version: {}}, 指定最新的version</p>
<p>也可以指定特定的版本</p>
<h4 id="指定instance-group"><a href="#指定instance-group" class="headerlink" title="指定instance_group"></a>指定instance_group</h4><p>这个指定是利用了对同一个模型开启多个instance_excution，并行执行提高模型吞吐。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">instance_group [</span><br><span class="line">&#123; </span><br><span class="line">count:2</span><br><span class="line">kind: KIND_CPU</span><br><span class="line">&#125;&#123;</span><br><span class="line">count:1</span><br><span class="line">kind:KIND_GPU</span><br><span class="line">gpus:[0]</span><br><span class="line">&#125;&#123;</span><br><span class="line">count:2</span><br><span class="line">kind:KIND_GPU</span><br><span class="line">gpus:[1, 2]   //指定的使用gpu</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="指定策略"><a href="#指定策略" class="headerlink" title="指定策略"></a>指定策略</h4><p>如果不在config文件里面配策略，不写dynamic_batching，默认使用default scheduling。</p>
<p>默认的batch_size是多少就是多少。</p>
<p>仅对stateless，和streaming的模型，是没有用的，对于dynamic batcher策略。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dynamic_batching &#123;</span><br><span class="line">   preferred_batch_size:[4,8]</span><br><span class="line">   max_queue_delay_microseconds:100</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>拼的batch_size越大，吞吐量越高，但是delay时间越长。<br>server端对于比较小的请求，拼接成为比较大的batch去推理。</p>
<p>dynamic参数可以设置。preserve_ordering(先进先出），priority_levels(batch的优先级)，queuePolicy队列长度（过长的队列会丢弃请求）</p>
<ul>
<li>对于sequence Batcher，是针对stateful Model, 就在其他资料上说。</li>
<li>对于emsemble batcher，也是同上。<br>还可以设置热身策略, 对每个模型的instance进行热身，进入ready模式。</li>
</ul>
<h2 id="triton-server"><a href="#triton-server" class="headerlink" title="triton server"></a>triton server</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker run --gpus all -it --rm \</span><br><span class="line">--shm-size=1g \</span><br><span class="line">-p8000:8000 -p8001:8001 -p8002:8002 \ (8000 http, 8001 grpc, 8002 metric)</span><br><span class="line">-v &lt;host_model_repo&gt;:&lt;container_model_repo&gt; \</span><br><span class="line">nvcr.io/nvidia/tritonserver:21.07-py3</span><br><span class="line"></span><br><span class="line">$ tritonserver --model-repository &lt;repo&gt;</span><br></pre></td></tr></table></figure>
<p>启动server</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -v &lt;ip&gt;:8000/v2/health/ready</span><br></pre></td></tr></table></figure>
<p>检查server是否ok<br>常用的triton server的参数：</p>
<p>–log-verbose 0&#x2F;1 控制日志</p>
<p>–strict-model-config true&#x2F;false  控制模型的配置文件是否需要</p>
<p>–strict-readiness <boolean>  模型库全部模型上线才ready</p>
<p>–exit-on-error <boolean>   只要有一个模型load失败就fail</p>
<p>–http&#x2F;grpc&#x2F;metrics&#x2F;-port <int> 指定三个重要端口</p>
<p>–model-control-model <string> none&#x2F;poll&#x2F;explicit 可以手动&#x2F;自动&#x2F;卸载更新模型</p>
<p>–repository-poll-secs <int> 检查模型是否更新频率</p>
<p>–load-model <string> 可以指定加载模型卸载模型</p>
<p>–pinned-memory-pool-byte-size <int> 是cpu效率关键</p>
<p>–cuda-memory-pool-byte-size <int> gpu显存设置</p>
<p>–backend-diretory <str> 指定自己动态so库</p>
<p>–repoagent-directory <str> 加密时有用</p>
<h2 id="send-request-to-Triton-Server"><a href="#send-request-to-Triton-Server" class="headerlink" title="send request to Triton Server"></a>send request to Triton Server</h2><p>grpc和http的协议代码区别很小，以grpc为例子。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import tritonclient.http as httpclient</span><br><span class="line">import tritonclient.grpc as grpcclient</span><br><span class="line">//如果使用grpc</span><br><span class="line">triton_client = grpcclient.inferenceServerClient(url=FLAGS.url, verbose=FLAGS.verbose, concurrency=concurrency)</span><br><span class="line">//如果使用http</span><br><span class="line">triton_client = httpclient.inferenceServerClient(url=FLAGS.url, verbose=FLAGS.verbose, concurrency=concurrency)</span><br></pre></td></tr></table></figure>
<p>获取meta_data，进而获取所有的config，进而继续获得各种参数如<br>max_batch_size, input_name, output_name, c, h, w, format, dtype等等</p>
<h2 id="框架backend设计理论"><a href="#框架backend设计理论" class="headerlink" title="框架backend设计理论"></a>框架backend设计理论</h2><p>简单的说，tirton的自定义backend，在目录下实现一个后端文件，那么triton会自动创建triton inference 实例。除了初始化，执行，结束需要用户手动实现。里面的创建该实例，都是由triton去负责的。这是为了适配各种各样的推理后端框架 pytorch, tensorflow等等。</p>
<h2 id="实践-Triton-chatglm4-vllm-单机模式"><a href="#实践-Triton-chatglm4-vllm-单机模式" class="headerlink" title="(实践) Triton + chatglm4 + vllm 单机模式"></a>(实践) Triton + chatglm4 + vllm 单机模式</h2><h3 id="triton-server-docker"><a href="#triton-server-docker" class="headerlink" title="triton server docker"></a>triton server docker</h3><p>第一步当然是先docker pull triton的镜像。</p>
<p>使用服务器拉取镜像在封闭受控的环境是非常痛苦的，尤其是服务器上存在各种各样的容器的情况下。</p>
<p>配置docker参考这篇:<a target="_blank" rel="noopener" href="https://zeonzhuang.com/2024/05/14/ai%E5%B7%A5%E7%A8%8B/dify/#more">https://zeonzhuang.com/2024/05/14/ai%E5%B7%A5%E7%A8%8B/dify/#more</a></p>
<p>这里存放了各种triton server的镜像文件。</p>
<p><a target="_blank" rel="noopener" href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver">https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver</a></p>
<p>xx.yy-vllm-python-py3 镜像是有triton的同时，支持python框架后端+vllm，这个是我所青睐的</p>
<p>执行命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull nvcr.io/nvidia/tritonserver:24.05-vllm-python-py3</span><br></pre></td></tr></table></figure>

<p>完成后，可以进入镜像，执行命令测试triton</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tritonserver --help</span><br></pre></td></tr></table></figure>

<p>正常的方式是使用正确的命令进去镜像</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">docker run -itd （交互，守护模式）</span><br><span class="line">--name chatglmtest （指定名称）</span><br><span class="line">--gpus all （指定gpu）</span><br><span class="line">--shm-size=1g （共享内存）</span><br><span class="line">--ulimit memlock=-1 （接触内存锁限制）</span><br><span class="line">-p 8000:8000 -p 8001:8001 -p 8002:8002 （映射端口）</span><br><span class="line"> --net=host （主机网络共享）</span><br><span class="line">-v /home/server/model_repository:/models （映射模型文件） </span><br><span class="line">--ulimit stack=67108864 （指定堆栈区大小）</span><br><span class="line">nvcr.io/nvidia/tritonserver:23.12-py3 （运行指定镜像）</span><br><span class="line"></span><br><span class="line">//进行了些修改，由于我运行的是triton+vllm_chatglm4修改了名称</span><br><span class="line">//由于大量8开头端口占用了，所以该了映射为4开头，但是这里偷懒用了主机共享网络不需要映射端口</span><br><span class="line">//对于目录映射，是triton的重头戏，直接映射triton目录</span><br><span class="line">docker run -itd --name triton_vllm_chatglm4 --gpus all --shm-size=1g --ulimit memlock=-1 -p 8000:4000 -p 8001:4001 -p 8002:4002 --net=host -v /home/zzh/triton:/triton --ulimit stack=67108864 nvcr.io/nvidia/tritonserver:24.05-vllm-python-py3</span><br></pre></td></tr></table></figure>

<h3 id="目录结构-1"><a href="#目录结构-1" class="headerlink" title="目录结构"></a>目录结构</h3><p>对于目录映射这里提一下，triton运行时，需要指定model_repository</p>
<ul>
<li>一级目录（server目录）：这个models_repository下面会有数个模型目录，qwen2，chatglm4，llama3等等</li>
<li>二级目录（单个模型目录）：这个目录自由命名，如chatglm4就这样命名</li>
<li>三级目录  (版本目录，config文件）</li>
<li>四级目录（版本目录下的模型权重目录，work目录下的模型推理文件目录, model.py自定义推理框架python后端）<br>根据以上的目录架构理论，我在本地构建了一个</li>
</ul>
<p>triton目录</p>
<p>-&gt;model_repository</p>
<p>-&gt;chatglm4</p>
<p>-&gt;1</p>
<p> -&gt; glm-4-9b-chat</p>
<p> -&gt; work</p>
<p> -&gt; model.py (后端文件, 使用vllm）</p>
<p>-&gt;config.pbtxt</p>
<p>进入成功以后，测试tritonserver成功以后，开始对model.py进行修改。</p>
<h3 id="model推理python后端"><a href="#model推理python后端" class="headerlink" title="model推理python后端"></a>model推理python后端</h3><p>对于后端推理，为了使用triton，需要完成TritonPythonModel类的开发。该类对象不是由使用者创建，但是初始化，执行，结束都是由使用者编写。</p>
<h4 id="参考对比"><a href="#参考对比" class="headerlink" title="参考对比"></a>参考对比</h4><p>这个函数只有模型被load到显存的时候，才会被调用一次，主要用于读取相关启动参数。</p>
<p>可以通过观察2个不同的模型推理文件来进行对比。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/THUDM/GLM-4">https://github.com/THUDM/GLM-4</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">from transformers import AutoTokenizer</span><br><span class="line">from vllm import LLM, SamplingParams </span><br><span class="line"></span><br><span class="line"># GLM-4-9B-Chat-1M</span><br><span class="line"># max_model_len, tp_size = 1048576, 4</span><br><span class="line"># 如果遇见 OOM 现象，建议减少max_model_len，或者增加tp_size</span><br><span class="line"></span><br><span class="line">max_model_len, tp_size = 131072, 1</span><br><span class="line">model_name = &quot;THUDM/glm-4-9b-chat&quot;</span><br><span class="line">prompt = [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你好&quot;&#125;]</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">llm = LLM(</span><br><span class="line">    model=model_name,</span><br><span class="line">    tensor_parallel_size=tp_size,</span><br><span class="line">    max_model_len=max_model_len,</span><br><span class="line">    trust_remote_code=True,</span><br><span class="line">    enforce_eager=True,</span><br><span class="line">    # GLM-4-9B-Chat-1M 如果遇见 OOM 现象，建议开启下述参数</span><br><span class="line">    # enable_chunked_prefill=True,</span><br><span class="line">    # max_num_batched_tokens=8192</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">stop_token_ids = [151329, 151336, 151338]</span><br><span class="line">sampling_params = SamplingParams(temperature=0.95, max_tokens=1024, stop_token_ids=stop_token_ids)</span><br><span class="line">inputs = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)</span><br><span class="line">outputs = llm.generate(prompts=inputs, sampling_params=sampling_params)</span><br><span class="line">print(outputs[0].outputs[0].text)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>也就是vllm封装了LLM类，和SamplingParams类。</p>
<ul>
<li>tokenizer依旧是使用hugging face的库，autoTokenizer去初始化一个tokenizer，然后prompt作为一个数组（元素是dict），会被tokenizer使用apply_chat_template转化成模型可以接受的input。</li>
<li>而这个LLM类会接model_name（仓名或者是本地路径名），ternsor_parallel_size, max_model_len，trust_remote_code, enforce_eager,  创建出一个llm对象，该对象有一个generate函数，就是执行推理的地方，但是该generate函数会接一个prompts，和一个sampling_params作为入参，这个sampling_params就是包含温度，max_tokens, stop_token_ids，所有的heavy job都被vllm给承包了。<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/cm2010_03_31/article/details/135986638">https://blog.csdn.net/cm2010_03_31&#x2F;article&#x2F;details&#x2F;135986638</a></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line"># 设置显存空闲block最大分割阈值</span><br><span class="line">os.environ[&#x27;PYTORCH_CUDA_ALLOC_CONF&#x27;] = &#x27;max_split_size_mb:32&#x27;</span><br><span class="line"># 设置work目录</span><br><span class="line"> </span><br><span class="line">os.environ[&#x27;TRANSFORMERS_CACHE&#x27;] = os.path.dirname(os.path.abspath(__file__))+&quot;/work/&quot;</span><br><span class="line">os.environ[&#x27;HF_MODULES_CACHE&#x27;] = os.path.dirname(os.path.abspath(__file__))+&quot;/work/&quot;</span><br><span class="line"> </span><br><span class="line">import json</span><br><span class="line"> </span><br><span class="line"># triton_python_backend_utils is available in every Triton Python model. You</span><br><span class="line"># need to use this module to create inference requests and responses. It also</span><br><span class="line"># contains some utility functions for extracting information from model_config</span><br><span class="line"># and converting Triton input/output types to numpy types.</span><br><span class="line">import triton_python_backend_utils as pb_utils</span><br><span class="line">import sys</span><br><span class="line">import gc</span><br><span class="line">import time</span><br><span class="line">import logging</span><br><span class="line">import torch</span><br><span class="line">from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM</span><br><span class="line">import numpy as np</span><br><span class="line"> </span><br><span class="line">gc.collect()</span><br><span class="line">torch.cuda.empty_cache()</span><br><span class="line"> </span><br><span class="line">logging.basicConfig(format=&#x27;%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s&#x27;,</span><br><span class="line">                    level=logging.INFO)</span><br><span class="line"> </span><br><span class="line">class TritonPythonModel:</span><br><span class="line">    &quot;&quot;&quot;Your Python model must use the same class name. Every Python model</span><br><span class="line">    that is created must have &quot;TritonPythonModel&quot; as the class name.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"> </span><br><span class="line">    def initialize(self, args):</span><br><span class="line">        &quot;&quot;&quot;`initialize` is called only once when the model is being loaded.</span><br><span class="line">        Implementing `initialize` function is optional. This function allows</span><br><span class="line">        the model to intialize any state associated with this model.</span><br><span class="line">        Parameters</span><br><span class="line">        ----------</span><br><span class="line">        args : dict</span><br><span class="line">          Both keys and values are strings. The dictionary keys and values are:</span><br><span class="line">          * model_config: A JSON string containing the model configuration</span><br><span class="line">          * model_instance_kind: A string containing model instance kind</span><br><span class="line">          * model_instance_device_id: A string containing model instance device ID</span><br><span class="line">          * model_repository: Model repository path</span><br><span class="line">          * model_version: Model version</span><br><span class="line">          * model_name: Model name</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # You must parse model_config. JSON string is not parsed here</span><br><span class="line">        self.model_config = json.loads(args[&#x27;model_config&#x27;])</span><br><span class="line">        </span><br><span class="line">        output_response_config = pb_utils.get_output_config_by_name(self.model_config, &quot;response&quot;)</span><br><span class="line">        output_history_config = pb_utils.get_output_config_by_name(self.model_config, &quot;history&quot;)</span><br><span class="line"> </span><br><span class="line">        # Convert Triton types to numpy types</span><br><span class="line">        self.output_response_dtype = pb_utils.triton_string_to_numpy(output_response_config[&#x27;data_type&#x27;])</span><br><span class="line">        self.output_history_dtype = pb_utils.triton_string_to_numpy(output_history_config[&#x27;data_type&#x27;])</span><br><span class="line">        </span><br><span class="line">        ChatGLM_path = os.path.dirname(os.path.abspath(__file__))+&quot;/chatglm3-6b-32k&quot;</span><br><span class="line">        self.tokenizer = AutoTokenizer.from_pretrained(ChatGLM_path, trust_remote_code=True)</span><br><span class="line">        #下面to(&#x27;cuda:&#x27;+args[&#x27;model_instance_device_id&#x27;])这里一定要注意，这里是把实例部署到对应的显卡上，如果不写会分散到所有显卡上或者集中到一个显卡上，都会造成问题</span><br><span class="line">        model = AutoModelForCausalLM.from_pretrained(ChatGLM_path,</span><br><span class="line">                                          torch_dtype=torch.float16,               trust_remote_code=True).half().to(&#x27;cuda:&#x27;+args[&#x27;model_instance_device_id&#x27;])</span><br><span class="line">        self.model = model.eval()</span><br><span class="line">        logging.info(&quot;model init success&quot;)</span><br><span class="line">        </span><br><span class="line">    def execute(self, requests):</span><br><span class="line">        &quot;&quot;&quot;`execute` MUST be implemented in every Python model. `execute`</span><br><span class="line">        function receives a list of pb_utils.InferenceRequest as the only</span><br><span class="line">        argument. This function is called when an inference request is made</span><br><span class="line">        for this model. Depending on the batching configuration (e.g. Dynamic</span><br><span class="line">        Batching) used, `requests` may contain multiple requests. Every</span><br><span class="line">        Python model, must create one pb_utils.InferenceResponse for every</span><br><span class="line">        pb_utils.InferenceRequest in `requests`. If there is an error, you can</span><br><span class="line">        set the error argument when creating a pb_utils.InferenceResponse</span><br><span class="line">        Parameters</span><br><span class="line">        ----------</span><br><span class="line">        requests : list</span><br><span class="line">          A list of pb_utils.InferenceRequest</span><br><span class="line">        Returns</span><br><span class="line">        -------</span><br><span class="line">        list</span><br><span class="line">          A list of pb_utils.InferenceResponse. The length of this list must</span><br><span class="line">          be the same as `requests`</span><br><span class="line">          </span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        output_response_dtype = self.output_response_dtype</span><br><span class="line">        output_history_dtype = self.output_history_dtype</span><br><span class="line"> </span><br><span class="line">        # output_dtype = self.output_dtype</span><br><span class="line">        responses = []</span><br><span class="line">        # Every Python backend must iterate over everyone of the requests</span><br><span class="line">        # and create a pb_utils.InferenceResponse for each of them.</span><br><span class="line">        for request in requests:</span><br><span class="line">            prompt = pb_utils.get_input_tensor_by_name(request, &quot;prompt&quot;).as_numpy()[0]</span><br><span class="line">            prompt = prompt.decode(&#x27;utf-8&#x27;)</span><br><span class="line">            history_origin = pb_utils.get_input_tensor_by_name(request, &quot;history&quot;).as_numpy()</span><br><span class="line">            if len(history_origin) &gt; 0:</span><br><span class="line">                history = np.array([item.decode(&#x27;utf-8&#x27;) for item in history_origin]).reshape((-1,2)).tolist()</span><br><span class="line">            else:</span><br><span class="line">                history = []</span><br><span class="line">            temperature = pb_utils.get_input_tensor_by_name(request, &quot;temperature&quot;).as_numpy()[0]</span><br><span class="line">            temperature = float(temperature.decode(&#x27;utf-8&#x27;))</span><br><span class="line">            max_token = pb_utils.get_input_tensor_by_name(request, &quot;max_token&quot;).as_numpy()[0]</span><br><span class="line">            max_token = int(max_token.decode(&#x27;utf-8&#x27;))</span><br><span class="line">            history_len = pb_utils.get_input_tensor_by_name(request, &quot;history_len&quot;).as_numpy()[0]</span><br><span class="line">            history_len = int(history_len.decode(&#x27;utf-8&#x27;))</span><br><span class="line">            </span><br><span class="line">            # 日志输出传入信息</span><br><span class="line">            in_log_info = &#123;</span><br><span class="line">                &quot;in_prompt&quot;:prompt,</span><br><span class="line">                &quot;in_history&quot;:history,</span><br><span class="line">                &quot;in_temperature&quot;:temperature,</span><br><span class="line">                &quot;in_max_token&quot;:max_token,</span><br><span class="line">                &quot;in_history_len&quot;:history_len</span><br><span class="line">                       &#125;</span><br><span class="line">            logging.info(in_log_info)</span><br><span class="line">            response,history = self.model.chat(self.tokenizer,</span><br><span class="line">                                               prompt,</span><br><span class="line">                                               history=history[-history_len:] if history_len &gt; 0 else [],</span><br><span class="line">                                               max_length=max_token,</span><br><span class="line">                                               temperature=temperature)</span><br><span class="line">            # 日志输出处理后的信息</span><br><span class="line">            out_log_info = &#123;</span><br><span class="line">                &quot;out_response&quot;:response,</span><br><span class="line">                &quot;out_history&quot;:history</span><br><span class="line">                       &#125;</span><br><span class="line">            logging.info(out_log_info)</span><br><span class="line">            response = np.array(response)</span><br><span class="line">            history = np.array(history)</span><br><span class="line">            </span><br><span class="line">            response_output_tensor = pb_utils.Tensor(&quot;response&quot;,response.astype(self.output_response_dtype))</span><br><span class="line">            history_output_tensor = pb_utils.Tensor(&quot;history&quot;,history.astype(self.output_history_dtype))</span><br><span class="line"> </span><br><span class="line">            final_inference_response = pb_utils.InferenceResponse(output_tensors=[response_output_tensor,history_output_tensor])</span><br><span class="line">            responses.append(final_inference_response)</span><br><span class="line">            # Create InferenceResponse. You can set an error here in case</span><br><span class="line">            # there was a problem with handling this inference request.</span><br><span class="line">            # Below is an example of how you can set errors in inference</span><br><span class="line">            # response:</span><br><span class="line">            #</span><br><span class="line">            # pb_utils.InferenceResponse(</span><br><span class="line">            #    output_tensors=..., TritonError(&quot;An error occured&quot;))</span><br><span class="line"> </span><br><span class="line">        # You should return a list of pb_utils.InferenceResponse. Length</span><br><span class="line">        # of this list must match the length of `requests` list.</span><br><span class="line">        return responses</span><br><span class="line"> </span><br><span class="line">    def finalize(self):</span><br><span class="line">        &quot;&quot;&quot;`finalize` is called only once when the model is being unloaded.</span><br><span class="line">        Implementing `finalize` function is OPTIONAL. This function allows</span><br><span class="line">        the model to perform any necessary clean ups before exit.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        print(&#x27;Cleaning up...&#x27;)</span><br></pre></td></tr></table></figure>
<ul>
<li>在初始化的时候，这里的代码读取了model_config, 然后使用了pb_utils进而读取了triton的config文件，最重要的两步是吧tokenizer，和llm创建出来后，把llm通过to(‘cuda:’+args[‘model_instance_device_id’])放置在应该的gpu上面去。</li>
<li>而这里的execute函数，会在triton server接收到了pb_util.InferenceRequest数组之后对他们进行处理。然后返回pb_util.InferenceRespond数组作为输出。处理的过程中，需要对每个request进行遍历，构建出重要的四个参数，温度，历史对话，max_token。接着通过llm的接口，得到response的参数后，组装出inferenceResponse。</li>
</ul>
<h4 id="triton-vllm-代码"><a href="#triton-vllm-代码" class="headerlink" title="triton + vllm 代码"></a>triton + vllm 代码</h4><p>在服务器上，使用vscode remote插件连接远程gpu服务器后，将model.py写好部署。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">设置显存空闲block最大分割阈值</span><br><span class="line">max_split_size_mb:32 表示 PyTorch 将显存中空闲块的最大分割阈值设置为 32 MB。</span><br><span class="line">这意味着，当有大于 32 MB 的空闲显存块时，PyTorch 可能会将其分割成更小的块，以便更灵活地分配和管理显存。</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">os.environ[&#x27;PYTORCH_CUDA_ALLOC_CONF&#x27;] = &#x27;max_split_size_mb:32&#x27;</span><br><span class="line"></span><br><span class="line"># 设置work目录, 指定transformer库的缓存目录</span><br><span class="line">os.environ[&#x27;TRANSFORMERS_CACHE&#x27;] = os.path.dirname(os.path.abspath(__file__))+&quot;/work/&quot;</span><br><span class="line">os.environ[&#x27;HF_MODULES_CACHE&#x27;] = os.path.dirname(os.path.abspath(__file__))+&quot;/work/&quot;</span><br><span class="line"></span><br><span class="line">import json</span><br><span class="line"> </span><br><span class="line"># triton_python_backend_utils is available in every Triton Python model. You</span><br><span class="line"># need to use this module to create inference requests and responses. It also</span><br><span class="line"># contains some utility functions for extracting information from model_config</span><br><span class="line"># and converting Triton input/output types to numpy types.</span><br><span class="line">import triton_python_backend_utils as pb_utils</span><br><span class="line">import sys</span><br><span class="line">import gc</span><br><span class="line">import time</span><br><span class="line">import logging</span><br><span class="line">import torch</span><br><span class="line">from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># 使用vllm进行加速</span><br><span class="line">from vllm import LLM, SamplingParams</span><br><span class="line"></span><br><span class="line"># 强制垃圾回收，可以确保尽可能多地释放内存资源，减少内存使用峰值。</span><br><span class="line">gc.collect()</span><br><span class="line"></span><br><span class="line"># 由于我只在4号卡上运行，不需要这。</span><br><span class="line"># torch.cuda.empty_cache()</span><br><span class="line"> </span><br><span class="line">logging.basicConfig(format=&#x27;%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s&#x27;,</span><br><span class="line">                    level=logging.INFO)</span><br><span class="line"> </span><br><span class="line">class TritonPythonModel:</span><br><span class="line">    &quot;&quot;&quot;Your Python model must use the same class name. Every Python model</span><br><span class="line">    that is created must have &quot;TritonPythonModel&quot; as the class name.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"> </span><br><span class="line">    def initialize(self, args):</span><br><span class="line">        &quot;&quot;&quot;`initialize` is called only once when the model is being loaded.</span><br><span class="line">        Implementing `initialize` function is optional. This function allows</span><br><span class="line">        the model to intialize any state associated with this model.</span><br><span class="line">        Parameters</span><br><span class="line">        ----------</span><br><span class="line">        args : dict</span><br><span class="line">          Both keys and values are strings. The dictionary keys and values are:</span><br><span class="line">          * model_config: A JSON string containing the model configuration</span><br><span class="line">          * model_instance_kind: A string containing model instance kind</span><br><span class="line">          * model_instance_device_id: A string containing model instance device ID</span><br><span class="line">          * model_repository: Model repository path</span><br><span class="line">          * model_version: Model version</span><br><span class="line">          * model_name: Model name</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # You must parse model_config. JSON string is not parsed here</span><br><span class="line">        self.model_config = json.loads(args[&#x27;model_config&#x27;])</span><br><span class="line">        </span><br><span class="line">        output_response_config = pb_utils.get_output_config_by_name(self.model_config, &quot;response&quot;)</span><br><span class="line">        output_history_config = pb_utils.get_output_config_by_name(self.model_config, &quot;history&quot;)</span><br><span class="line"> </span><br><span class="line">        # Convert Triton types to numpy types</span><br><span class="line">        self.output_response_dtype = pb_utils.triton_string_to_numpy(output_response_config[&#x27;data_type&#x27;])</span><br><span class="line">        self.output_history_dtype = pb_utils.triton_string_to_numpy(output_history_config[&#x27;data_type&#x27;])</span><br><span class="line">        </span><br><span class="line">        # 加载模型本身</span><br><span class="line">        ChatGLM_path = os.path.dirname(os.path.abspath(__file__))+&quot;/glm-4-9b-chat&quot;</span><br><span class="line">        #self.tokenizer = AutoTokenizer.from_pretrained(ChatGLM_path, trust_remote_code=True)</span><br><span class="line">        #下面to(&#x27;cuda:&#x27;+args[&#x27;model_instance_device_id&#x27;])把实例部署到对应的显卡上，不写会分散到所有显卡上或者集中到一个显卡上</span><br><span class="line">        # vllm初始化</span><br><span class="line">        max_model_len, tp_size = 20400, 1</span><br><span class="line">        self.model = LLM(model=ChatGLM_path,</span><br><span class="line">                    tokenizer=ChatGLM_path,</span><br><span class="line">                    tensor_parallel_size=tp_size,</span><br><span class="line">                    dtype=&#x27;float16&#x27;,</span><br><span class="line">                    max_model_len=max_model_len,</span><br><span class="line">                    enforce_eager=True,</span><br><span class="line">                    trust_remote_code=True)</span><br><span class="line">        # 我需要它在指定的卡上运行</span><br><span class="line">        # self.model = model.eval()</span><br><span class="line"></span><br><span class="line">        self.stop_token_ids = [151329, 151336, 151338]</span><br><span class="line">        # samplingParams不需要，post请求会带上这些参数</span><br><span class="line"></span><br><span class="line">        logging.info(&quot;model init success&quot;)</span><br><span class="line">        </span><br><span class="line">    def execute(self, requests):</span><br><span class="line">        &quot;&quot;&quot;`execute` MUST be implemented in every Python model. `execute`</span><br><span class="line">        function receives a list of pb_utils.InferenceRequest as the only</span><br><span class="line">        argument. This function is called when an inference request is made</span><br><span class="line">        for this model. Depending on the batching configuration (e.g. Dynamic</span><br><span class="line">        Batching) used, `requests` may contain multiple requests. Every</span><br><span class="line">        Python model, must create one pb_utils.InferenceResponse for every</span><br><span class="line">        pb_utils.InferenceRequest in `requests`. If there is an error, you can</span><br><span class="line">        set the error argument when creating a pb_utils.InferenceResponse</span><br><span class="line">        Parameters</span><br><span class="line">        ----------</span><br><span class="line">        requests : list</span><br><span class="line">          A list of pb_utils.InferenceRequest</span><br><span class="line">        Returns</span><br><span class="line">        -------</span><br><span class="line">        list</span><br><span class="line">          A list of pb_utils.InferenceResponse. The length of this list must</span><br><span class="line">          be the same as `requests`</span><br><span class="line">          </span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        output_response_dtype = self.output_response_dtype</span><br><span class="line">        output_history_dtype = self.output_history_dtype</span><br><span class="line"> </span><br><span class="line">        # output_dtype = self.output_dtype</span><br><span class="line">        responses = []</span><br><span class="line">        # Every Python backend must iterate over everyone of the requests</span><br><span class="line">        # and create a pb_utils.InferenceResponse for each of them.</span><br><span class="line"></span><br><span class="line">        for request in requests:</span><br><span class="line">            prompt = pb_utils.get_input_tensor_by_name(request, &quot;prompt&quot;).as_numpy()[0]</span><br><span class="line">            prompt = prompt.decode(&#x27;utf-8&#x27;)</span><br><span class="line">            history_origin = pb_utils.get_input_tensor_by_name(request, &quot;history&quot;).as_numpy()</span><br><span class="line">            if len(history_origin) &gt; 0:</span><br><span class="line">                history = np.array([item.decode(&#x27;utf-8&#x27;) for item in history_origin]).reshape((-1,2)).tolist()</span><br><span class="line">            else:</span><br><span class="line">                history = []</span><br><span class="line">            temperature = pb_utils.get_input_tensor_by_name(request, &quot;temperature&quot;).as_numpy()[0]</span><br><span class="line">            temperature = float(temperature.decode(&#x27;utf-8&#x27;))</span><br><span class="line">            max_token = pb_utils.get_input_tensor_by_name(request, &quot;max_token&quot;).as_numpy()[0]</span><br><span class="line">            max_token = int(max_token.decode(&#x27;utf-8&#x27;))</span><br><span class="line">            history_len = pb_utils.get_input_tensor_by_name(request, &quot;history_len&quot;).as_numpy()[0]</span><br><span class="line">            history_len = int(history_len.decode(&#x27;utf-8&#x27;))</span><br><span class="line">            </span><br><span class="line">            # 日志输出传入信息</span><br><span class="line">            in_log_info = &#123;</span><br><span class="line">                &quot;in_prompt&quot;:prompt,</span><br><span class="line">                &quot;in_history&quot;:history,</span><br><span class="line">                &quot;in_temperature&quot;:temperature,</span><br><span class="line">                &quot;in_max_token&quot;:max_token,</span><br><span class="line">                &quot;in_history_len&quot;:history_len</span><br><span class="line">                       &#125;</span><br><span class="line">            logging.info(in_log_info)</span><br><span class="line"></span><br><span class="line">            # 执行推理</span><br><span class="line">            sampling_params = SamplingParams(temperature=temperature, max_tokens=max_token, stop_token_ids=self.stop_token_ids)</span><br><span class="line">            response = self.model.generate(prompts=prompt,sampling_params=sampling_params)</span><br><span class="line"></span><br><span class="line">            # 日志输出处理后的信息</span><br><span class="line">            out_log_info = &#123;</span><br><span class="line">                &quot;out_response&quot;:response,</span><br><span class="line">                &quot;out_history&quot;:history</span><br><span class="line">                       &#125;</span><br><span class="line">            logging.info(out_log_info)</span><br><span class="line">            response = np.array(response)</span><br><span class="line">            # history = np.array(history)</span><br><span class="line">            </span><br><span class="line">            response_output_tensor = pb_utils.Tensor(&quot;response&quot;,response.astype(self.output_response_dtype))</span><br><span class="line">            # history_output_tensor = pb_utils.Tensor(&quot;history&quot;,history.astype(self.output_history_dtype))</span><br><span class="line"> </span><br><span class="line">            #  final_inference_response = pb_utils.InferenceResponse(output_tensors=[response_output_tensor,history_output_tensor])</span><br><span class="line">            final_inference_response = pb_utils.InferenceResponse(output_tensors=[response_output_tensor])</span><br><span class="line">            responses.append(final_inference_response)</span><br><span class="line">            # Create InferenceResponse. You can set an error here in case</span><br><span class="line">            # there was a problem with handling this inference request.</span><br><span class="line">            # Below is an example of how you can set errors in inference</span><br><span class="line">            # response:</span><br><span class="line">            #</span><br><span class="line">            # pb_utils.InferenceResponse(</span><br><span class="line">            #    output_tensors=..., TritonError(&quot;An error occured&quot;))</span><br><span class="line"> </span><br><span class="line">        # You should return a list of pb_utils.InferenceResponse. Length</span><br><span class="line">        # of this list must match the length of `requests` list.</span><br><span class="line">        return responses</span><br><span class="line"> </span><br><span class="line">    def finalize(self):</span><br><span class="line">        &quot;&quot;&quot;`finalize` is called only once when the model is being unloaded.</span><br><span class="line">        Implementing `finalize` function is OPTIONAL. This function allows</span><br><span class="line">        the model to perform any necessary clean ups before exit.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        print(&#x27;Cleaning up...&#x27;)</span><br></pre></td></tr></table></figure>


<h4 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h4><ul>
<li>oom问题： 这个时候和指定gpu有关，9b的模型实测跑起来使用float16的话，大概指定pt数目为1（单卡部署），如果还是不够，则需要缩短上下文长度，glm4-9b-chat支持128k，但是内存不构，连40K跑起来都无法装入一整张32GV100, 最后博主缩短到20K(20400）的情况下，才可以运行，大概静态显存是占用23G。</li>
<li>gpu指定的问题： 在启动docker镜像的时候如果是其他gpu已经被占用的情况下（博主的情况只能使用1块32G的V100），使用这个docker参数，-gpus&#x3D;’”device&#x3D;4”，这里的指定让代码无需去指定。</li>
<li>vllm构造问题：vllm一些问题，最好使用vllm的llm接口，同时把tokenizer一并传入。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">  max_model_len, tp_size = 20400, 1</span><br><span class="line">   self.model = LLM(model=ChatGLM_path,</span><br><span class="line">                    tokenizer=ChatGLM_path,</span><br><span class="line">                    tensor_parallel_size=tp_size,</span><br><span class="line">                    dtype=&#x27;float16&#x27;,</span><br><span class="line">                    max_model_len=max_model_len,</span><br><span class="line">                    enforce_eager=True,</span><br><span class="line">                    trust_remote_code=True)</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="一些总结和理解"><a href="#一些总结和理解" class="headerlink" title="一些总结和理解"></a>一些总结和理解</h2><p>triton的下一层各种backend是可以兼容，比如vllm加速推理。</p>
<p>triton的更上一层是可以用k8s抽象管理的。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://docs.vllm.ai/en/latest/dev/offline_inference/llm.html#">https://docs.vllm.ai/en/latest/dev/offline_inference&#x2F;llm.html#</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/cm2010_03_31/article/details/135986638">https://blog.csdn.net/cm2010_03_31&#x2F;article&#x2F;details&#x2F;135986638</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/THUDM/GLM-4">https://github.com/THUDM/GLM-4</a></p>
</div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/AI-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">AI(人工智能)</a><a class="link-muted mr-2" rel="tag" href="/tags/Triton/">Triton</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2024/06/17/%E6%95%B0%E6%8D%AE%E5%BA%93/redis(unfinish%209)/"><span class="level-item">redis一些总结</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/./img/myavatar.png" alt="Sam"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Sam</p><p class="is-size-6 is-block">极客，星际流浪者</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>现居深圳，2年达拉斯</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">41</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">0</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">19</p></a></div></div></nav><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://google.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">google</span></span><span class="level-right"><span class="level-item tag">google.com</span></span></a></li></ul></div></div></div><!--!--><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-06-17T15:22:02.589Z">2024-06-17</time></p><p class="title"><a href="/2024/06/17/ai%E5%B7%A5%E7%A8%8B/triton%20+%20chatglm4%20(finish)/">triton + chatglm4 部署</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-06-17T15:21:30.768Z">2024-06-17</time></p><p class="title"><a href="/2024/06/17/%E6%95%B0%E6%8D%AE%E5%BA%93/redis(unfinish%209)/">redis一些总结</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-06-17T15:20:11.449Z">2024-06-17</time></p><p class="title"><a href="/2024/06/17/%E7%AE%97%E6%B3%95/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E6%9C%80%E5%A4%A7%E5%80%BC(finish)/">滑动窗口最大值</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-06-03T14:59:19.717Z">2024-06-03</time></p><p class="title"><a href="/2024/06/03/%E5%89%8D%E7%AB%AF/typescript%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95%EF%BC%88finish%EF%BC%89/">TypeScript基本语法</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-06-03T14:40:24.890Z">2024-06-03</time></p><p class="title"><a href="/2024/06/03/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%80%E4%BA%9B%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93%EF%BC%88%E7%B4%A2%E5%BC%95skip%EF%BC%89/">关系型数据库的基础总结</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/"><span class="level-start"><span class="level-item">2024</span></span><span class="level-end"><span class="level-item tag">41</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/AI-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"><span class="tag">AI(人工智能)</span><span class="tag">21</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Agents/"><span class="tag">Agents</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Language-c/"><span class="tag">Language(c++)</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Language-python/"><span class="tag">Language(python)</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux%E7%B3%BB%E7%BB%9F/"><span class="tag">Linux系统</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Triton/"><span class="tag">Triton</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/docker%E5%AE%B9%E5%99%A8/"><span class="tag">docker容器</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/llm-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"><span class="tag">llm(语言模型)</span><span class="tag">17</span></a></div><div class="control"><a class="tags has-addons" href="/tags/redis/"><span class="tag">redis</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/unfinish/"><span class="tag">unfinish</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%89%8D%E7%AB%AF/"><span class="tag">前端</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%90%8E%E6%AE%B5%E6%9E%B6%E6%9E%84/"><span class="tag">后段架构</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/"><span class="tag">向量数据库</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"><span class="tag">多线程</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"><span class="tag">操作系统</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE/"><span class="tag">数据</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"><span class="tag">数据库</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95/"><span class="tag">算法</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"><span class="tag">设计模式</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/mylogo.png" alt="zeon&#039;s blog" height="28"></a><p class="is-size-7"><span>&copy; 2024 Sam</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2023</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>