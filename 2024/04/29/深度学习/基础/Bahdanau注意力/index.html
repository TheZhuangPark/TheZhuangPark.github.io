<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>bahdanau注意力 - Hexo</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Hexo"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Hexo"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="bahdanau注意力机制的初心本意是为了seq2seq学习而设计出的编码器解码器架构，有个弊端。对于编码器："><meta property="og:type" content="blog"><meta property="og:title" content="bahdanau注意力"><meta property="og:url" content="http://example.com/2024/04/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80/Bahdanau%E6%B3%A8%E6%84%8F%E5%8A%9B/"><meta property="og:site_name" content="Hexo"><meta property="og:description" content="bahdanau注意力机制的初心本意是为了seq2seq学习而设计出的编码器解码器架构，有个弊端。对于编码器："><meta property="og:locale" content="en_US"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:published_time" content="2024-04-28T23:05:35.876Z"><meta property="article:modified_time" content="2020-04-08T16:00:00.000Z"><meta property="article:author" content="Sam"><meta property="article:tag" content="ai"><meta property="article:tag" content="llm"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://example.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com/2024/04/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80/Bahdanau%E6%B3%A8%E6%84%8F%E5%8A%9B/"},"headline":"bahdanau注意力","image":["http://example.com/img/og_image.png"],"datePublished":"2024-04-28T23:05:35.876Z","dateModified":"2020-04-08T16:00:00.000Z","author":{"@type":"Person","name":"Sam"},"publisher":{"@type":"Organization","name":"Hexo","logo":{"@type":"ImageObject","url":"http://example.com/img/logo.svg"}},"description":"bahdanau注意力机制的初心本意是为了seq2seq学习而设计出的编码器解码器架构，有个弊端。对于编码器："}</script><link rel="canonical" href="http://example.com/2024/04/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80/Bahdanau%E6%B3%A8%E6%84%8F%E5%8A%9B/"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Hexo" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-04-28T23:05:35.876Z" title="4/29/2024, 7:05:35 AM">2024-04-29</time></span><span class="level-item">12 minutes read (About 1766 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">bahdanau注意力</h1><div class="content"><h2 id="bahdanau注意力机制的初心"><a href="#bahdanau注意力机制的初心" class="headerlink" title="bahdanau注意力机制的初心"></a>bahdanau注意力机制的初心</h2><p>本意是为了seq2seq学习而设计出的编码器解码器架构，有个弊端。<br>对于编码器：</p>
<span id="more"></span>

<blockquote>
<p>h(t) &#x3D; f ( x(t), h(t-1) )，每一时间步的隐状态由前一步的隐状态，和输入x(t) 决定。<br>c &#x3D; g( h(t), h(t-1), h(t-2), …. , h(1) )， 最终得到的上下文变量由隐藏状态决定。</p>
</blockquote>
<p>ps: 一般而言这个g函数，就是选择最后一个隐状态。<br>对于解码器：</p>
<blockquote>
<p>s(t) &#x3D; g( y(t-1), s(t-1), c ), 每一时间步的隐状态由前一步隐状态, 和输出y(t-1), 和由编码器来的上下文变量c决定。<br>y(t) &#x3D; o( s(t),  c）， 输出y(t)由当前隐状态和上下文变量决定。</p>
</blockquote>
<p>这就是编码解码器架构，简单的四个公式决定。<br>但是这里有个问题，这个c，上下文变量在解码的过程中，从来没有变过。</p>
<blockquote>
<p>Attention &#x3D; Sum(  a(q, k) * v )，注意力汇聚公式：<br>c(t) &#x3D; Sum( a(s(t-1), h(t) ) * h(t) )， 为了改善c从来没变过的事实：把query设置为解码器的隐变量s(t-1)，key和value设置为编码器隐变量h(t)。</p>
</blockquote>
<p>这个时候c(t)蕴含了X(t1, t2,….t),  Y(t1, t2,… t-1)的信息，非常利于预测下一个时间步的值。<br>举个例子：X(t1, t2,….t) &#x3D; “l love you”,   Y(t1, t2,… t-1)&#x3D;”我爱“， 下一个时间步预测出”你“</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>基本seq2seq基类</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line">class Encoder(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;编码器-解码器架构的基本编码器接口&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, **kwargs):</span><br><span class="line">        super(Encoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    def forward(self, X, *args):</span><br><span class="line">        raise NotImplementedError</span><br><span class="line"></span><br><span class="line">class Decoder(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;编码器-解码器架构的基本解码器接口&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, **kwargs):</span><br><span class="line">        super(Decoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    def init_state(self, enc_outputs, *args):</span><br><span class="line">        raise NotImplementedError</span><br><span class="line"></span><br><span class="line">    def forward(self, X, state):</span><br><span class="line">        raise NotImplementedError</span><br><span class="line">				</span><br><span class="line">class EncoderDecoder(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;编码器-解码器架构的基类&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, encoder, decoder, **kwargs):</span><br><span class="line">        super(EncoderDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line"></span><br><span class="line">    def forward(self, enc_X, dec_X, *args):</span><br><span class="line">        enc_outputs = self.encoder(enc_X, *args)</span><br><span class="line">        dec_state = self.decoder.init_state(enc_outputs, *args)</span><br><span class="line">        return self.decoder(dec_X, dec_state)</span><br></pre></td></tr></table></figure>

<p>训练代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">def sequence_mask(X, valid_len, value=0):</span><br><span class="line">    &quot;&quot;&quot;在序列中屏蔽不相关的项&quot;&quot;&quot;</span><br><span class="line">    maxlen = X.size(1)</span><br><span class="line">    mask = torch.arange((maxlen), dtype=torch.float32,</span><br><span class="line">                        device=X.device)[None, :] &lt; valid_len[:, None]</span><br><span class="line">    X[~mask] = value</span><br><span class="line">    return X</span><br><span class="line">		</span><br><span class="line">class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):</span><br><span class="line">    &quot;&quot;&quot;带遮蔽的softmax交叉熵损失函数&quot;&quot;&quot;</span><br><span class="line">    # pred的形状：(batch_size,num_steps,vocab_size)</span><br><span class="line">    # label的形状：(batch_size,num_steps)</span><br><span class="line">    # valid_len的形状：(batch_size,)</span><br><span class="line">    def forward(self, pred, label, valid_len):</span><br><span class="line">        weights = torch.ones_like(label)</span><br><span class="line">        weights = sequence_mask(weights, valid_len)</span><br><span class="line">        self.reduction=&#x27;none&#x27;</span><br><span class="line">        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(</span><br><span class="line">            pred.permute(0, 2, 1), label)</span><br><span class="line">        weighted_loss = (unweighted_loss * weights).mean(dim=1)</span><br><span class="line">        return weighted_loss</span><br><span class="line"></span><br><span class="line">def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):</span><br><span class="line">    &quot;&quot;&quot;训练序列到序列模型&quot;&quot;&quot;</span><br><span class="line">    def xavier_init_weights(m):</span><br><span class="line">        if type(m) == nn.Linear:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">        if type(m) == nn.GRU:</span><br><span class="line">            for param in m._flat_weights_names:</span><br><span class="line">                if &quot;weight&quot; in param:</span><br><span class="line">                    nn.init.xavier_uniform_(m._parameters[param])</span><br><span class="line"></span><br><span class="line">    net.apply(xavier_init_weights)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">    loss = MaskedSoftmaxCELoss()</span><br><span class="line">    net.train()</span><br><span class="line">    animator = d2l.Animator(xlabel=&#x27;epoch&#x27;, ylabel=&#x27;loss&#x27;,</span><br><span class="line">                     xlim=[10, num_epochs])</span><br><span class="line">    for epoch in range(num_epochs):</span><br><span class="line">        timer = d2l.Timer()</span><br><span class="line">        metric = d2l.Accumulator(2)  # 训练损失总和，词元数量</span><br><span class="line">        for batch in data_iter:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]</span><br><span class="line">            bos = torch.tensor([tgt_vocab[&#x27;&lt;bos&gt;&#x27;]] * Y.shape[0],</span><br><span class="line">                          device=device).reshape(-1, 1)</span><br><span class="line">            dec_input = torch.cat([bos, Y[:, :-1]], 1)  # 强制教学</span><br><span class="line">            Y_hat, _ = net(X, dec_input, X_valid_len)</span><br><span class="line">            l = loss(Y_hat, Y, Y_valid_len)</span><br><span class="line">            l.sum().backward()      # 损失函数的标量进行“反向传播”</span><br><span class="line">            d2l.grad_clipping(net, 1)</span><br><span class="line">            num_tokens = Y_valid_len.sum()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            with torch.no_grad():</span><br><span class="line">                metric.add(l.sum(), num_tokens)</span><br><span class="line">        if (epoch + 1) % 10 == 0:</span><br><span class="line">            animator.add(epoch + 1, (metric[0] / metric[1],))</span><br><span class="line">    print(f&#x27;loss &#123;metric[0] / metric[1]:.3f&#125;, &#123;metric[1] / timer.stop():.1f&#125; &#x27;</span><br><span class="line">        f&#x27;tokens/sec on &#123;str(device)&#125;&#x27;)		</span><br></pre></td></tr></table></figure>

<p>注意力评分函数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">class AdditiveAttention(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;加性注意力&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):</span><br><span class="line">        super(AdditiveAttention, self).__init__(**kwargs)</span><br><span class="line">        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)</span><br><span class="line">        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)</span><br><span class="line">        self.w_v = nn.Linear(num_hiddens, 1, bias=False)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    def forward(self, queries, keys, values, valid_lens):</span><br><span class="line">        queries, keys = self.W_q(queries), self.W_k(keys)</span><br><span class="line">        # 在维度扩展后，</span><br><span class="line">        # queries的形状：(batch_size，查询的个数，1，num_hidden)</span><br><span class="line">        # key的形状：(batch_size，1，“键－值”对的个数，num_hiddens)</span><br><span class="line">        # 使用广播方式进行求和</span><br><span class="line">        features = queries.unsqueeze(2) + keys.unsqueeze(1)</span><br><span class="line">        features = torch.tanh(features)</span><br><span class="line">        # self.w_v仅有一个输出，因此从形状中移除最后那个维度。</span><br><span class="line">        # scores的形状：(batch_size，查询的个数，“键-值”对的个数)</span><br><span class="line">        scores = self.w_v(features).squeeze(-1)</span><br><span class="line">        self.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        # values的形状：(batch_size，“键－值”对的个数，值的维度)</span><br><span class="line">        return torch.bmm(self.dropout(self.attention_weights), values)</span><br></pre></td></tr></table></figure>

<p>注意力seq2seq模型，bahdanau注意</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">class AttentionDecoder(d2l.Decoder):</span><br><span class="line">    &quot;&quot;&quot;带有注意力机制解码器的基本接口&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, **kwargs):</span><br><span class="line">        super(AttentionDecoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def attention_weights(self):</span><br><span class="line">        raise NotImplementedError</span><br><span class="line">				</span><br><span class="line">class Seq2SeqAttentionDecoder(AttentionDecoder):</span><br><span class="line">    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):</span><br><span class="line">        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)</span><br><span class="line">        #q是decoder的隐藏层，kv是encoder的隐藏层</span><br><span class="line">        self.attention = AdditiveAttention(num_hiddens,num_hiddens,num_hiddens,dropout)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.GRU(embed_size+num_hiddens, num_hiddens, num_layers, dropout=dropout)</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    # defenitely don&#x27;t understand </span><br><span class="line">    def init_state(self, enc_outputs, enc_valid_lens, *args):</span><br><span class="line">        outputs, hidden_state = enc_outputs</span><br><span class="line">        return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens)</span><br><span class="line">    </span><br><span class="line">    def forward(self, X, state):</span><br><span class="line">        # enc_outputs的形状为(batch_size,num_steps,num_hiddens).</span><br><span class="line">        # hidden_state的形状为(num_layers,batch_size,</span><br><span class="line">        # num_hiddens)</span><br><span class="line">        enc_outputs, hidden_state, enc_valid_lens = state</span><br><span class="line">        # 输出X的形状为(num_steps,batch_size,embed_size)</span><br><span class="line">        X = self.embedding(X).permute(1, 0, 2)</span><br><span class="line">        outputs, self._attention_weights = [], []</span><br><span class="line">        for x in X:</span><br><span class="line">            # query的形状为(batch_size,1,num_hiddens)</span><br><span class="line">            query = torch.unsqueeze(hidden_state[-1], dim=1)</span><br><span class="line">            # context的形状为(batch_size,1,num_hiddens)</span><br><span class="line">            context = self.attention(</span><br><span class="line">                query, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">            # 在特征维度上连结</span><br><span class="line">            x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)</span><br><span class="line">            # 将x变形为(1,batch_size,embed_size+num_hiddens)</span><br><span class="line">            out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state)</span><br><span class="line">            outputs.append(out)</span><br><span class="line">            self._attention_weights.append(self.attention.attention_weights)</span><br><span class="line">        # 全连接层变换后，outputs的形状为</span><br><span class="line">        # (num_steps,batch_size,vocab_size)</span><br><span class="line">        outputs = self.dense(torch.cat(outputs, dim=0))</span><br><span class="line">        return outputs.permute(1, 0, 2), [enc_outputs, hidden_state,</span><br><span class="line">                                          enc_valid_lens]</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def attention_weights(self):</span><br><span class="line">        return self._attention_weights</span><br></pre></td></tr></table></figure>

<p>测试形状</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">encoder = d2l.Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,</span><br><span class="line">                             num_layers=2)</span><br><span class="line">encoder.eval()</span><br><span class="line">decoder = Seq2SeqAttentionDecoder(vocab_size=10, embed_size=8, num_hiddens=16,</span><br><span class="line">                                  num_layers=2)</span><br><span class="line">decoder.eval()</span><br><span class="line">X = torch.zeros((4, 7), dtype=torch.long)  # (batch_size,num_steps)</span><br><span class="line">state = decoder.init_state(encoder(X), None)</span><br><span class="line">output, state = decoder(X, state)</span><br><span class="line">output.shape, len(state), state[0].shape, len(state[1]), state[1][0].shape</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">(torch.Size([4, 7, 10]), 3, torch.Size([4, 7, 16]), 2, torch.Size([4, 16]))</span><br></pre></td></tr></table></figure>

<p>训练</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1</span><br><span class="line">batch_size, num_steps = 64, 10</span><br><span class="line">lr, num_epochs, device = 0.005, 250, d2l.try_gpu()</span><br><span class="line"></span><br><span class="line">train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)</span><br><span class="line">encoder = d2l.Seq2SeqEncoder(</span><br><span class="line">    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class="line">decoder = Seq2SeqAttentionDecoder(</span><br><span class="line">    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class="line">net = EncoderDecoder(encoder, decoder)</span><br><span class="line">train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span><br></pre></td></tr></table></figure>

<p>运行结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">engs = [&#x27;go .&#x27;, &quot;i lost .&quot;, &#x27;he\&#x27;s calm .&#x27;, &#x27;i\&#x27;m home .&#x27;]</span><br><span class="line">fras = [&#x27;va !&#x27;, &#x27;j\&#x27;ai perdu .&#x27;, &#x27;il est calme .&#x27;, &#x27;je suis chez moi .&#x27;]</span><br><span class="line">for eng, fra in zip(engs, fras):</span><br><span class="line">    translation, dec_attention_weight_seq = d2l.predict_seq2seq(</span><br><span class="line">        net, eng, src_vocab, tgt_vocab, num_steps, device, True)</span><br><span class="line">    print(f&#x27;&#123;eng&#125; =&gt; &#123;translation&#125;, &#x27;,</span><br><span class="line">          f&#x27;bleu &#123;d2l.bleu(translation, fra, k=2):.3f&#125;&#x27;)</span><br><span class="line">				</span><br><span class="line">结果：</span><br><span class="line">go . =&gt; va !,  bleu 1.000</span><br><span class="line">i lost . =&gt; j&#x27;ai perdu .,  bleu 1.000</span><br><span class="line">he&#x27;s calm . =&gt; il est riche .,  bleu 0.658</span><br><span class="line">i&#x27;m home . =&gt; je suis chez moi .,  bleu 1.000</span><br></pre></td></tr></table></figure>
</div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/ai/">ai</a><a class="link-muted mr-2" rel="tag" href="/tags/llm/">llm</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2024/04/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/300%E8%A1%8C%E7%9A%84mini-gpt/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">mini-gpt</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2024/04/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80/%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B/"><span class="level-item">Transformer的从0开始实现</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="Sam"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Sam</p><p class="is-size-6 is-block">极客，星际流浪者</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>现居深圳，2年达拉斯</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">30</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">0</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">16</p></a></div></div></nav><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://google.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">google</span></span><span class="level-right"><span class="level-item tag">google.com</span></span></a></li></ul></div></div></div><!--!--><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-05-29T13:44:37.356Z">2024-05-29</time></p><p class="title"><a href="/2024/05/29/c++/C++%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AF%AD%E8%A8%80%E7%89%B9%E6%80%A7/">c++的一些语言特性总结</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-05-15T15:06:09.071Z">2024-05-15</time></p><p class="title"><a href="/2024/05/15/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%8E%A9%E7%8E%A9gpt4o/">gpt4o观测</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-05-14T13:44:13.516Z">2024-05-14</time></p><p class="title"><a href="/2024/05/14/ai%E5%B7%A5%E7%A8%8B/dify/">dify实践</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-05-13T14:42:44.157Z">2024-05-13</time></p><p class="title"><a href="/2024/05/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80/%E4%B8%80%E4%BA%9B%E9%9C%80%E8%A6%81%E6%8E%A2%E7%A9%B6%E7%9A%84%E9%97%AE%E9%A2%98/">一些需要探究的问题</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-05-13T14:41:10.263Z">2024-05-13</time></p><p class="title"><a href="/2024/05/13/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux%E7%9A%84%E8%BF%9B%E7%A8%8Bcapability/">权限和linux的capabilities</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/"><span class="level-start"><span class="level-item">2024</span></span><span class="level-end"><span class="level-item tag">30</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Language-c/"><span class="tag">Language(c++)</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/agent/"><span class="tag">agent</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ai/"><span class="tag">ai</span><span class="tag">19</span></a></div><div class="control"><a class="tags has-addons" href="/tags/c/"><span class="tag">c++</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/dify/"><span class="tag">dify</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/docker/"><span class="tag">docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/linux/"><span class="tag">linux</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/llm/"><span class="tag">llm</span><span class="tag">19</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/unfinish/"><span class="tag">unfinish</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%86%85%E5%AD%98/"><span class="tag">内存</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/"><span class="tag">向量数据库</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"><span class="tag">操作系统</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE/"><span class="tag">数据</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"><span class="tag">数据库</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95/"><span class="tag">算法</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Hexo" height="28"></a><p class="is-size-7"><span>&copy; 2024 Sam</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2023</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>