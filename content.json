{"posts":[{"title":"c++的一些思考","text":"一些对c++的类的思考 c++的类 思想性 数据设计 和操作限制，以股票为例子，公共借口定义 封装的定义 宏定义 # ifndef, #define 来规避一些重复包含头文件","link":"/2023/11/07/c++%E7%9A%84%E7%B1%BB%E6%80%9D%E8%80%83/"},{"title":"多层rnn","text":"两个例子 GRU，LSTM，但是层数都没有什么增加。 在线性模型内，添加层数很简单，但是对于一个rnn如何添加层数。没有期待一个简单的单层rnn及其变种能够捕捉股市的非常复杂的信息。 以前是 X1-&gt;H1-&gt;O1, (X2,H1)-&gt;H2-&gt;O2, (X3,H2)-&gt;H3-&gt;O3现在是 X1-&gt;H1(1), H1(2) …H1(n)-&gt;O1, (X3, H1(1), H1(2), H1(3)…H1(n)) -&gt;H2(1), H2(2) … H2(n)-&gt;O3 公式：Ht(l) = activate( Ht(l-1)Wxh(l) + Ht-1(l)Whh(l) + Bh(l) )一个隐藏层，需要除了时间步t，还有层数l，输入来自H(t)(l-1），H(t-1)(l) 再torch里面：lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers) 这里可以指定num_hiddens超参数。 图","link":"/2024/04/09/%E5%A4%9A%E5%B1%82rnn/"},{"title":"数据集合下载和预处理","text":"12345678def read_data_nmt(): &quot;&quot;&quot;载入“英语－法语”数据集&quot;&quot;&quot; data_dir = d2l.download_extract('fra-eng') with open(os.path.join(data_dir, 'fra.txt'), 'r', encoding='utf-8') as f: return f.read() raw_text = read_data_nmt() print(raw_text[:75]) 预处理数据集：这里是很简单的，把多空格替换为一个空格，使用小写字母替代大写字母，再单词和标点符号插入空格 1234567891011121314def preprocess_nmt(text): &quot;&quot;&quot;预处理“英语－法语”数据集&quot;&quot;&quot; def no_space(char, prev_char): return char in set(',.!?') and prev_char != ' ' # 使用空格替换不间断空格 # 使用小写字母替换大写字母 text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower() # 在单词和标点符号之间插入空格 out = [' ' + char if i &gt; 0 and no_space(char, text[i - 1]) else char for i, char in enumerate(text)] return ''.join(out) text = preprocess_nmt(raw_text) print(text[:80]) 然后词元化，tokenize？ 123456789101112131415#@savedef tokenize_nmt(text, num_examples=None): &quot;&quot;&quot;词元化“英语－法语”数据数据集&quot;&quot;&quot; source, target = [], [] for i, line in enumerate(text.split('\\n')): if num_examples and i &gt; num_examples: break parts = line.split('\\t') if len(parts) == 2: source.append(parts[0].split(' ')) target.append(parts[1].split(' ')) return source, target source, target = tokenize_nmt(text) source[:6], target[:6] 这个tokenizer函数就是最简单通过空格把单词分成一个一个token 词表：数据集合里所有的唯一单词构成了词表。 加载数据集?对于不够时间步的使用词表里的pad填充，多出的则使用截断。当出现一批量的张量对子input, output就是数据集合准备完成了。","link":"/2024/04/09/%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E4%B8%8B%E8%BD%BD%E5%92%8C%E9%A2%84%E5%A4%84%E7%90%86/"},{"title":"编码器-解码器架构","text":"一些编码器-解码器架构的思考 基本ai问题都可以归类为文字生文字（语言大模型），文字生图（midjourney），文字生视频（sora)。输入和输出都是长度可变的序列，机器翻译语种问题就是这个架构的一个简单的应用。 图？ 数学原理就是基本就是把输入序列映射为形状固定的编码状态S，然后映射成长度可变的序列。编码器应该就是最基本的一个继承自nn.module的具有forward方法的类解码器有点特殊，是将一个编码状态S，和前一个时间步生成的词元，映射成当前时间步的输出词元，所以初始化的时候需要把编码器的output state存起来 for example：S(I love you !) 【】 -&gt; Decoder -&gt; 【我】 , t1时间步S(I love you !) 【我】-&gt; Decoder -&gt; 【爱】，t2时间步S(I love you!) 【我爱】-&gt; Decoder -&gt; 【我爱你】, t3时间步S(I love you!) 【我爱你】-&gt; Decoder -&gt; 【我爱你 !】, t4时间步 Demo代码： 12345678910111213141516171819202122232425from torch import nnclass Encoder(nn.Module): def __init__(self, **kwargs): super(Encoder, self).__init__(**kwargs) def forward(self, X, *args): raise NotImplementedErrorclass Decoder(nn.Module): def __init__(self,**kwargs): super(Decoder, self).__init__(**kwargs) def init_state(self, enc_output, *args): raise NotImplementedError def forward(self, X, state): raise NotImplementedErrorclass EncoderDecoder(nn.Module): def __init__(self, encoder, decoder, **kwargs): super(EncoderDecoder, self).__init__(**kwargs) self.encoder=encoder self.decoder=decoder def forward(self, enc_X, dec_X, **kwargs): enc_output=self.encoder(enc_X,*args) dec_state=self.decoder.init_state(enc_output, *args) return self.decoder(dec_X, dec_state) 落地encoder实现，就是序列的开始是以特殊词元 bos，和结束词元 eos 来完成。 1 输入的X：示例：X = torch.zeros((4, 7), dtype=torch.long)可以理解为批量为4，长度为7的，数字组成的序列。【【3,4,5,1,5,7,8】【3,4,5,1,5,7,8】【3,4,5,1,5,7,8】【3,4,5,1,5,7,8】】 2 经给embedding的X_emb会变成示例：【【【1，2，3，0，5，6】【1，3，4，0，5，6】…】【…】【…】【…】】 3 经给 X_rnn = X = X_emb.permute(1, 0, 2)这里更换成形状 num_steps, batch_size, emb_size，因为始终要按照时间步 t1,t2,t3-&gt;…这样进行forward的，所以每个时间步可以处理多个不同批量的单个词元，这就是变换目的。说到底，rnn和其变种接受的X形状是(时间步，批量，词表大小） 4 output, state=self.rnn(X_rnn)这个时候就是rnn的表演了。由于state的一开始在rnn里面规定的形状就是nxh，由于layer是多层，所以有多个隐藏状态，所以给出的形状(layer_num, n, h)而output这里没有添加dens线性层去映射到vocab范围，所以也是nxh，由于每个时间步都有output，所以给出的形状是(n_steps, n, h) demo代码： 12345678910111213141516171819class Seq2SeqEncoder(d2l.Encoder): &quot;&quot;&quot;用于序列到序列学习的循环神经网络编码器&quot;&quot;&quot; def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs): super(Seq2SeqEncoder, self).__init__(**kwargs) # 嵌入层 self.embedding = nn.Embedding(vocab_size, embed_size) self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout=dropout) def forward(self, X, *args): # 输出'X'的形状：(batch_size,num_steps,embed_size) X = self.embedding(X) # 在循环神经网络模型中，第一个轴对应于时间步 X = X.permute(1, 0, 2) # 如果未提及状态，则默认为0 output, state = self.rnn(X) # output的形状:(num_steps,batch_size,num_hiddens) # state的形状:(num_layers,batch_size,num_hiddens) return output, state 1 上下文变量是如何来？通过取最后一个隐藏层(batch_size, num_hiddens)，即然后赋值n份，以匹配n的批量值。 2 怎么拼接？以下是模拟拼接的代码： 12345678state=torch.ones(2,4,3) #模拟一个state， num_layer=2, batch_size=4, hiddens=3 context=state[-1].repeat(5,1,1) #取最后一个layer，repeat steps次，变成(5,4,3) print(context) X=torch.rand(4,5,5) #本来是batch_size=4, steps=5, emb=5 X=X.permute(1,0,2) #变成(step=5,batch=4,emb=5) X_and_context = torch.cat((X, context), 2)#所以对于X变成steps次，和batch，然后emb，而隐藏层本来就是batch x h，最后复制steps次，然后就是emb+h print(X) X_and_context 可以认为隐藏层的nxh，对每个时间步都跟着X进去了。落地decoder的实现 123456789101112131415161718192021222324class Seq2SeqDecoder(d2l.Decoder):&quot;&quot;&quot;用于序列到序列学习的循环神经网络解码器&quot;&quot;&quot;def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,dropout=0, **kwargs):super(Seq2SeqDecoder, self).__init__(**kwargs)self.embedding = nn.Embedding(vocab_size, embed_size)self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,dropout=dropout)self.dense = nn.Linear(num_hiddens, vocab_size) def init_state(self, enc_outputs, *args): return enc_outputs[1] def forward(self, X, state): # 输出'X'的形状：(batch_size,num_steps,embed_size) X = self.embedding(X).permute(1, 0, 2) # 广播context，使其具有与X相同的num_steps context = state[-1].repeat(X.shape[0], 1, 1) X_and_context = torch.cat((X, context), 2) output, state = self.rnn(X_and_context, state) output = self.dense(output).permute(1, 0, 2) # output的形状:(batch_size,num_steps,vocab_size) # state的形状:(num_layers,batch_size,num_hiddens) return output, state 后面的loss构建，训练，和评估虽然很重要，但是我感觉现在我无法搞定。","link":"/2024/03/18/%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E6%9E%B6%E6%9E%84/"},{"title":"LSTM","text":"隐变量模型，隐藏变量存在一个问题，就是长期信息的保存，和短期输入的缺失。 长期信息的保存可以理解，一系列的序列预测后，最开始的序列信息早已经在模型里失去了。短期信息的输入缺失，这倒是难以理解了。设计灵感来自计算机的逻辑门，逻辑门。 对于信息的保存，lstm可能引入了记忆单元一样的小组件，然后对于这个记忆单元的使用，则设计了3个门来控制这个记忆单元。这三个门分别是，输出门，控制记忆单元输出，输入门，控制记忆单元的输入，遗忘门，重置单元的内容。 这三个门的公式和GRU的差不都都是公式1：Ot = activate(XtWxo + Ht-1Who + Bo)It = activate(XtWxi + Ht-1Whi + Bi)F = activate(XtWxf + Ht-1Whf + Bf)也就是Xt和Ht-1的输入，作为门的输入控制。 还有候选记忆单元C公式2Ct-=tanh(XtWxc + Ht-1Whc + Bc) 对于新的记忆实体公式3Ct = Ft O Ct-1 + It O Ct-其中这里的遗忘门控制着需要忘记多少过去的记忆，It则控制要接受多少新的记忆。 公式4Ht = Ot O Ct这里输出门发挥作用，就i是将有效的记忆传递给预测部分的隐状态，但是！输出门这里可以为0，或者接近0，这样的可以保留记忆信息，但是不需要更新隐状态。 图 后面则是实现的部分。not gonna implement it","link":"/2024/04/09/LSTM/"},{"title":"GRU(门控循环单元)","text":"门控rnn是一个现代的rnn的变体。在rnn计算梯度的时候，往往出现了消失爆炸的情况。 1 早期观测值对预测所有未来观测值具有非常重要的意义，需要给早期值一个大梯度，这句话无法理解。2 情绪分析中，出现的h5之类的词元和观测无相关性，需要机制来跳过隐状态表示这类词元。3 中间出现连续中断，一本书中间的章节，还有牛市和熊市之间的中断，需要机制重置内部状态表示。 总结上述可能导致梯度的问题，和机制就是，1需要跳过无关词元机制2需要重置内部状态3需要给早期值一个大梯度 总共两个方式来解决，LSTM(长短记忆），门控循环单元(gate recurrent unit)GRU比较简单，它可以在观测不相关词元，则不更新隐状态，对应机制1， 也可以在必要的时候重置隐状态，对应机制2，对于机制3，可以在观测第一个词元后不更新隐状态，对应机制1。 对于这个机制，可以理解的是，需要通过两个小组件来决定是否要更新，是否需要重置？所以这里才命名成，更新门，和重置门。 这里可能需要放如何实现更新门，和重置门的公式。 简单的说，可以通过进化和退化，来说明重置门，和候选隐状态的关系。公式1再通过更新门来说明候选隐状态，和下一个真正的隐状态的关系。公式2 图 然后就是demo的实现了。","link":"/2024/04/09/GRU(%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83)/"}],"tags":[{"name":"C++","slug":"C","link":"/tags/C/"},{"name":"ai","slug":"ai","link":"/tags/ai/"},{"name":"llm","slug":"llm","link":"/tags/llm/"},{"name":"数据","slug":"数据","link":"/tags/%E6%95%B0%E6%8D%AE/"}],"categories":[],"pages":[]}