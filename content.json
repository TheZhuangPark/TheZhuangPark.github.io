{"posts":[{"title":"c++创建内存的方式","text":"c++内存管理问题若干 【1 c++ 内存管理】c++ 创建内存的方式有多少种？ 1 栈内存2 堆内存3 静态存储区4 常量区 c++ elf格式的文件有哪些？ elf格式是长什么样的？ 【2 堆和栈内存】info reg ulimit -s 8176 【3 变量定义与生命周期】ojbk了 【内存对齐】【智能脂针】","link":"/2024/05/13/c++/c++%E5%88%9B%E5%BB%BA%E5%86%85%E5%AD%98%E7%9A%84%E6%96%B9%E5%BC%8F/"},{"title":"python效率问题","text":"一个高性能的深度学习库中进行了大量的字典查找、 代码执行和许多其他的Python代码。 Python的问题全局解释器锁是众所周知的。在深度学习环境中，我们担心速度极快的GPU可能要等到CPU运行Python代码后才能运行另一个作业。Cython是一个很好解决该问题的方法。","link":"/2024/04/29/python/Python%E7%9A%84%E6%93%8D%E4%BD%9C%E6%95%88%E7%8E%87/"},{"title":"c++的一些语言特性总结","text":"左值和右值: 区别 引用 转化（这个地方非常复杂）左值通常都是存在的持久化对象，有地址，可以通过&amp;来获取地址右值通常是临时变量，不可以通过&amp;来获取地址 常见右值： 12int x = 3 + 1; (右值）int y = a + b; (右值） 函数返回值当然可以是右值所谓的左值引用底层都是指针实现的。一个普通的左值引用可以绑定什么？只能绑定左值 12345678910int &amp;ref = x;int &amp;ref = x+y; （因为右值无法取地址，所以会失败）int &amp;ref = 10; (因为右值无法取地址，所以会失败）但是常量左值引用可以绑定右值, 左值, 常量左值，（这是为什么参数都是常量左值引用的原因）const int &amp;ref = x+y;const int &amp;ref = 30;const int &amp;ref = x;const int &amp;ref = y; 指针及其大小，用法在64位系统里，一个指针类型占8个字节，也就是64位，专门用来存地址。 12char* p = nullptr;sizeof(p); //会打印8 在C里面实际NULL是个0，而C++里面的nullptr不是。指针还可以和整型or常量进行叠加运算，或者比较大小。而且还有各种类型： 1 指向普通对象的指针 2 指向常量对象的指针 const修饰 3 指向函数的指针 typedef int (*funp)(int, int) 4 指向成员变量的指针 5 指向成员函数的指针 &amp;A::add 6 指向静态成员函数的指针 A::get由于成员函数的多态性，成员函数的地址到运行时才会知道，所以才需要这个&amp;符号还有对象特有的this指针 指针和引用的区别引用是一个变量的别名，本质上是通过存储对象地址来实现的，只是编译器帮忙省略了*号 1 可变: 指针指向可以改变，引用不可以 2 占内存: 指针本身占内存，引用不占内存 3 为空: 指针可以不初始化直接野指针，引用一开始必须初始化绑定对象 4 多级: 指针多级可以，引用不可以 常量指针和指针常量123456789const int * p;int const * p; 都一样的只要const在*右边都说明，这指针是个指向常量的指针，常量指针。但是一旦，int * const p; 那就不一样了，说明是个指针常量，该指针的指向是不可以改变的。int const * const p; 这是一个指向常量的指针常量。int ** const p; 一个二级指针常量int * const * p; 指向常量的指针常量int const **p; 一个二级常量指针int * const * const p; 首先是一个指针常量，同时它指向一个指针常量 函数指针的定义1234int func1(){}int func2(){}fun=&amp;func1;fun=func1; 第一种取法是把函数当成一个对象，取其地址，返回的是一个指针。第二种取法是因为func1函数名字存放就是函数首地址，也是一个指针。 迭代器的作用 vector是随机访问迭代器，输入，输出，向前，双向，指针运算都可以 List是双向迭代器 Deque是随机迭代器 Map是双向，MultiMap也是双向，Set也是双向 Stack没有迭代器，Queue没有迭代器，Priority-queue 没有迭代器 这也不是bug的点，而是在于迭代的过程中，碰到某些条件进而添加了某个元素，如果刚好容量不足， 则会将旧的内存块复制或移动到新的内存块。然后释放旧的内存块。这个时候迭代器就失效了。 1 可以使用reserve()函数 2 或者索引遍历 3 或者是收集添加的元素，之后添加。 野指针和悬空指针详解野指针就是哪些不确定的尚未初始化的指针。 1void *p; 悬空指针就是原本指向一块内存，但是被free掉了，这个指针也没有被置空。当然很多宏定义free直接把，置空也给搞定了。 123456#define FREE(p) do { \\if(ptr) { \\free(ptr); \\ptr=NULL; \\} \\} while(0) \\ 类型转换有一共4种类型转换： static_cast:静态转换，编译期间转换，失败的话会抛出一个编译错误。一般用于如下数据类型强制转换基本数据类型转换基类和子类的指针转换空指针转变为目标类型的指针任何类型的表达式转化为void类型 cons_cast:用于const和非const，volatile和非volatile，之间的转换。只能用取掉指针或引用的常量性。 reinterpret_cast:指针和引用与整型的互相转换，执行的过程是逐个比特复制。 dynamic_cast:这种和前3种的巨大区别在于它是运行时态的，主要用于子类和派生类的之间的转换，就是向下转化和向上转化。p3 = dynamic_cast&lt;Derive *&gt;(p1); 类型萃取对于普通类型，得到其类型很简单，但是使用模板编程的时候确定其类型就很难。传入的模板为不确定的。而且还要针对不同类型进行处理，比如一个自定义的拷贝函数。 1bool copy(T *dest, T *src); 如果传入的T是int类型，则只需要 *dest = src如果传入的T是char类型，则需要更复杂的处理这是类型萃取的来由 123456789101112131415#include &lt;type_traits&gt;template&lt;typename T&gt;void checkIntegral() {if (std::is_integral&lt;T&gt;::value) {std::cout &lt;&lt; &quot;T is an integral type&quot; &lt;&lt; std::endl;} else {std::cout &lt;&lt; &quot;T is not an integral type&quot; &lt;&lt; std::endl;}}int main() {checkIntegral&lt;int&gt;(); // 输出: T is an integral typecheckIntegral&lt;double&gt;(); // 输出: T is not an integral typecheckIntegral&lt;char&gt;(); // 输出: T is an integral typereturn 0;} C++的nullptr和NULL的比较比如说函数重载的时候，其实传入的是一个NULL指针，但是本质底层NULL是0.这个时候到底是重载哪个函数好呢？而且nullptr是有单独的类型:typdef decltype(nullptr) nullptr_t 方便类型检查 结构体相等判断方式及memcmp函数的使用? ：如果只是对比结构体之间的值，比如两个学生结构体，对比其学号和考试分数。最好使用友元=号运算符重载，因为结构体对齐补充的字节内容是随机垃圾值。memcmp对空间内字节进行逐个逐个比较，容易出错。 模板及其实现模板很多种黑魔法，但是当前就只介绍一些入门的模板编程这是函数模板 123456789101112131415161718192021222324252627282930313233template &lt;typename T&gt;T add_fun(const T &amp; tmp1, const T &amp; tmp2){ return tmp1 + tmp2;}类模板template &lt;typename T&gt;class Complex{ public: //构造函数 Complex(T a, T b) { this-&gt;a = a; this-&gt;b = b; } //运算符重载 Complex&lt;T&gt; operator+(Complex &amp;c) { Complex&lt;T&gt; tmp(this-&gt;a + c.a, this-&gt;b + c.b); cout &lt;&lt; tmp.a &lt;&lt; &quot; &quot; &lt;&lt; tmp.b &lt;&lt; endl; return tmp; } private: T a; T b;};Complex&lt;int&gt; a(10, 20);Complex&lt;int&gt; b(20, 30);Complex&lt;int&gt; c = a + b;可惜的是类不能从构造函数的入参来推导类型，所以最后还是得加个&lt;&gt; 变量模板：在 C++14 以后，变量也可以参数化为特定的类型，这称为变量模板 123456template&lt;typename T&gt;constexpr T pi = T{3.141592653589793238462643383L};std::cout &lt;&lt; pi&lt;double&gt; &lt;&lt; '\\n';std::cout &lt;&lt; pi&lt;float&gt; &lt;&lt; '\\n';这里有个问题一个类如果它有静态变量，这个时候如果该类有多个模板对象。则它含有的静态成员是根据类型数目来的，如果有两个类型，则两个静态变量。 模板特化很多都是全通用的模板，但是有些场景下，只允许几个类型进入函数体。 1234567template &lt;&gt; //函数模板特化bool compare(char *t1, char *t2){cout &lt;&lt; &quot;特化版本：&quot;;return strcmp(t1, t2) == 0;}还有偏特化，也就是模板参数值确定一部分。 为什么switch的case不建议定义变量简单的说就是生命周期不符合，在switch这个花括号里面，变量的生命周期拉太长。其实只在一两个case里面使用，但是不应该拉到整个花括号里使用。 什么是可变参数模板这个留下次吧","link":"/2024/05/29/c++/C++%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AF%AD%E8%A8%80%E7%89%B9%E6%80%A7/"},{"title":"jupyter环境搭建","text":"之前觉得jupyter notebook是个玩具，但是pycharm使用起来笨重，后面基本都是小的想法都靠jupyter notebook验证了 使用 Jupyter Notebook、NumPy 和 PyTorch 编写简单的个人项目。当你完成它们时 a) 发布良好的、记录良好的代码（参见我的 github） b) 写一篇关于你所做的事情的简短博客文章（参见我的博客） 重装系统 很简单的动作，一般远程服务器，都有服务器管理软件，使用该软件进行加载镜像，重启重装即可。 镜像选择，我一般选择 ubuntu 18.04 desktop（安装点最小安装），后续的步骤有 有的服务器老，会出现起不来，要进入recovery模式去clean的问题 配置网卡 使用sudo passwd 输入密码 来更改密码 设置不自动更新 配置apt-get 源 安装必要的软件SSH，网络工具等 使用docker 一般服务器都有基础镜像 可以使用docker image ls 查看后 使用下列命令进入 docker run -itd --privileged -e “container=docker” -d -v /home/:/home/ -p 9999:9999 --name=容器名字 --gpus all 镜像名:镜像tag /bin/bash 这个命令对端口，目录进行的映射 当安装好环境后，使用 docker commit 容器id 镜像名字 保存镜像 但是如果镜像过多，可以使用 docker rmi 删除镜像， docker rm 删除容器 安装python 安装选择，一般选择 python 3.10 看需求 一定要先apt install 安装所有的python依赖 sudo apt install build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev libsqlite3-dev wget libbz2-dev liblzma-dev 去python 官网下载 tar包，使用tar -zxvf Python-xxx 解压缩命令后 直接 cd 进入目录 使用命令： ./configure make install 安装完毕后，使用 “ln -s 源文件 目标文件” 命令去构建符号软链接 然后进入 ~/.pip/pip.conf 配置 pip 源 安装Nvidia驱动，Cuda 可以使用命令直接安装，但一般会有很多坑 apt install nvidia-driver-525-server nvidia-fabricmanager-525 最好还是直接去下载官网的 对应驱动的 deb 安装程序进行安装 对于cuda也是安装deb包，由于已经安装了driver了，所以进去安装程序要避免安装的时候也安装driver，把安装选项上面的driver取消了，只安装toolkit 然后重启机器。 安装jupyter notebook 一切就绪后，可以使用命令 pip install jupyter 安装完毕后在工作目录，使用命令 nohup jupyter notebook –port 9999 –ip=0.0.0.0 –allow-root &gt; jupyter.log 2&gt;&amp;1 &amp; 这样可以把端口号透传出去，且让jupyter在服务器起来 验证 最后，可以开心的打开notebook import torch print(torch.cuda.device_count()) 进行验证","link":"/2024/05/13/python/jupyter%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"},{"title":"虚拟内存和物理内存","text":"虚拟内存可以是物理内存的1.5-3倍左右，为什么可以突破物理内存的限制呢？这个问题其实想了解很久了。 那必定是有个表格，来映射程序的虚拟内存到实际物理内存。 简单页表：类似数组，时间复杂度为 O(1)。但空间复杂度为数组的长度，即页的个数。32 位的内存地址为 4MB（= 2^20 * 4byte）= 4MB 左右，把32位的内存，前20位用来当成页号，后12位当作偏移量。想知道物理内存，只需要在这个页表查到相应的物理页号，再加上自己本身的偏移量，就可以知道实际物理地址。(PS: 1K byte=1KB，1 Million Byte等于4MB， 2^20 = 1048576) 但是一个程序就需要维护一个4mb页表，一部机器运行1000个程序都是常态，这样内存需要维护的页表为4GB。完全不可行 多级页表（页表树）：类似b+树，把20为的页表号分成4段，每段总共5个位。每个段对应一个页表，举个例子：00011, 10011, 00110, 11011, 偏移量根据第一段 00011 到4级页表，找到其3级页表（多个）根据第二段 10011 到其3级页表，找到对应的2级页表（多个）根据第三段 00110 到其2级页表，找到对应的1级页表（多个）根据第四段 11011 到其1级页表，找到对应的物理页表号，再加上偏移量。 一级的页表可以映射到 2^5 = 32 条记录，数组长度为32，同时每个元素4byte，那么就是128byte的长度。 323232*128byte一样的大小，但是由于只有一级页表会存储在内存里。 举个例子：校长的公文包里，每次只需要带上一张表，这张表记录了一千个班，每个班的班主任手机号。当校长想找某个学生时， 先根据学号，计算出他在哪个班。2.在表中找到这个班，里面有这个班的班主任手机号。3.打电话给班主任，班主任再送来他们班的一张表，表中记录了这个班每个学生的手机号。4.校长再根据学号，再这个班的班级表中找到这个学生和他的手机号。从此，校长的公文包只需要带上一张总表就行，需要找哪个学生时，就把对应班的班级表放到包里，比如校长今天只想找 3班和4班，那么公文包里最多放三张纸即可。(PS 这里我认为原文没讲清楚） 这个时候需要访问4次内存才能确定物理地址，也就是访问的次数增加了。但是由于，寻找虚拟地址和物理地址的访问，其实是有局部定律的，那么对于一个页而言，可以使用缓存机制。 地址变换高速缓冲(Translation-Lookaside Buffer，TLB)CPU 内封装了 MMU， 内存管理单元，和TLB，其中TLB分指令TLB（即ITLB）和数据TLB（DTLB），这些缓存已经存放了之前做地址转换的的时候结果，所以不需要麻烦的多此访问多级页表。","link":"/2024/05/13/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E5%92%8C%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98/"},{"title":"权限和linux的capabilities","text":"inux的capabilities有多少个？ 如何调用api来设置进程的capabilities？capability是线程的么？如何检测进程的对linux内核调用的检测？如果该进程有多个线程 strace还有用么？linux 进程通讯有多少种方式？ 权限最小化，是让系统更加安全的一条原则，就是让所有进程处于能正常运行，且特权和访问权限是最小的。 且权限最小化，是一个时间比较长的工作，需要对齐各个阶段的目标，才能把这件事做好。 目标当然是1 切换uid，2 降低cap 3 限制文件目录权限。 但是这里各个阶段都是这三个问题，uid&amp;gid，cap，文件目录权限互相糅杂在一起的。、 1 第一阶段，用户id的切换。 原来公司很多业务的进程，都是root起来的。 root是超级用户，需要把这些进程的用户切换成普通用户。 首先从三个地方分析，代码量，这里的代码量着实不多，就是第一从配置文件读取uid，有切换uid gid函数的直接切，没有就自己实现一个，切换uid gid函数到处都有参考代码。然后就是调用函数，切换。代码量至多100以内。 坑，切uid水很深，如果就以为完成了100行左右代码然后就可以切换成功，那就大错特错了。 坑1，切换uid从root到普通用户会导致进程的capablity的p位，e位，a位的丢失，一下子进程降权到几乎为0，系统肯定会挂。 如何解决？只能是使用prctl 函数，设置一个保住p位不丢失的flag，在切换以后马上把cap设置回来。 坑2，把cap设置回来，这里很多人思维都可能默认capablity是进程级别的，其实capablity是线程粒度，同一进程不同线程的cap都不一样，这个时候设置回来的时机要选对，首先是设置回来的时机最好选在进程main函数，开始阶段，什么都没做之前就设置回来。也就是要在主线程把用户uid，gid切换后，再马上把cap给设置回来。 如果在不考虑线程问题，在进程随意选个地方把cap设置回来会有大问题。 坑3，切换uid后，cap的设置问题，在笔者这个工作里，切换uid后设置cap是根据linux的系统的算法来实现的，不是简单的一个赋值行为。 P’(ambient) = (file is privileged) ? 0 : P(ambient) P'(permitted) = (P(inheritable) &amp; F(inheritable)) | (F(permitted) &amp; P(bounding)) | P'(ambient) P'(effective) = F(effective) ? P'(permitted) : P'(ambient) P'(inheritable) = P(inheritable) [i.e., unchanged] P'(bounding) = P(bounding) [i.e., unchanged] 这里我大白话解释一下，非常简单。就是一般设置cap，一般都是这样的一个场景： 管理进程（已经是普通用户）fork一个子进程，子进程然后切换uid，切后设置cap，然后exec（）可执行文件变成业务进程。此时父进程的p位，e位，a位，fork之后完整的给了子进程，子进程切uid后从普通用户到另一个普通用户则丢失e位，然后设置A位，接着执行exec 根据规则1，执行exec后， 我们的可执行文件没有设置文件cap（公司内部文档已说明文件cap是不能设上去），所以A位直接原封不动。 根据规则2，A位原封不动，和可执行文件的文件cap都为0，所以变成， (0 &amp; P-inheritable) | (0 &amp; P-bouding) | P-ambient = P-ambient ，所以P位就被此时A位给赋值了。 根据规则3, A为直接赋值给E位。 最后当你填满所有的坑，把读取uid，gid代码切换uid，gid的上库了，读cap，设置cap的机制也上库了，才能做第一步去root化。 第一阶段切换每个进程用户id，然后把每个进程的cap配高，先完成去root化，后续再慢慢降cap才是可行之路。不能一步登天会出问题。","link":"/2024/05/13/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux%E7%9A%84%E8%BF%9B%E7%A8%8Bcapability/"},{"title":"mini-gpt","text":"karpathy 的300行mini-gptkarpathy 300行实现了mini-gpt，是一个很好的学习范例。 1234567class NewGELU(nn.Module): &quot;&quot;&quot; Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415 &quot;&quot;&quot; def forward(self, x): return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0)))) ReLU被GELU这样更好的激活函数替代了。 12345678910111213141516171819202122232425262728293031323334353637383940414243class CausalSelfAttention(nn.Module): &quot;&quot;&quot; A vanilla multi-head masked self-attention layer with a projection at the end. It is possible to use torch.nn.MultiheadAttention here but I am including an explicit implementation here to show that there is nothing too scary here. &quot;&quot;&quot; def __init__(self, config): super().__init__() assert config.n_embd % config.n_head == 0 # key, query, value projections for all heads, but in a batch self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd) # output projection self.c_proj = nn.Linear(config.n_embd, config.n_embd) # regularization self.attn_dropout = nn.Dropout(config.attn_pdrop) self.resid_dropout = nn.Dropout(config.resid_pdrop) # causal mask to ensure that attention is only applied to the left in the input sequence self.register_buffer(&quot;bias&quot;, torch.tril(torch.ones(config.block_size, config.block_size)) .view(1, 1, config.block_size, config.block_size)) self.n_head = config.n_head self.n_embd = config.n_embd def forward(self, x): B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd) # calculate query, key, values for all heads in batch and move head forward to be the batch dim q, k ,v = self.c_attn(x).split(self.n_embd, dim=2) k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs) q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs) v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs) # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh, T, T) att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf')) att = F.softmax(att, dim=-1) att = self.attn_dropout(att) y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs) y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side # output projection y = self.resid_dropout(self.c_proj(y)) return y 从karpathy的代码告诉我们，要把注意力机制从零开始理清楚，So，let’s Go! 注意力机制注意力机制的发展追溯至19世纪90年代，把注意力分类成自主的和非自主的。 非自主性注意力一个人刷朋友圈，看到通过机器学习专业的同班心仪对象post酒吧喝酒的自拍，非自住的注意力就被drag到这里，产生了一系列想法，比如去喝点，或者他/她和谁在喝？非自主性注意力正在发挥效果。 自主性注意力这个时候，受自我驱动力的影响，脑子提示这个时候有些阅读任务没有完成，于是转头看向床头那本尚未翻完的《深度学习》，自主性注意力正在发挥效果。 非自主与自主性相互融合与碰撞最后，在两种注意力的对撞和冲击下，这个人带着手机出门去那个bar和对象喝一杯，在车上观看着《深度学习》，思索着这一切，到了bar后和他/她来了一场关于机器学习的哲学辩论。在这个不恰当的例子里， query是自我驱动力（自主性注意力），key是目之所视的一切(非自主注意力）,query和key的结合，通过脑皮层，最终得出了这个值（”why not just go for a drink while reading at texi and think abou it) 最简单的注意机制，注意力汇聚机制注意力汇聚机制是最简单的注意力机制。图：（todo: 已经画好了） 也就是对于一个query，通过a函数遍历每个k，a函数的功能在于找到比较相似的key值。a(q, k1) + a(q, k2) + a(q, k3) + a(q, k4) …. + a(q, kn), 这里的q和k的到的a值就是注意力权重值也就是attention weight。在上述的例子里：key是非自主注意力（也就是躺床上看到的一切，朋友圈里的同个机器学习专业的心仪对象k1，床边的书k2，桌子上的台灯k3，衣柜里的袜子k4）q是自主注意力（也就是需要翻完深度学习这本书） 注意力评分函数这个a函数的特点是：如果q和k越接近，则得到的值越大，反之越小。这类注意力评分函数有许多选择先，按下不表。 a(q,k1) = 0.3 这个q和k1还是有点关系的，毕竟是同一个专业。a(q,k2) = 0.8 关系很高，这本书是专业书籍。a(q,k3) = 0 毫无关系a(q,k4) = 0 毫无关系 最后k1对应的v1，肯定是date with her, k2对应的v2是把这本书学习完。F(q) = 0.3 x v1 + 0.8 x v2 混合而成最终的行为。 引入w参数到注意力评分函数但是下一步参数w，因为训练就是迭代w到自己满意的地步。公式： 引入参数后特点是，w是可以学习的，可以变化的，举个例子，经过一段时间的date，发现这个同系的crush不太适合自己，对其已经下头。而且频繁去bar，已经把自己喝坏了。所以w值正在下降。 经过一段时间后：a(q,k1) x w1 = 0.01 w1发挥了作用，下头了兄弟。a(q,k2) x w2 = 0.9 关系很高，依旧很高，这本书是专业书籍，而且觉得自己更加必须看了。a(q,k3) x w3 = 0 毫无关系a(q,k4) x w4 = 0 毫无关系 F(q) = 0.01 x v1 + 0.8 x v2 混合而成的行为就像，已经对这个同专业的crush下头，不想和对方进行类似的思辨和喝酒了。最终，pyq就小小点个赞，然后迅速放下手机，拿起床头边的这本书，开始翻起来。","link":"/2024/04/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/300%E8%A1%8C%E7%9A%84mini-gpt/"},{"title":"chatglm3部署","text":"下载模型 ，先创建目录 chatglm3 先部署，用egal-get 下载模型文件, 小文件用浏览器下载，大文件用egal-get下载，这个时候要验证下载的SHA256 在window里面，调用 certutil -hashfile 文件名 SHA256 进行查看，查看完了，都没有问题 环境部署 接下来是部署，部署很简单，上去运行gunicorn，或者直接运行官方的py也行，因为我只想研究function call，所以不想在部署上花时间，所以就是普通的部署就行了。 如果要达到普通的部署，需要pycharm进入服务器，部署pycharm 查看pycharm 远程服务： https://blog.csdn.net/qq_44614026/article/details/118109014 很简单，就是使用pycharm的tool，配置服务器用户名密码，然后最重要的配置mapping，把服务器某个目录映射到本机某个目录下面，然后过一会在project那边上传代码，或者下载代码即可。 修改路径 怎么进行普通的部署？，当前这个部署失败是因为path不对 MODEL_PATH = os.environ.get('MODEL_PATH', 'THUDM/chatglm3-6b') PT_PATH = os.environ.get('PT_PATH', None) TOKENIZER_PATH = os.environ.get(&quot;TOKENIZER_PATH&quot;, MODEL_PATH) 这段代码通过os.environ.get获取了三个变量，这里做了什么 其实就是os.envrion.get(xxx, default), 如果没有这类环境变量就直接返回默认值 于是设置了模型所在的目录，作为MODEL_PATH 把TOKENIZER_PATH也设置为模型所在的目录 在composite_demo运行streamlit run main.py即可 依赖问题 但是有可能stream版本不对，需要运行 pip install streamlit==版本号，下载一个最新的streamlit的版本。 先普通的部署，对tool_using进行测试，测试问题得等等看，完成了部署了。 量化部署 model = AutoModel.from_pretrained(&quot;THUDM/chatglm3-6b&quot;,trust_remote_code=True).quantize(4).cuda() 大概6b的参数占的内存*2 + 1， 大概就是13GB 如果没有量化的话，那就是17GB 多卡且指定卡部署 from utils import load_model_on_gpus model = load_model_on_gpus(&quot;THUDM/chatglm3-6b&quot;, num_gpus=2) 你也可以传入 device_map 参数来自己指定，在chatglm3的openai_api_demo的utils.py文件里，有一个函数auto_configure_device_map，里面的device_map是一个字典 device_map = { 'transformer.embedding.word_embeddings': 0, 'transformer.encoder.final_layernorm': 0, 'transformer.output_layer': 0, 'transformer.rotary_pos_emb': 0, 'lm_head': 0 } used = 2 gpu_target = 0 for i in range(num_trans_layers): if used &gt;= per_gpu_layers: gpu_target += 1 used = 0 assert gpu_target &lt; num_gpus device_map[f'transformer.encoder.layers.{i}'] = gpu_target used += 1 在这里可以选择层数与gpu卡号进行绑定，其中的transformer.encoder.layers.0,1,2,3… 可以设置到gpu不同的卡号上面。","link":"/2024/05/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chatglm3%E9%83%A8%E7%BD%B2/"},{"title":"number gpt","text":"skip了环境准备，数据下载和数据清洗 1 先从模型开始看：https://huggingface.co/microsoft/phi-2/blob/main/modeling_phi.py这里需要的是，暂停一下，思考一下encoderLayer怎么变成可以训练的语言模型？ 2 这里收集了大量的信息https://github.com/PolymathicAI/xVal/blob/main/xval/numformer.pyxval基本代码里，沿用了transformer的decode layer，并且定义了两个简单得线性层来分别预测文字token和数字token。 3 mingpt : https://github.com/karpathy/minGPT/blob/master/mingpt/model.pykarpathy 300行实现min gpt，模仿gpt2，算是一个1.5b的小模型，揭示了如何从decoder layer构造一个语言模型 4 phi-2以及phi-0.2实验版：https://github.com/charent/Phi2-mini-Chinesehttps://github.com/charent/ChatLM-mini-Chinesehttps://huggingface.co/microsoft/phi-2/blob/main/modeling_phi.pyphi-2模型的架构代码十分的复杂，基本无法很快速了解，如何从decoder layer构造一个语言模型即使是phi2-mini-chinese, 也是沿用了hugging face里面得关于phi2的架构，不过更改了模型参数，down to 2.7b的参数量。chatlm-mini-chinese更是直接沿用了hugging face里面关于T5的架构。这两个项目的模型层面是黑盒子，无从得知。 5 路线图：https://mp.weixin.qq.com/s/JAdHriuLW4meEGtCquPBZw基本讲述了从transformer到gpt的发展历程，是一个不错的学习工具。从transformer到bert，到T5, 结果被GPT-1, GPT-2, GPT-3给取代了。 6 深度学习圣经：https://zh.d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-bert.html过去一段时间，一直一直在读的，学习的基础知识。 综上所述：我认为可以得出结论，我需要学习的是，mingpt, 以及如何使用xval的概念进入mingpt里面。如何进行测试？就是根据phi2-mini-chinese, 的训练pipe line 玩一下。","link":"/2024/04/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/number-gpt/"},{"title":"gpt4o观测","text":"在这次openai发布会，发布了gpt4o，简单的说，openai的目标一直都是朝着《太空漫游2012》去的，所以更强调交互性，给我一种感觉就是他们希望完美通过图灵测试，让人完全感觉不到这个模型是一个工具而是一个人，当然，人在某个程度上也是一个工具。 要点： 实时多模态能力， 2x 速度 50% 偏移 5x速度限制 第一个关键场景，实时对话能力，在手机端启动，以前的voice mode是不可以被打断的，这次是可以打断。而且像rtos那样实时回复，不需要等待。而且可以生成富有情感语音。甚至用一种唱歌的语气也可以。 第二个关键场景，实时的洞察世界的能力，演示者拿出一个纸片，然后写一个简单的方程式，然后打开摄像头，然后实时展示了它的逻辑能力。 第三个关键场景，在演示者开始编程的时候，然后他妈的，它就实时看着屏幕，然后把屏幕上的信息输入到模型里面去，进行推理，然后人们跟他交互屏幕上的信息。 第四个关键场景在于这个实时翻译的能力，也有点bug。 第五个关键场景在于实时的摄像中它可以捕捉到画面中的笑脸。这是处理实时数据流的能力。 猜想它的多模态能力，我认为是基于一种万物皆可tokenize的思维。比如一个人正在开车，听着音乐。1 他的视网膜实时接受5.7亿的像素，也就是可以理解为5.7亿的token在20-24毫秒内流入大脑2 听觉方面的则是人类以20-24毫秒接收20-20000赫兹的声音token进入大脑 任何信号都可以tokenize，任何tokenize的东西都可以通过训练得到大模型(多模态)，再用moe，蒸馏，剪纸，等各种方式加速和降低推理成本，速度快到一定的程度到达人类反应时间300ms的时候，那就是实时。","link":"/2024/05/15/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%8E%A9%E7%8E%A9gpt4o/"},{"title":"并查集算法","text":"对于leetcode 323题目，还有类似的题目。 初始一开始想到的都是DFS， BFS，之类的暴力搜索，可是这样的作法是O(E+V)时间复杂度。有一个特别好的通用的手法去实现这类，连通分量的问题。 从代码上看, 使用make_unique声明一个UnionFind对象后，只需要针对每个边进行遍历和合并，即可得到整体的连通分量。 12345678910class Solution {public: int countComponents(int n, vector&lt;vector&lt;int&gt;&gt;&amp; edges) { auto ufind = make_unique&lt;UnionFind&gt;(n); for(auto const&amp; edge: edges){ ufind-&gt;unionTwo(edge[0],edge[1]); } return ufind-&gt;cnt; }}; 下面是整个UnionFind类的实现 12345678910111213141516171819202122232425262728293031323334353637383940class UnionFind {public: vector&lt;int&gt; fa; vector&lt;int&gt; rank; //定制化cnt int cnt; UnionFind(int n){ rank.assign(n,1); cnt=n; for(int i=0;i&lt;n;i++){ fa.push_back(i); } } //查询x y的父节点，递归查询，当查询 inline int find(int x){ if(x==fa[x]) return x; return find(fa[x]); } //当合并的时候，需要对比rank大小， inline void unionTwo(int x, int y){ int fa_x = find(x); int fa_y = find(y); if(rank[fa_x]&lt;=rank[fa_y]){ //合并小的到大的 if(fa_x == fa[fa_x] &amp;&amp; fa_x != fa_y){ cnt--; } fa[fa_x] = fa_y; }else{ if(fa_y == fa[fa_y] &amp;&amp; fa_x != fa_y){ cnt--; } fa[fa_y] = fa_x; } if(rank[fa_x]==rank[fa_y] &amp;&amp; fa_x!=fa_y) rank[fa_y]++; }}; 实现分析father数组和rank数组点1连着点2， 点2连着点3， 那么就是这样 1-2-3点4连着点5， 点5连着点6， 那么就是这样 4-5-6father数组的意味就是希望记录每个点的父节点是谁，但是不是直接父节点，而是祖父节点，比如 father(3) = 1, 而非等于2。所以对于这个案例： father数组index: (1， 2，3，4，5，6）father数组value: (1， 1，1，4，4，4） 从这个father数组可以看得出是类似一个树状的，这颗树可以是 1-2-3 这样的，rank此时对于根节点1而言就是3，因为高度为3。也可以是 1-(2,3), 此时对于根节点1的rank为2，高度为2。当然树的高度越小越好。 合并Union操作所以对于Union操作，需要找到两个节点的祖父节点，所以又有一个简单的find动作。找到了两个祖父节点x，y之后，查看rank的大小， 比如知道了x的rank大小为2，比y的rank为1大。那么可以断定，x就是 1-2 这样的树状结构，y为1那就是单节点，只需要合并进去变成 1-(2,3) 这样的结构就行了。那么如果是x的rank大小为3，比y的rank为2大呢？合并进去也并不会影响其x的高度。 合并改变rank的情况只有一种情况会合并的时候改变rank结构，就是两个节点的rank都是一样的，比如都是1，都是单节点，合并后变成 x-y， x的rank变成了2两个rank节点都是2，x-y, u-v, 合并后，肯定是x - (y,u) - v ， 那么x的rank也是会加1。 find操作本质上是个递归找到祖父的节点的操作。","link":"/2024/05/13/%E7%AE%97%E6%B3%95/%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%AE%97%E6%B3%95/"},{"title":"语言模型的数学能力实验","text":"具体技术来自xval：https://github.com/PolymathicAI/xVal/blob/main/xval_demo.ipynb 语言模型的各项能力各项能力的提升是趋势，月之暗面和claude的超长文本能力，大海捞针。多模态能力，cot能力，数学能力，代码能力，总结能力， 等等。 其中让我比较关心的是数学能力，举个例子，一个人在路上步行速度为4km/h，然后经给123分钟后，到哪里了？对于这里的4， 123, 在人类视角里分别代表一个数字, 只有涉及到计算的时候，才单独抽取出来进行计算。但是当前普遍语言模型的做法，不太符合真正高效的处理。 一个人在路上步行速度为4km/h，然后经给123分钟后，到哪里了？1 普遍的模型：-&gt; 【“一个”，“人”，“在”，“路上”，“步行”，“速度”，“为”，“4”，“km/h”，“然后”，“经给“，”123“，“分钟“，”后“，”到哪里”，“了”】 2 更好的模型：-&gt; 【“一个”，“人”，“在”，“路上”，“步行”，“速度”，“为”，“【num】”，“km/h”，“然后”，“经给“，”【num】“，“分钟“，”后“，”到哪里”，“了”】-&gt; 【-，-，-，-，-，-，-，4，-，-，-，123，-，-，-，-】映射到-&gt; h-embed: 【】也就是在tokenizer的时候，需要把数字不再认定是离散的单个字符，而是通过一个类似bos，eos的特殊token，命名为num来取代。此举的目的主要是将离散的数字token变成连续的，更加符合现实情况，可以提高数学能力，我认为数学能力，和对数字的敏感性提高后，必然带来推理能力的提高。 这是xval的基本思想，在tokenize的时候对数字特殊处理，然后使用一个更改过的numformer，是transformer的变体，以此来提高对数字的掌控力，可惜这个xval并没有把事情完整的完成。基本测试完numformer就完事了。 所以需要进一步进行实验，取phi小模型，分别两组，一个是没有进行numformer改造的，一个是引入numformer改造的。然后开始训练引入numformer改造的，预料的选择和配比和原来的完全一致。最后进行测试。 对于xval的了解文件结构是xval_demo.ipynb： 主要是使用numformer，和算法的实现，然后进行tokenizer训练，和使用numformer进行seq2seq的训练和预测。tokenizer.json：tokenizer本体tokenize-dataset.py：训练tokenizer的数据准备env.yaml：conda python环境 还有xval包init.py：初始化文件analyze.py： ？make_tokenizer.py: ？numformer.py: 应该是numformer的实现，transformer的变体。preprocess.py: 预处理文件 对于xval的了解应该可以进一步学会如何将transformer改造成numformer，如何使用该token训练方式、也就是xval_demo.ipynb的内容。 1 首先导入库 2 查看数据内容和结构可以看到这些数据就是一叠一叠字符串，这里字符串是json格式的。 1ds['text'][0][:n_characters] 总共12w5k个 3 tokenize（最重要的一环之一）因为这里处理的数据比较特殊，是一个有一个json格式的，所以只需要提取key值作为tokens。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# Building a tokenizer with the extracted keys as individual tokens.make_tokenizer.make_tokenizer( save_file=&quot;./tokenizer.json&quot;, efficient_json=True, sample_keys=sample_keys)````这个函数是xval实现的，sample_keys就是数据集合里面所有出现的key值。```pythontokenizer = PreTrainedTokenizerFast( tokenizer_file=&quot;./tokenizer.json&quot;, bos_token=&quot;[END]&quot;, # beginning of sentence eos_token=&quot;[END]&quot;, # end of sentence mask_token=&quot;[MASK]&quot;, # mask token pad_token=&quot;[PAD]&quot;, # pad token)````就这样一个tokenizer就搞定了。- 4 接下来使用该tokenizer试试 上面的tokenizer.json已经出现了【NUM】的token了，所以实际该tokenizer已经可以用了。```pythontokenized_x = preprocess.tokenize_fnc(ds['text'][0][:100], tokenizer)print('input_ids:', tokenized_x['input_ids'])print('numbers:', tokenized_x['numbers'])````对于一个句子：{'description':{'planet0':{'m':+1.31e+0,'a':+1.94e+0,'e':+1.90e+0},'plane会形成两个长度一致数组：input_ids:【4, 18, 4, 26, 4, 21, 3, 8, 20, 3, 8, 16, 3, 5, 8】numbers: 【1. 1. 1. 1. 1. 1. 1.3125 1. 1. 1.944，1. 1. 1.897 1. 1.】其中token_id为3的其实是数字，对应到下面numbers数组上，会有小数存起来。```pythonprint(&quot;\\nStarting tokenization...&quot;)tokenize_lambda = lambda x: preprocess.tokenize_fnc(x, tokenizer)tokenized_ds = ds.map( tokenize_lambda, batched=False, num_proc=30, remove_columns=[&quot;text&quot;], load_from_cache_file=False,)tokenized_ds输出：Dataset({ features: ['input_ids', 'numbers', 'len'], num_rows: 125000}) 对125000个json进行处理后得到这么一个tokenize_ds 5 对照实验后面xval为了对比，从而tokenize方式进行了对比P10(一个数字5个token) (词表大小：28)P1000(一个数字3个token) (词表大小：918)P1999(一个数字2个token) (词表大小：1816)FP15(一个数字1个token) (词表大小：28800)XVAL(一个数字1个token) (词表大小：1) 6 训练 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108import torchfrom torch import optimfrom datasets import DatasetDictfrom torch.utils.data import DataLoaderimport torch.nn.functional as Ffrom tqdm import tqdmimport matplotlib.pyplot as pltimport pandas as pdfrom xval import numformer### Define model# The vocab_size is the number of different tokens in the tokenizer.# context length is the maximum sequence size. model = numformer.Numformer(vocab_size=27, nhead=3, num_layers=3, d_model=384, dim_feedforward=1536, context_length=955).cuda()lr = 1e-4weight_decay = 0.01optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)### Load the tokenizer tokenizer_path = &quot;./tokenizer.json&quot;tokenizer = PreTrainedTokenizerFast( tokenizer_file=tokenizer_path, bos_token=&quot;[END]&quot;, eos_token=&quot;[END]&quot;, mask_token=&quot;[MASK]&quot;, pad_token=&quot;[PAD]&quot;,)pad_token_id = tokenizer.pad_token_idnum_token_id = tokenizer.convert_tokens_to_ids(&quot;[NUM]&quot;)mask_token_id = tokenizer.mask_token_idmlm_probability = 0.3epochs = 10### Load tokenized datasets dataset_path = &quot;./data/tokenized_ds_xval&quot;tokenized_ds = DatasetDict.load_from_disk(dataset_path)# Define the masked xVal collator which takes samples of unequal length and masks out both the token_ids and the numbers.collator = numformer.define_masked_num_collator(pad_token_id, mask_token_id, mlm_probability)train_loader = DataLoader( tokenized_ds[&quot;train&quot;], batch_size=32, shuffle=True, collate_fn=collator,)### Run training looploss_hist = []loss_mlm_hist = []loss_num_hist = []max_n_batches = 100 # without capping the number of batches, training takes many hourstry: for e in tqdm(range(epochs)): n_batches = 0 for batch in train_loader: if n_batches &gt; max_n_batches: break logit_preds, num_preds = model(batch[&quot;x&quot;].cuda(), batch[&quot;x_num&quot;].cuda()) with torch.autocast(device_type=&quot;cuda&quot;): loss_mlm = F.cross_entropy( logit_preds.view(-1, logit_preds.size(-1)), batch[&quot;y&quot;].cuda().view(-1), ignore_index=-100, reduction=&quot;mean&quot;, ) num_mask = batch['y']==num_token_id loss_num = F.mse_loss( num_preds[num_mask], batch[&quot;y_num&quot;][num_mask].view(-1,1).cuda(), reduction=&quot;mean&quot;, ) loss = loss_mlm + loss_num optimizer.zero_grad() loss.backward() optimizer.step() loss_hist.append(loss.item()) loss_mlm_hist.append(loss_mlm.item()) loss_num_hist.append(loss_num.item()) n_batches += 1 # calculate the running average of the losses try: loss_avg = 0.99*loss_avg + 0.01*loss.item() loss_mlm_avg = 0.99*loss_mlm_avg + 0.01*loss_mlm.item() loss_num_avg = 0.99*loss_num_avg + 0.01*loss_num.item() except: loss_avg = loss.item() loss_mlm_avg = loss_mlm.item() loss_num_avg = loss_num.item() ### Save checkpoint at each epoch checkpoint = { &quot;model&quot;: model.state_dict(), &quot;optimizer&quot;: optimizer.state_dict(), &quot;loss&quot;: loss_avg, &quot;loss_hist&quot;: loss_hist, &quot;loss_mlm_hist&quot;: loss_mlm_hist, &quot;loss_num_hist&quot;: loss_num_hist, } torch.save(checkpoint, &quot;./ckpt.pt&quot;) print(f&quot;Epoch #{e}: loss_mlm = {loss_mlm_avg:.3f}; loss_num = {loss_num_avg:.3f}; loss_total = {loss_avg:.3f}&quot;)except KeyboardInterrupt: print('Interrupted') 这份训练代码不理解的是loss_hist, loss_mlm_hist, loss_num_hist,tqdm的用法，logit_preds, num_preds, torch.autocast(),cross_entropy, mse_loss, num_maskloss = loss_mlm+loss_num （这个是唯一理解的）后续还有一系列不理解的变量和模块，但是check_point是可以理解的，每个epoch都可以。 评估模型基本所有的预测函数，都给封装到xval的 123masked_sample = analyze.mask_numbers(sample, tokenizer, n_list=i) out = analyze.predict(model, masked_sample, device) analyze.predict_numbers(model, sample, tokenizer, n_list=number_ids[:3], device='cuda', all_at_once=False) 这些预测函数基本都是为了预测单个被mask的number。","link":"/2024/04/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%95%B0%E5%AD%A6%E8%83%BD%E5%8A%9B/"},{"title":"向量数据库","text":"向量数据库的存储，主要是用来存储向量，这里向量可以是任何东西。 比如说： 歌曲：【歌手，歌名，歌曲的评论数，歌曲的风格，歌曲的上市的时间….】这样把一首歌曲的，各种特征值，汇聚成一个向量。然后就可以存库，并且已知某个人喜欢一首歌，就可以搜索相似的歌曲推荐给他。 词组：【词向量值】把这样一个词向量存库后，将文字向量化存储，就可以根据用户的提问，搜索到近似的知识，近似的问题。 声音：【声音向量】把一段声音向量化存库，是语音识别的常规操作 图片：【图向量】如果将图片向量化存储，就可以搜索相似的图片。 视频：【音视频向量】将视频向量化存储，可以搜索到相似的视频。 而且向量的距离也预示着一种关系，比如将“警察”和“小偷”，“猫”和“老鼠”， 存向量数据库后，警察到小偷的向量距离，和猫与老鼠的向量距离接近。 传统数据库是不适合存储向量数据的，所以需要引入传统的数据库。传统的数据库当然是sql这个时间点，正好适合看看数据库选型，之前看过微博一张图，讲数据库的， HBase特点:基于Hadoop的分布式列存储系统。适合处理大规模数据集。支持高并发读写和强一致性。适用场景:大数据分析和存储。需要快速随机访问大量数据的场景。 MySQL特点:关系型数据库，使用SQL查询语言。支持事务、复杂查询和关系数据模型。适用场景:传统的Web应用。需要复杂查询和事务支持的场景。 TiDB特点:兼容MySQL协议的分布式数据库。支持水平扩展和强一致性。设计用于处理HTAP（混合事务/分析处理）工作负载。适用场景:需要MySQL兼容性和水平扩展能力的应用。实时分析和事务处理。 Redis特点:高性能的内存数据结构存储，支持多种数据结构。通常用作缓存和消息队列。适用场景:需要高速缓存的应用。实时消息系统。 MongoDB特点:面向文档的NoSQL数据库。支持灵活的数据模型和丰富的查询语言。适用场景:需要存储半结构化或无结构化数据的应用。快速开发和迭代的项目。 Kafka特点:分布式流处理平台。高吞吐量、可扩展、持久化消息系统。适用场景:大数据实时处理和分析。构建可靠的数据管道和流应用。 消息队列（如RabbitMQ）特点:支持异步消息传递和解耦应用组件。提供可靠的消息传递保证。适用场景:需要解耦微服务架构的应用。异步处理和背压管理。这样一看确实向量数据库独特的向量存储和检索是需要针对化设计的。 向量数据库存的是向量，主要场景也是找到相似的向量集合，所以和传统数据库的精确查找不太一样，这样的检索方式一般是最近邻(Nearest Neighbors)的方式 第一是检索： 暴力搜索总是最准的，可以直接算向量的欧式距离，来暴力搜索，搜索的质量很好，但是速度极慢。 聚类算法是一种优化，定下几个点，然后不断的优化迭代，让这几点代表了几个群体。然后检索的时候，只需要找这几个中心点和检索向量的距离，但是如果实际这样会不准确。一般来时聚类个数可以提升来提升准确性，但是聚类个数提升意味着速度下降，当聚类个数提升到存的向量个数，那么就是退化成暴力搜索了。 还有位置敏感哈希函数，当一个向量点经给哈希函数后，相似的向量会因为这个哈希函数得到相同的哈希值，这样就只需要去该桶检索即可。这样位置敏感哈希函数，可以使用随机超平面的方式得到其编码。但是随机超平面的方式也是有问题的。 所以对位置敏感哈希函数，会进行一个分段措施，会对向量点的段进行检索，第一段会去第一段区的桶内检索，第二段会去第二段区的通内检索。 第二是内存：当有10000w个向量，每个向量维度为128的时候，就占有了4.7GB了 可以把一个向量点，周围的相似点都把它替代为质心点，这样的有损压缩，实际上压缩率是跟质心数目有关的。落地实现是用向量和质心编码号进行关联，然后维持一个小码本，码本是编码号和对应的向量值。这个过程就是量化。 如果像两个的分布非常不集中，很稀疏，那么需要很多质心，越高维度，则数据越稀疏，这样需要的质心非常多。128维度的向量需要的质心数目为 2^64。但是依旧可以用分段来解决这些问题，将128维度分割成8个段，每个段只有16维度的向量值。这样8个子空间的码本其实相当小，这个方式也叫PQ，积量化。 PQ处理完后，4.7GB就到了70多mb。 这里需要引入一个6人理论，就是美国总统和一个任何人都是连接不高于6。所以如果遍历向量的时候，使用这样的连接来进行检索。那么检索速度为很快。这样的搜索方式叫做NSW（导航小世界） 这里需要进行手动帮这些节点建立连接1 两点很近，则需要建立连接2 每个点都有连接3 12原则满足的情况下建立的连接需要少！ 如何建立这个图是最关键的点，可以把点随机放回图，然后建立连接，在一开始这样的点分布会很稀疏。这样可以建立一个先快后慢的检索。 后面的人又演进了一个分层级的，HNSW，也就一层一层都有不同点，最上层的点是很稀疏，连接长度很长，每个层级都是按照连接长度来建立的，这样可以稳定的进行先快后慢的搜索方式。这样的方式比普通的NSW好，但是维护的内存高。","link":"/2024/05/13/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"title":"dify实践","text":"dify是个很好解决agent的诉求的好工具，同时也能兼顾工作流诉求，我现在也没有什么创建agent和workflow的欲望，我们都需要dify，所以部署吧，尽量会从0开始， 问题1： linux还是我本地mac的环境，进行 git clone的操作都是非常慢，而且还会报错。解决办法是挂代理之类的，或者直接下载。 问题2：docker部署的一系列问题12// 安装docker apt install docker.io 1234567891011121314//安装完docker后，需要配置docker hub的代理在这个 /etc/docker/daemon.json 配置如下也可以使用网易的代理 https://hub-mirror.c.163.com{ &quot;registry-mirrors&quot;: [&quot;https://a90tkz28.mirror.aliyuncs.com&quot;]}$ sudo systemctl daemon-reload$ sudo systemctl restart docker//最后使用docker info来查看$ docker info 1234567891011121314//安装完docker后，需要配置docker http代理sudo mkdir -p /etc/systemd/system/docker.service.d[Service]Environment=&quot;HTTP_PROXY=http://proxy.example.com:8080/&quot;Environment=&quot;HTTPS_PROXY=http://proxy.example.com:8080/&quot;Environment=&quot;NO_PROXY=localhost,127.0.0.1,.example.com&quot;$ sudo systemctl daemon-reload$ sudo systemctl restart docker//最后使用docker info来查看$ docker info 12//最后使用 docker search nginx 来进行测试docker search nginx 12//然后到dify的docker目录下运行docker-compose up -d","link":"/2024/05/14/ai%E5%B7%A5%E7%A8%8B/dify/"},{"title":"投资型llm","text":"RAG系统已经趋向成熟，几乎任何一个玩llm的人都知道，向量数据库，文档分割，向量文段召回，上下文学习。作者在偶然发现了这篇有趣的论文，这个LLM 打败华尔街的论文，也是基于这个系统。但是不一样的是，他们的玩法更加垂直领域，而且多次使用了LLM的 ”总结“ 功能，融合RAG系统里面去，本文的就是通过这篇论文，探索LLM-RAG的未来发展趋势。 在投资领域，该论文认为LLM表现比人类更好，更稳定，没有偏见，而且很多大机构比如高盛，和摩根大通已经开始利用LLM加强业务，但是应该如何做的呢？ 如果设想一下一个llm驱动的投资AI，会变成什么样？ 1 集成LLM作为底座到量化投资领域 2 可对输出的投资策略进行解释 3 对B端公司，团队可用，对C端个人散户也可用 4 比人类更加稳健，且具有更好的回报率 5 独立且不依附任何集团，不为任何金融集团背书，绝对的客观公正 基础架构 基本架构图如下 总共使用了4个独立的信息来源，底座也是基于api的RAG系统，分别是 （下文提及的RAG系统，指的是通过api获取数据，然后作为上下文输入给模型，进行信息总结，总结后入库，定期更新） 新闻数据总结RAG系统 基本总结RAG系统（基本分析指的是，上市公司公开的财报信息等） 宏观经济总结RAG系统 (宏观经济分析是，类似黑石，高盛，这样的权威的集团给出的经济报告) 动态股价RAG系统 最后把所有的子系统的输出，汇聚到一个”数据库“ bucket里面，然后对数据进行提取，再输入给强大如GPT4，进行step by step的总结，把系统行为收敛到三个行为，并且给出解释 持有股票 卖出股票 买入股票 新闻数据总结RAG系统 该论文认为，要模拟一个投资团队对新闻的处理行为。 他们把新闻分为三种： 对市场有短期影响的新闻 （比如某终端公司的发布会新闻） 对市场有长期影响的新闻 （比如美联储和央银的一些措施新闻) 对市场毫无影响的新闻 如下图： 该模块会通过api调用获取各种经济新闻，然后对返回的Json做数据预处理。这里预处理的目的在于把新闻的主要信息填充到预设好的 one-shot prompt 里给llm进行上下文推理，让llm对这些日更新闻做总结，进而存储到向量数据库里。 这里比较难以理解的是PN，PN其实代表的是月度新闻，用来捕捉公司的现状和内在。 月度新闻会把日新闻集成到一起，但是如何丢弃无关新闻，老旧新闻，以及集成后好处有哪些，论文没说。 如何集成？ 当月的PN是来自PN-1，也就是上个月的PN，同时还有30天内的每日新闻。 PN(this month) = PN (last month) + Daily News(30 days) 而且这里PN可以设置为，一个月，双周，视情况而定。 举了一个苹果公司的例子，每天都有非常多信息流涌入：比如iphone15的发部，M3芯片，但是销售速度下降，这些信息会汇总为到一起, 与上个月的月度信息汇总一起，按照上述公式进行迭代。 基本信息总结RAG系统 和上文一样的，通过api获取财报之类的信息，处理数据：比如把数据里的各种0替换为百万和亿，提取关键信息等。 prompt是有严格的格式的，比如盈利性，负债，回报率等财报性质相关的数据。 预处理阶段把财报信息填充入prompt。并且和前两个季度对比。llm会敏锐的发现财务的变化，还是和上文一样，这个信息迭代是通过总结上一个，或者上两个信息体，进行递进。（有点像斐波那契数列） 这个summarize promopt故意设置为比较没有偏见的，尤其是要客观，论文强调了一点，prompt的设计要避免给出直接的投资决策。 新闻和基本信息的总结如下图： 动态股价总结RAG系统 这个系统依旧会通过api获取实时的股价数据，而这里关键在于这里的prompt，这里prompt要填写目标公司的股价信息外，还有加入5个同类公司的股价进去，作为输入给大模型。 但是如何根据目标公司找到5个同类公司，这里论文给的是，根据公司的描述信息进行向量召回，算法如下：（本质上就是一个RAG） 最后把这些股价各个维度信息输入到预设好的prompt，给gpt4后进行总结。这种方式，somehow也是常见投资者进行同类分析的方式。 当然还有更高级的对冲分析，这些比较复杂，所以论文应该是简单省略。 宏观经济分析总结 宏观经济的分析，这类分析，比如疫情，和战争对市场是会很大的影响。 这类数据来源于，类似摩根大通，和高盛，ubs，黑石，世界银行之类的机构的投资报告，采取双周制度更新的机制。 简单的说，还是总结，就是把这些报告信息进行预处理，填入prmopt， 使用gpt4进行总结，最后存库，双周更新一次。 决策阶段 这是最后一个阶段，决定某在一个“赌场” ：）比如 A股，美股，日股里的一只股票是继续持有，还是加仓，还是卖掉。 此时已经有了四大信息了 N,F,P,M,（上文已经赘述） 就差一个函数来决定 F(N,F,P,M)来进行决策，gpt4 此时会扮演这个投资专家角色(promopt)，并且还要step by step推理，给出 {卖出，买入，持有} 的返回值。 并且要对做出的决定给与解释。 prmopt 格式如下： 总结： 后续论文还给出了股票排行榜之类的做法，但是基础架构就是介绍完了，还有实验数据论文声称自己得到一个很好回报率，细节不赘述。 底层技术实现底层实现是用了 langchain 框架 gpt4 的 api vectorbt pro 一个量化交易的python库","link":"/2024/05/13/ai%E5%B7%A5%E7%A8%8B/%E6%8A%95%E8%B5%84%E5%9E%8Bllm/"},{"title":"多头注意力","text":"自注意力的意思是，query，key，value都是同一个X。说明一个词语会咨询所有其他的词元，看其相似度来计算value值。所以最后演变成下面的结构。 自注意力图(todo) 与之相比的卷积神经网络图(todo) 和循环神经网络图(todo) 可以看出自注意力图能够关联更多信息。 试试看用多头注意力，把q，k，v都设置为同一个输入x。 12345678num_hiddens, num_heads = 100, 5attention = d2l.MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens, num_hiddens, num_heads, 0.5)attention.eval()batch_size, num_queries, valid_lens = 2, 4, torch.tensor([3, 2])X = torch.ones((batch_size, num_queries, num_hiddens))attention(X, X, X, valid_lens).shape 位置编码当走上了注意力机制来处理词序列的一刻，就已经失去了时间步的概念，也就根本没有把词的位置信息给放进去。举个例子： I do not love you, but I do like your hair color.I do love you, but I do not like your hair color.两句话用的一模一样的词，但是意思完全不同，搞错这个意思就完蛋了 ：），可能与爱情失之交臂。 所以需要引入位置编码。举个例子：I do love you, but I do not like your hair color, can you change ?1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 最简单的方式就是把这些位置编码信息带入，比如把1转化为二进制0001，变成一个向量(0,0,0,1)添加到词元的embedding 向量里去，比如 i = (23, 1, 0, 28) + 位置信息 (0,0,0,1) = (23, 1, 0 ,29)。 但是这样做有严重的问题，遇到特别长的句子，数值会变得很大，遇到训练期没见过的超级长句，泛化能力有限。所以需要找到一个满足一下条件的位置编码方案： 1 给每个时间步有唯一的编码2 在一个长句和一个短句，两个时间步的距离应该相等3 句子长度不影响该编码上界值 继续思考下去： 0: 0 0 0 01: 0 0 0 12: 0 0 1 03: 0 0 1 14: 0 1 0 05: 0 1 0 16: 0 1 1 07: 0 1 1 18: 1 0 0 09: 1 0 0 110: 1 0 1 011: 1 0 1 112: 1 1 0 013: 1 1 0 114: 1 1 1 015: 1 1 1 1 发现了规律没有，对于最后一位，1总是每个1位出现一次，对于倒数第二位，1总是隔两位出现两次对于倒数第三位，1总是每隔4位出现4次，对于倒数第四位，1总是每隔8位出现一次。也就是遵循这样的规律，最右来回变化频率快，最左来回变化频率慢, 位置编码沿维度频率在变小或者变大。 所以这个跟cos sin函数很像，引入cos，sin函数遵循了位置的规律。对每个维度引入位置编码： t为位置p(t) = sin( wk * t) 当t等于偶数 t=2kp(t) = cos( wk * t) 当t等于奇数 t=2k+1wk = 1/pow(1000, 2k/d) 比如对于一个embeding维度为4句子：I do love you, but I do not like your hair color.pt = （ sin(t), cos(t), sin(t/100), cos(t/100) ) 代入上面函数i = （ sin(1), cos(1), sin(1/100), cos(1/100) ) 1 这样满足的是最右来回频率慢，最左来回频率慢, 满足了位置的频率变化规律2 在原论文里面，数学家证明了，位置信息不依赖句子总长度。3 两个位置编码相减也是满足条件的。 这种位置编码方案就是我们想要的。 代码 1234567891011121314151617181920212223242526272829303132class PositionalEncoding(nn.Module): &quot;Implement the PE function.&quot; def __init__(self, d_model, dropout, max_len=5000): super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(p=dropout) # 初始化Shape为(max_len, d_model)的PE (positional encoding) pe = torch.zeros(max_len, d_model) # 初始化一个tensor [[0, 1, 2, 3, ...]] position = torch.arange(0, max_len).unsqueeze(1) # 这里就是sin和cos括号中的内容，通过e和ln进行了变换 div_term = torch.exp( torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model) ) # 计算PE(pos, 2i) pe[:, 0::2] = torch.sin(position * div_term) # 计算PE(pos, 2i+1) pe[:, 1::2] = torch.cos(position * div_term) # 为了方便计算，在最外面在unsqueeze出一个batch pe = pe.unsqueeze(0) # 如果一个参数不参与梯度下降，但又希望保存model的时候将其保存下来 # 这个时候就可以用register_buffer self.register_buffer(&quot;pe&quot;, pe) def forward(self, x): &quot;&quot;&quot; x 为embedding后的inputs，例如(1,7, 128)，batch size为1,7个单词，单词维度为128 &quot;&quot;&quot; # 将x和positional encoding相加。 x = x + self.pe[:, : x.size(1)].requires_grad_(False) return self.dropout(x) 参考：https://kazemnejad.com/blog/transformer_architecture_positional_encoding/https://zhuanlan.zhihu.com/p/106644634https://zh.d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.htmlhttps://blog.csdn.net/zhaohongfei_358/article/details/126019181","link":"/2024/04/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80/%20%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B/"},{"title":"斯坦福小镇","text":"从openai演进路线，和结合sam的访谈记录，还有OpenAI联合创始人Karpathy分享。 Sam Altman 早在2023年4月底说了 “I think we’re at the end of the era where it’s going to be these, like, giant, giant models,” he told an audience at an event held at MIT late last week. “We’ll make them better in other ways.” Karpathy 也在大概6月份说过类似的话 “如果一篇论文提出了某种不同的训练方法，那么我们OpenAI内部的Slack群组里的讨论会类似于，哦是的，有人在两年半尝试过，它不起作用，我们对这种方法的来龙去脉非常了解。但是当新的AI Agents论文出来的时候，我们都非常感兴趣，觉得它非常酷，因为我们的团队并没有花费五年时间在这上面，我们并不比你们更多掌握什么，我们正在与你们所有人一起竞争。这就是认为你们处于AI Agents能力的最前沿的原因” 可以看出他们的思路，越来越趋向ai agent模式，大模型不应该只是一个提问-回答的模式，而是越来越像人类，人类会用工具，会有不同的分工，产生组织，进行协作。结合openai的官方更新日志来看，验证了他们的思路，以下是他们的更新表格。 可以看23年第一季度，openai主要集中在模型上，但是第二季开始，openai的注意力就放到了ai agent上面，给大模型装上插件，给予访问互联网的能力，函数回调，代码解释器。 以下列表是openai一些重要更新查看。 更新日期 发布内容8/3, 2023 1 提示词示例，一些帮助用户如何写提示词的一些实例。2 会提示用户，有推荐提示帮助用户继续对话。3 plus用户会默认使用gpt4 模型 4 plus用户可以上传各类文件，让gpt4分析和总结。5 保持登录，不会再2周自动登出 6 增加了一些快捷键来复制对话，和查看对话列表。7/6, 2023 1 plus用户可以使用代码解释器，用户可以上传文件，运行代码，访问用户上传的文件，分析数据，绘制图表，遍接文件，运行计算。5/12, 2023 1 plus会员可以使用beta 版本的chatgpt，即具有访问互联网能力，且可以使用第三方插件的chatgpt模型，可以到插件store安装和启用插件。3/23, 2023 1 插件模型上线，chatgpt可以访问最新的信息，可以做数学计算，使用第三方服务。启动waitlist给用户和开发者，1.1：浏览器模型，可以访问互联网。1.2，代码解释器模型，实验性质的chat gpt模型，可以使用python，可以上传文件和下载文件。1.3第三方插件模型，模型有能力知道什么场景使用什么第三方插件3/14, 2023 1 gpt4 上线，更强的逻辑推理能力，处理更加负责的指示，更强的创造力，并且动态给予plus会员尝试gpt4的服务。 更新日期 博客更新内容6/13,2023 1 函数调用，这个特性是openai对tool思考的一个落地，也就是说在调用gpt api时添加了，一个function字段来描述你自定义的工具（这个工具的描述），一旦用户问了某个问题，模型判断需要用到这个函数，就会请求开发者调用并且使用该结果 2 推出了GPT4-0613，GPT-3.5-turbo-0613, 具备了函数调用能力 3 推出了延长输入的，GPT4-32k，GPT3.5-turbo-16K 4 还有价格下降，GPT3.5-turbo 价格下降到700 page对话仅需要1美元 https://openai.com/blog/function-calling-and-other-api-updates7/6, 2023 GPT-4 API 会开放给所有的开发者 https://openai.com/blog/gpt-4-api-general-availability7/20, 2023 prompt优化更新，允许自定义两个设定，A告诉gpt自己的信息，B希望gpt怎么回应自己，以便更好的得到回答 https://openai.com/blog/custom-instructions-for-chatgpt 总结：从chatgpt演化思路来看，以及全世界范围内，大模型的参数规模边际效益减弱，agent模式越来越成为llm的未来。 业界当前主要实践两种agent: 1 ai + 工具 = agent，这种模式有：https://github.com/Significant-Gravitas/Auto-GPT chat gpt + 各种插件（天气，股票…) chat gpt + 向量数据库 autp gpt: https://github.com/Significant-Gravitas/Auto-GPT meta gpt: https://link.zhihu.com/?target=https%3A//github.com/geekan/MetaGPT agent gpt: https://github.com/reworkd/AgentGPT 2 agent + agent = agents 当前这个领域，主要是斯坦福在探索，清华在跟随。 斯坦福大学论文《Generative Agents: Interactive Simulacra of Human Behavior》：https://arxiv.org/pdf/2304.03442.pdf 清华大学论文《Communicative Agents for Software Development》：https://arxiv.org/abs/2307.07924 本文主要探讨agents模式的前沿探索，和其未来的范式。 2 Memory Stream，ai agent的记忆 现在很多人去langchain+llm的工作，本质上就是用向量数据库去存储，一些私有领域的知识，在提问大模型的时候，把相关的信息作为上下文带入，然后获得一个回答的案例，但是这仅仅是一种非常粗糙人尽皆知的用法而已。 这里可以通过斯坦福小镇获得很多灵感，在斯坦福小镇里，一个ai agent具备有memory stream，相当于一个记忆实体的流，这个流是按照时间排序下来的一个一个记忆实体，记忆实体里最基本的就是观察。 由上图可见，是一系列ai agent在通过 “观察”，获得了周围环境的一些记忆实体，这里需要注意的是，这种“观察”是 1其他agent的行为和状态 2身处的环境的状态。 2.1 提取记忆实体 不是所有的记忆实体都是需要，一个agent需要做什么决策，都需要提取相关的记忆实体，来做相关的决策。所以斯坦福小镇的做法，是将agent当前的状况作为输入，提取和当前状况有相关性的，且时间戳比较相近的，还有重要性比较高的记忆实体。这里最关键的重要性指标，没有“算法”，没有“if else”判断语句，重要性的判断是模型去判断。 这里做一个引申：模型判断 替换 if else 判断。 就是agent的逻辑判断if语句，演化成了，问模型，这是一种很反程序员的思路，因为预示了agent的行为不是明确的，而是概率性的。过去的开发一个工具，和一个程序来完成特定的任务，编程范式从控制流式，到面向对象。但是都是有明确的输入和输出结果，但是斯坦福这里做的尝试就是基本，让agent里的if else逻辑用模型自己去判断替换掉了。这是一个思路。让模型来把关if else语句 由于斯坦福纯粹是为了学术研究，所以他们这里制造的agent，就是模拟一个居民而已。制造memory stream，和提取momory object的机制，可能会让读者觉得这些没啥用。 后来清华大学在这方面研究，模拟了三个agents互相协作，一个产品经理agent，和一个程序员agent，和一个测试人员agent，产出了一个可运行，可测试的程序实体。这个过程，memory stream和记忆实体提取机制就产生了很大作用。 3 Reflection，ai agent自省机制。 在斯坦福小镇里，一个agent每日三省其身，使用reflection机制，这是什么意思呢？ 这里斯坦福小镇有一种创造性的想法，他们认为 一个agent的 “自省”， 也就是reflection，也是一种类型的记忆，从上图可以看到，一个agent，由于有“观察”，它的记忆流，一开始只充满这类纯观察类记忆，后面每固定的一个时间周期，agent会对observation进行一个总结，记忆流出现第二类记忆，就是总结类的记忆。 简单的说就是 输入一系列的 记忆实体，让大模型对这些记忆实体总结浓缩，输出一个reflection，再存入memory stream里面去。 总结类记忆也会存到memory stream里面，下一次，reflection会对观察类和总结类记忆再自省一次，形成更高维度的总结类记忆。 这种总结类记忆，其实有点像形成一个结论，这对指导agent的行为和计划有很好的作用。而且如果仅仅对纯观测类的记忆进行抽取，可能会形成错误的结论。reflection很好的纠正了这点。 后面会看到清华大学把reflection机制，当作agents之间协作的收敛和闭环的机制。 4 plan 和 action，agent的手脚 对于一个agent而言，行动之前，肯定是需要让大模型把一系列agent需要做的行动先安排好。再斯坦福小镇里，这些action包含了时间，地点，持续时间，和做的事情。当然如果要让agent来完成现实世界的一个任务，肯定没那么简单，这些action可能包含了，需要写的代码，需要调用的api等等。但是思想都是一样的，（加粗）需要让大模型安排出一个plan 关键在于如何让大模型安排plan，如何执行这个plan 这里斯坦福小镇的思路在于，把每个角色的定位信息作为输入，然后把agent当前的环境信息，以及提取的memory state作为集合，输入给大模型，通过top-down的递归方式，先让大模型安排出一个粗颗粒的计划，再逐步的迭代到比较细致的步骤。接着才调用api，去让角色执行plan。 并且这个plan是需要更新的，一个agent的memory stream会流入很多观测信息和reflection信息，他们这里就使用了上述的“不确定性编程”，这里流入信息后，需要决定是否更新plan，还是不更新plan。这个if else 语句，依旧交给大模型来做。 斯坦福小镇的这个制定计划的思路，其实本质和auto gpt的如出一辙，都是让大模型分解一个任务到最小可执行的细颗粒度，然后才去执行。但是不一样的是，这里的agent具备自己的memory stream，让agent和agent之间的合作变得可能，协作。 总结 从斯坦福小镇，展示了Memory Stream， Reflection，制定plan和执行action，这三个机制其实是就是agent之间互相协作的基础。大模型的一问一答的模式，其实远没有发挥出它的实力。大模型为底座，agents之间的互相协作肯定是比单个模型的一问一答强大很多的。","link":"/2024/05/13/ai%E5%B7%A5%E7%A8%8B/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%B0%8F%E9%95%87/"},{"title":"bahdanau注意力","text":"bahdanau注意力机制的初心本意是为了seq2seq学习而设计出的编码器解码器架构，有个弊端。对于编码器： h(t) = f ( x(t), h(t-1) )，每一时间步的隐状态由前一步的隐状态，和输入x(t) 决定。c = g( h(t), h(t-1), h(t-2), …. , h(1) )， 最终得到的上下文变量由隐藏状态决定。 ps: 一般而言这个g函数，就是选择最后一个隐状态。对于解码器： s(t) = g( y(t-1), s(t-1), c ), 每一时间步的隐状态由前一步隐状态, 和输出y(t-1), 和由编码器来的上下文变量c决定。y(t) = o( s(t), c）， 输出y(t)由当前隐状态和上下文变量决定。 这就是编码解码器架构，简单的四个公式决定。但是这里有个问题，这个c，上下文变量在解码的过程中，从来没有变过。 Attention = Sum( a(q, k) * v )，注意力汇聚公式：c(t) = Sum( a(s(t-1), h(t) ) * h(t) )， 为了改善c从来没变过的事实：把query设置为解码器的隐变量s(t-1)，key和value设置为编码器隐变量h(t)。 这个时候c(t)蕴含了X(t1, t2,….t), Y(t1, t2,… t-1)的信息，非常利于预测下一个时间步的值。举个例子：X(t1, t2,….t) = “l love you”, Y(t1, t2,… t-1)=”我爱“， 下一个时间步预测出”你“ 代码基本seq2seq基类 12345678910111213141516171819202122232425262728293031323334import torchfrom torch import nnfrom d2l import torch as d2lclass Encoder(nn.Module): &quot;&quot;&quot;编码器-解码器架构的基本编码器接口&quot;&quot;&quot; def __init__(self, **kwargs): super(Encoder, self).__init__(**kwargs) def forward(self, X, *args): raise NotImplementedErrorclass Decoder(nn.Module): &quot;&quot;&quot;编码器-解码器架构的基本解码器接口&quot;&quot;&quot; def __init__(self, **kwargs): super(Decoder, self).__init__(**kwargs) def init_state(self, enc_outputs, *args): raise NotImplementedError def forward(self, X, state): raise NotImplementedError class EncoderDecoder(nn.Module): &quot;&quot;&quot;编码器-解码器架构的基类&quot;&quot;&quot; def __init__(self, encoder, decoder, **kwargs): super(EncoderDecoder, self).__init__(**kwargs) self.encoder = encoder self.decoder = decoder def forward(self, enc_X, dec_X, *args): enc_outputs = self.encoder(enc_X, *args) dec_state = self.decoder.init_state(enc_outputs, *args) return self.decoder(dec_X, dec_state) 训练代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def sequence_mask(X, valid_len, value=0): &quot;&quot;&quot;在序列中屏蔽不相关的项&quot;&quot;&quot; maxlen = X.size(1) mask = torch.arange((maxlen), dtype=torch.float32, device=X.device)[None, :] &lt; valid_len[:, None] X[~mask] = value return X class MaskedSoftmaxCELoss(nn.CrossEntropyLoss): &quot;&quot;&quot;带遮蔽的softmax交叉熵损失函数&quot;&quot;&quot; # pred的形状：(batch_size,num_steps,vocab_size) # label的形状：(batch_size,num_steps) # valid_len的形状：(batch_size,) def forward(self, pred, label, valid_len): weights = torch.ones_like(label) weights = sequence_mask(weights, valid_len) self.reduction='none' unweighted_loss = super(MaskedSoftmaxCELoss, self).forward( pred.permute(0, 2, 1), label) weighted_loss = (unweighted_loss * weights).mean(dim=1) return weighted_lossdef train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device): &quot;&quot;&quot;训练序列到序列模型&quot;&quot;&quot; def xavier_init_weights(m): if type(m) == nn.Linear: nn.init.xavier_uniform_(m.weight) if type(m) == nn.GRU: for param in m._flat_weights_names: if &quot;weight&quot; in param: nn.init.xavier_uniform_(m._parameters[param]) net.apply(xavier_init_weights) net.to(device) optimizer = torch.optim.Adam(net.parameters(), lr=lr) loss = MaskedSoftmaxCELoss() net.train() animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[10, num_epochs]) for epoch in range(num_epochs): timer = d2l.Timer() metric = d2l.Accumulator(2) # 训练损失总和，词元数量 for batch in data_iter: optimizer.zero_grad() X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch] bos = torch.tensor([tgt_vocab['&lt;bos&gt;']] * Y.shape[0], device=device).reshape(-1, 1) dec_input = torch.cat([bos, Y[:, :-1]], 1) # 强制教学 Y_hat, _ = net(X, dec_input, X_valid_len) l = loss(Y_hat, Y, Y_valid_len) l.sum().backward() # 损失函数的标量进行“反向传播” d2l.grad_clipping(net, 1) num_tokens = Y_valid_len.sum() optimizer.step() with torch.no_grad(): metric.add(l.sum(), num_tokens) if (epoch + 1) % 10 == 0: animator.add(epoch + 1, (metric[0] / metric[1],)) print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} ' f'tokens/sec on {str(device)}') 注意力评分函数 1234567891011121314151617181920212223class AdditiveAttention(nn.Module): &quot;&quot;&quot;加性注意力&quot;&quot;&quot; def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs): super(AdditiveAttention, self).__init__(**kwargs) self.W_k = nn.Linear(key_size, num_hiddens, bias=False) self.W_q = nn.Linear(query_size, num_hiddens, bias=False) self.w_v = nn.Linear(num_hiddens, 1, bias=False) self.dropout = nn.Dropout(dropout) def forward(self, queries, keys, values, valid_lens): queries, keys = self.W_q(queries), self.W_k(keys) # 在维度扩展后， # queries的形状：(batch_size，查询的个数，1，num_hidden) # key的形状：(batch_size，1，“键－值”对的个数，num_hiddens) # 使用广播方式进行求和 features = queries.unsqueeze(2) + keys.unsqueeze(1) features = torch.tanh(features) # self.w_v仅有一个输出，因此从形状中移除最后那个维度。 # scores的形状：(batch_size，查询的个数，“键-值”对的个数) scores = self.w_v(features).squeeze(-1) self.attention_weights = masked_softmax(scores, valid_lens) # values的形状：(batch_size，“键－值”对的个数，值的维度) return torch.bmm(self.dropout(self.attention_weights), values) 注意力seq2seq模型，bahdanau注意 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class AttentionDecoder(d2l.Decoder): &quot;&quot;&quot;带有注意力机制解码器的基本接口&quot;&quot;&quot; def __init__(self, **kwargs): super(AttentionDecoder, self).__init__(**kwargs) @property def attention_weights(self): raise NotImplementedError class Seq2SeqAttentionDecoder(AttentionDecoder): def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs): super(Seq2SeqAttentionDecoder, self).__init__(**kwargs) #q是decoder的隐藏层，kv是encoder的隐藏层 self.attention = AdditiveAttention(num_hiddens,num_hiddens,num_hiddens,dropout) self.embedding = nn.Embedding(vocab_size, embed_size) self.rnn = nn.GRU(embed_size+num_hiddens, num_hiddens, num_layers, dropout=dropout) self.dense = nn.Linear(num_hiddens, vocab_size) # defenitely don't understand def init_state(self, enc_outputs, enc_valid_lens, *args): outputs, hidden_state = enc_outputs return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens) def forward(self, X, state): # enc_outputs的形状为(batch_size,num_steps,num_hiddens). # hidden_state的形状为(num_layers,batch_size, # num_hiddens) enc_outputs, hidden_state, enc_valid_lens = state # 输出X的形状为(num_steps,batch_size,embed_size) X = self.embedding(X).permute(1, 0, 2) outputs, self._attention_weights = [], [] for x in X: # query的形状为(batch_size,1,num_hiddens) query = torch.unsqueeze(hidden_state[-1], dim=1) # context的形状为(batch_size,1,num_hiddens) context = self.attention( query, enc_outputs, enc_outputs, enc_valid_lens) # 在特征维度上连结 x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1) # 将x变形为(1,batch_size,embed_size+num_hiddens) out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state) outputs.append(out) self._attention_weights.append(self.attention.attention_weights) # 全连接层变换后，outputs的形状为 # (num_steps,batch_size,vocab_size) outputs = self.dense(torch.cat(outputs, dim=0)) return outputs.permute(1, 0, 2), [enc_outputs, hidden_state, enc_valid_lens] @property def attention_weights(self): return self._attention_weights 测试形状 12345678910111213encoder = d2l.Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)encoder.eval()decoder = Seq2SeqAttentionDecoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)decoder.eval()X = torch.zeros((4, 7), dtype=torch.long) # (batch_size,num_steps)state = decoder.init_state(encoder(X), None)output, state = decoder(X, state)output.shape, len(state), state[0].shape, len(state[1]), state[1][0].shape结果：(torch.Size([4, 7, 10]), 3, torch.Size([4, 7, 16]), 2, torch.Size([4, 16])) 训练 1234567891011embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1batch_size, num_steps = 64, 10lr, num_epochs, device = 0.005, 250, d2l.try_gpu()train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)encoder = d2l.Seq2SeqEncoder( len(src_vocab), embed_size, num_hiddens, num_layers, dropout)decoder = Seq2SeqAttentionDecoder( len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)net = EncoderDecoder(encoder, decoder)train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device) 运行结果 12345678910111213engs = ['go .', &quot;i lost .&quot;, 'he\\'s calm .', 'i\\'m home .']fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']for eng, fra in zip(engs, fras): translation, dec_attention_weight_seq = d2l.predict_seq2seq( net, eng, src_vocab, tgt_vocab, num_steps, device, True) print(f'{eng} =&gt; {translation}, ', f'bleu {d2l.bleu(translation, fra, k=2):.3f}') 结果：go . =&gt; va !, bleu 1.000i lost . =&gt; j'ai perdu ., bleu 1.000he's calm . =&gt; il est riche ., bleu 0.658i'm home . =&gt; je suis chez moi ., bleu 1.000","link":"/2024/04/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80/Bahdanau%E6%B3%A8%E6%84%8F%E5%8A%9B/"},{"title":"一些需要探究的问题","text":"大模型预训练数据从哪里获取？主要都是什么？ 只用网上爬取的数据是否可以训练一个足够好的大模型？大模型预训练中数据重要吗？是怎么影响模型性能的？数据如何清洗过滤？数据清洗是如何影响模型效果的？训练大模型时，数据是训练一个epoch好，还是多训练几个epoch好？大模型训练数据的天花板在哪？又决定了模型的天花板在哪？为什么模型会有Scaling law？是否和训练数据有关？如何突破模型训练的Scaling law？什么是openai的scaling law？是否和摩尔定律相关？什么是 n-gram repetitions?什么是哈希方法去重，比如：局部敏感哈希MinHash、SimHash去重是文档级别还是段落级别?规则过滤式文档级别么？https://zhuanlan.zhihu.com/p/641013454 为什么深度学习最后演化到transformer这个架构上去？为什么percetron无法解决xor问题？Hopfield Networks是用来干啥的？hinton 他们是如何证明一个多层神经网络如何克服xor问题Lecun的手写体识别是怎么用卷积，和权重sharing。backpropagation是怎么样的？为什么会有vanishing gradients，exploding gradients 和不记忆long-term memory的问题?LSTM是如何克服back propagation的各种问题？hinton 2006年发表的方法是 unsupervised + supervised fine-tune？bengio 如何证明多层比少层更好？bengio 和 lecun 发表的cnn 和 rbms 和dbns 是什么？imagenet 规模多大?rnn + lstm 怎么变成了nlp基本架构Collobert 和 Weston 的pre-trained embedding 和 cnns for text 和sharing embedding 有啥意义Mikolov 对这个 word2vec 是怎么基于他们的成果改善的？sutskerver 的 hessian-free 优化器是如何改善rnn训练的？Sutskever可以说是奠定了基础了吧，他发明的encoder 和 decoder模式如何影响后世的（在ner，qa，sumerize）Bahdanau 发明的注意力机制是怎么基于sutskever ？ transformer是基于什么情况发明的？后世对他的研究集中在架构改造，训练方法，和应用上是如何做的？对transformer的安排是如何改造的？对transformer内部是如何改造的？transformer安排研究方向：降低内存和降低计算 如何连接两个trans 容错性训练-可暂停机制 recurrence 结构 大刀阔斧改变其架构？transformer内部研究方向： low-rank attion-prior 多头 复杂度降低 pt-query 聚合kv内存low-rank意思？sparsity意思?关于多头计算的复杂度降低的方法？位置编码的3种方式？residual 连接和position-wise怎么改善transformer？这些pre-train方法的演化是？bert（encode only），generative-pretrained-transformer models （gpt3，decode only), t5(encode, decode both)为什么其他模型最后都被抛弃？他们是哪些？encoder - decoder 架构 为什么 beats othersattention 是怎么演化出来的？这种mult-head attention 有什么稀疏的？encoder和decoder可以合并么？机器翻译如何应用这些？bert一开始谁用的？transformer是如何组成bert？bert是稀疏的么？multilingual - transformer 怎么从没听过？如何改transformer各领域的transformer是怎么样的？图片领域trans视频领域trans文字领域trans语音trans 1 一个transformer有哪些组成部分？2 encoder输入输出是什么？ decoder输入输出是什么？3 x1..xt 先被embedding到v1….vt 向量，ht = tanh(w(hh) ht-1 + w(hx)xt)的图示是如何?4 rnn可以单向也可以双向，对于encoder而言5 decoder hidden state 是si = g(si-1, yt-1, c)6 decoder得到预测词向量这里难懂，需要具体的计算7 同时最后使用了交叉熵+softmax来反向更新权重以此训练8 上述的rnn结构有长距离信息丢失，无法并行计算，gradient消失爆炸之类的9 这个rnn-based 有点无趣 Transformer Encoder 有什么子层？写一下self-attention的公式Transformer的优缺点Encoder端和Decoder端是如何进行交互的？Transformer中为什么需要线性变换？Transformer attention的注意力矩阵的计算为什么用乘法而不是加法？Transformer attention计算为什么要在softmax这一步之前除Transformer attention计算注意力矩阵的时候如何对padding做mask操作的？Transformer的残差结构及意义Transformer为什么使用LN而不是BN？Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？/ 为什么decoder自注意力需要进行sequence mask？Transformer的并行化体现在哪里，Decoder可以做并行化嘛？Transformer计算量最大的部分是哪里Transformer、LSTM和单纯的前馈神经网络比，有哪些提升？Transformer处理篇章级的长文本的变体有哪些处理超长文本的方法","link":"/2024/05/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80/%E4%B8%80%E4%BA%9B%E9%9C%80%E8%A6%81%E6%8E%A2%E7%A9%B6%E7%9A%84%E9%97%AE%E9%A2%98/"},{"title":"Transformer的从0开始实现","text":"在多头自注意力，和encoder-decoder架构上，基本构成了一个transformer的架构。但是transformer的架构，还有一些其他的优化。 架构图（todo) 1 在多头自注意力后面基本都引入了FFN 2 使用了残差连接 3 每个多头自注意力和FFN后面都有 add &amp; norm 层。 下面分别阐述，各自的作用。 FFN： 和早期的End-To-End Memory Networks架构很相似，可以认定是FFN部分承担了transformer中记忆的功能，也有说是因为attention带来的是线性拟合，添加ffn这样非线性拟合可以更好捕捉pattern参考：https://zhuanlan.zhihu.com/p/604739354 残差连接： 残差连接可以很好的解决网络过深导致的，求解链式梯度时，梯度爆炸或消失的问题，也解决网络权重退化问题的。参考：https://zhuanlan.zhihu.com/p/42833949 add &amp; norm 层：残差连接是X+layer（X)，所以add这个操作是残差连接的实现， Norm有许多品种例如：Batch Normalization，Layer Normalization，Group Normalization，Instance Normalization。主要解决问题是，输入分布的偏移问题，输入分布偏移会导致，在过激活层的时候，容易陷入激活层的梯度饱和区，降低模型收敛速度。参考：https://www.zhihu.com/question/309177367 总结下来就是：transformer基本架构是个encoder-decoder架构。encoder有n层block，每个block由输入：相对位置编码, embedding，处理：多头自注意力，以残差连接方式，连接一个FFN层组成。decoder有n层block，每个block由输入: 相对位置编码，embedding，处理：masked多头自注意力，以残差连接方式，连接一个多头注意力（其中quey，key来自encoder），和FFN组成。 transformer各个模块代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170#!/usr/bin/python# -*- coding: UTF-8 -*-import mathimport torchfrom torch import nnfrom d2l import torch as d2limport pandas as pdfrom DotProductAttention import *from EncoderDecoder import *from MultiHeadAttention import *from PositionalEncoding import *#kid stuff okclass PositionWiseFFN(nn.Module): &quot;&quot;&quot;基于位置的前馈网络&quot;&quot;&quot; def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs, **kwargs): super(PositionWiseFFN, self).__init__(**kwargs) self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens) self.relu = nn.ReLU() self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs) def forward(self, X): return self.dense2(self.relu(self.dense1(X)))# okclass AddNorm(nn.Module): &quot;&quot;&quot;残差连接后进行层规范化&quot;&quot;&quot; def __init__(self, normalized_shape, dropout, **kwargs): super(AddNorm, self).__init__(**kwargs) self.dropout = nn.Dropout(dropout) self.ln = nn.LayerNorm(normalized_shape) def forward(self, X, Y): return self.ln(self.dropout(Y) + X)# okclass EncoderBlock(nn.Module): &quot;&quot;&quot;Transformer编码器块&quot;&quot;&quot; def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias=False, **kwargs): super(EncoderBlock, self).__init__(**kwargs) self.attention = MultiHeadAttention( key_size, query_size, value_size, num_hiddens, num_heads, dropout, use_bias) self.addnorm1 = AddNorm(norm_shape, dropout) self.ffn = PositionWiseFFN( ffn_num_input, ffn_num_hiddens, num_hiddens) self.addnorm2 = AddNorm(norm_shape, dropout) def forward(self, X, valid_lens): Y = self.addnorm1(X, self.attention(X, X, X, valid_lens)) return self.addnorm2(Y, self.ffn(Y))# okclass TransformerEncoder(d2l.Encoder): &quot;&quot;&quot;Transformer编码器&quot;&quot;&quot; def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, use_bias=False, **kwargs): super(TransformerEncoder, self).__init__(**kwargs) self.num_hiddens = num_hiddens self.embedding = nn.Embedding(vocab_size, num_hiddens) self.pos_encoding = PositionalEncoding(num_hiddens, dropout) self.blks = nn.Sequential() for i in range(num_layers): self.blks.add_module(&quot;block&quot;+str(i), EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias)) def forward(self, X, valid_lens, *args): # 因为位置编码值在-1和1之间， # 因此嵌入值乘以嵌入维度的平方根进行缩放， # 然后再与位置编码相加。 X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens)) self.attention_weights = [None] * len(self.blks) for i, blk in enumerate(self.blks): X = blk(X, valid_lens) self.attention_weights[ i] = blk.attention.attention.attention_weights return Xclass DecoderBlock(nn.Module): &quot;&quot;&quot;解码器中第i个块&quot;&quot;&quot; def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i, **kwargs): super(DecoderBlock, self).__init__(**kwargs) self.i = i self.attention1 = MultiHeadAttention( key_size, query_size, value_size, num_hiddens, num_heads, dropout) self.addnorm1 = AddNorm(norm_shape, dropout) self.attention2 = MultiHeadAttention( key_size, query_size, value_size, num_hiddens, num_heads, dropout) self.addnorm2 = AddNorm(norm_shape, dropout) self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens) self.addnorm3 = AddNorm(norm_shape, dropout) def forward(self, X, state): enc_outputs, enc_valid_lens = state[0], state[1] # 训练阶段，输出序列的所有词元都在同一时间处理， # 因此state[2][self.i]初始化为None。 # 预测阶段，输出序列是通过词元一个接着一个解码的， # 因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表示 if state[2][self.i] is None: key_values = X else: key_values = torch.cat((state[2][self.i], X), axis=1) state[2][self.i] = key_values if self.training: batch_size, num_steps, _ = X.shape # dec_valid_lens的开头:(batch_size,num_steps), # 其中每一行是[1,2,...,num_steps] dec_valid_lens = torch.arange( 1, num_steps + 1, device=X.device).repeat(batch_size, 1) else: dec_valid_lens = None # 自注意力 X2 = self.attention1(X, key_values, key_values, dec_valid_lens) Y = self.addnorm1(X, X2) # 编码器－解码器注意力。 # enc_outputs的开头:(batch_size,num_steps,num_hiddens) Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens) Z = self.addnorm2(Y, Y2) return self.addnorm3(Z, self.ffn(Z)), stateclass TransformerDecoder(d2l.AttentionDecoder): def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, **kwargs): super(TransformerDecoder, self).__init__(**kwargs) self.num_hiddens = num_hiddens self.num_layers = num_layers self.embedding = nn.Embedding(vocab_size, num_hiddens) self.pos_encoding = PositionalEncoding(num_hiddens, dropout) self.blks = nn.Sequential() for i in range(num_layers): self.blks.add_module(&quot;block&quot;+str(i), DecoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i)) self.dense = nn.Linear(num_hiddens, vocab_size) def init_state(self, enc_outputs, enc_valid_lens, *args): return [enc_outputs, enc_valid_lens, [None] * self.num_layers] def forward(self, X, state): X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens)) self._attention_weights = [[None] * len(self.blks) for _ in range (2)] for i, blk in enumerate(self.blks): X, state = blk(X, state) # 解码器自注意力权重 self._attention_weights[0][ i] = blk.attention1.attention.attention_weights # “编码器－解码器”自注意力权重 self._attention_weights[1][ i] = blk.attention2.attention.attention_weights return self.dense(X), state @property def attention_weights(self): return self._attention_weights","link":"/2024/04/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80/Transformer%E7%9A%84%E4%BB%8E0%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0/"},{"title":"循环神经网络 rnn","text":"卷积网络(cnn)是用来处理空间信息（一张图像），循环神经网络(rnn)则用来处理序列信息（一句话，一个视频的图像帧，温度的变化序列）rnn引入了状态变量存储序列信息和当前输入，从而预测下一个输入，以此不断循环，预测整段序列信息。 序列模型对序列模型进行建模，考虑到成千上万的生活中的例子，比如许多观众对该电影的评分，比如一次大地震后的数次余震，一段新闻标题，一段股票价格序列，还有一长段人们在社交媒体上的对话。综上，可以引入时间步t的概念，时间序列t1, t2, t3, t4…., 还有X(t1), X(t2), X(t3) X(t4) …..， 对于电影评分来说，X(t)就是在t时间步该电影的评分，新闻标题则, X(t)就是在t时间步的单词，股价则是t时间步股票的价格。整个建模的公式就是P(xt| xt-1, … xt1)-xt, 预测在已经发生了xt1到xt-1的时间步后，下一个xt的概率是多少。以地震为例子，比如某地发生了8级大地震，记xt0=8, 后面发生了数次小级别的余震分别是3, 4, 3, 5，记xt1=3, xt2=4, xt3=3, xt4=5。所以需要计算 P(1 | xt4=5,xt3=3,xt2=4,xt1=3,xt0=8)，也就是在这些地震发生后，下一次余震是1级的概率。以此类推，算出2，3，4，5，6，7，8，9，10级的余震的概率，然后取概率最高者作为xt5。 在这种模型里会遇到一个问题，这个问题在于序列越来越长的时候，1 计算量将会变得不可接受2 同时对现实时间的模拟也会失真。比如，某地遇地震等级的序列在现实中，其实大部分时间步都为0，【0，0，0，0，0，0，0，0，0，无数的0….. , 8, 3 4 5 ,4, 2, 3, 1, 1, 1】两种策略：1 自回归(autogressive)，对过去序列取一个有效的时间步长度，比如在上述例子中，摒弃掉所有前面的0。2 隐式变量自回归(latent autogressive) 使用一个变量来对过去的序列进行总结，比如上述例子中，使用sum，对之前的无数的0序列，都叠加到这个sum中，最终还是0。 对于1类策略，落地到数学工具则是马尔可夫模型。举个例子，预测地震等级，P(xt=2|xt-1=3, xt-2=4 … xt0=8) 约等于 P(xt=2| xt-1=3)，当然也可以约等于 P(xt=2| xt-1=3, xt-2=4)，前者是1阶马尔可夫，后者是2阶马尔可夫。这样形成一个马尔科夫链，从而很容易计算出 P(xt=2|xt-1=3, xt-2=4 … xt0=8) 针对马尔可夫链的思考还有因果关系，也是就是时间步总是向前的，很显然未来不能影响过去，可以针对现有的历史数据，预测下一个时间步出现的数据，但是不可逆。但是我个人对这个因果关系是持有反对立场，因为在考古学，过去是对自然界是已知的，但是对人类是未知的，比如发生在数万年前的地球温度的变化，也可以通过逆向的马尔可夫链来预测。 demo：暂无 对于2类策略，先不表。 序列模型中的明珠，就是文字序列问题。这一章节主要介绍了一些，读取数据集的手段：1 读取文字序列demo中读取了一本小说，主要是一行一行字符串序列读取的。 2 tokenize 词元化还有tokenize函数，这是用来对字符串序列进行分割，成一个一个单词(word-level，后面bpe的byte-level才是好的，先不说）。 3tokenizer了之后，是词表的构建，对于一个单词分配出现频次记作索引值，低频词会被移除，映射为【unk】，并且补充【pad】，【bos】，【eos】作为填充词元，开始词元，结束词元。遍历语料，构建词表。 语言模型和数据集合 语言模型是自然语言处理的关键。而语言模型的建模和序列模型的建模一致，都是计算下一个词汇出现的概率P(xt| xt-1, … xt1)-xt,比如一个序列： deep learning is funP(deep) ?P(learning) = n(deep, learning)/n(deep)p(is) = n(deap, learning, is)/n(deep, learning) 这里就提到了 拉普拉斯平滑，有点不理解，因为拉普拉斯平滑依旧不适合应对多个n元词出现的情况。 齐普夫定律支配着单词的分布，这个分布不仅适用于一元语法，还适用于其他n元语法。这里我是有疑问，为什么齐普夫高估了尾部单词的频率对于zipf’law 指导了几个事实：1 n元词受zip定律支配2 n元词组数目不大3 有可以使用深度学习的希望 读取长序列的主要方式是随机采样和顺序分区。在迭代过程中，后者可以保证来自两个相邻的小批量中的子序列在原始序列上也是相邻的关于顺序分区和随机采样有demo 所以如何使用深度学习的方法来研究字符串序列模型，是最主要的方法。而可落地的方式就是之前讲到的隐式变量法P( x(t) | x(t-1), x(t-2), … x(t1) ) 约等于 P ( x(t) | h(t-1))问题在于，h(t-1) 是个必须包含【x(t-1), x(t-2), … x(t1) 】所有信息的隐变量。 而为了避免大量的计算，需要转变成一个一个时间步去计算 h(t)=f(h(t-1), x(t)) 为什么设置为这样的函数，因为h(t)需要有过去所序列的信息，同时也包含 x(t)的信息，kind of like 斐波那契函数。 so, 变成了这样的格式后。下一步要思考的就是h(t-1) 与 x(t) 如何结合起来。 先考虑特例情况： h(1)是只包含了x(1)信息的隐变量。 h(1) = f(x(1)), 这种情况比就是最普遍最简单的，多层感知机么？但是多层感知机，是肯定有output的。 h(1) = x(1)*w(xh) + b(h), ps:h(1)的维度是n x h o(1)= activation(h1)*w(ho) + b(o) 那么考虑一下h(2), 其实参考多层感知机的思想，就是给自变量加个权重和bias h(2) = f(h(1), x(2)) = h(1)*w(hh) + x(2)*w(xh) + b(h) ps: h(1)是nxh，所以h(1)*w(hh)是nxh, x(2)是nxd，x(2)*w(xh)是nxh，两个相加依旧是nxh, 然后加上bias o(2) = activation(h2)*w(ho) + b(o) x(2)是哪里来的呢？这种情况下，只有从o(1)来。所以这个最终总结下来的公式是：x(t) 是nxd， n批量大小，d为特征H(t), H(t-1)是nxh，n是批量大小，h是隐藏数目W(ho)是hxo，h隐藏数目，o是输出维度，一般是词表大小，映射都某一个词 H(t) =activation( x(t)*w(dh) + H(t-1)*w(hh) + b(h) )O(t) = H(t)*W(ho) + b(o) 代码测试“””X, W_xh = torch.normal(0, 1, (3, 1)), torch.normal(0, 1, (1, 4))H, W_hh = torch.normal(0, 1, (3, 4)), torch.normal(0, 1, (4, 4))torch.matmul(X, W_xh) + torch.matmul(H, W_hh)但是应该可以使用拼接再相乘torch.matmul(torch.cat((X, H), 1), torch.cat((W_xh, W_hh), 0))“””这里的公式里的两次矩阵相乘，可以转化成一次相乘，但是这样的转化效率会不会变高，则不知道了。 perplexity，当有了这样的一个模型，我们预测三组数据And don’t be sad and don’t cryAnd don’t be sad and don’t eat apple and bannaAnd don’t be sad and don’t sdsdsadawda很显然需要一个数值来衡量模型的准确性，从人类的角度，第一句话肯定是最对的。第二句话说明模型可以合成单词了，但是没有把握含义，第三句话说明模型欠拟合。 想探索perplexity，有点难。我还是记住就好了。当perplexity为1的时候是最好的，当perplexity为1的时候是最差的，当perplexity为词表唯一词的数目是基线。 所以接下来就是实践，也就是write from scratch, 有两个章节和很多代码，just skip从0实现和简洁实现rnn 当训练的时候，其实是有两个主要的问题，梯度爆炸和梯度消失。但是这个问题，这个章节，在未晚上上述的情况下，很难进行学习。也就是通过实践反向传播这一章节","link":"/2024/04/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80/rnn%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"title":"Transformer的从0开始实现","text":"在一个空间内，有query，keys，values，然后产生了注意力汇聚。 但是这样的空间只允许有一个么？可以有多个，然后最后统一输出么？H1=f( W1(q) * Q, W1(k) * K, W1(v) * V)H2=f( W2(q) * Q, W2(k) * K, W2(v) * V)H3=f( W3(q) * Q, W3(k) * K, W3(v) * V)…Hi=f( Wi(q) * Q, Wi(k) * K, Wi(v) * V) 这里的f其实就是注意力评分函数，可以是加性的可以是点积。最后再经由一个线性映射。Output = W0 * ( H1, H2, H3, H4, H5….Hi) 但是理解不是最难的，这里比较难的点，是张量的分解和合并以此加强并行运算。 要用点积注意力评分，因为效率高。 12345678910111213141516171819202122232425262728293031323334353637import mathimport torchfrom torch import nnfrom d2l import torch as d2ldef masked_softmax(X, valid_lens): &quot;&quot;&quot;通过在最后一个轴上掩蔽元素来执行softmax操作&quot;&quot;&quot; # X:3D张量，valid_lens:1D或2D张量 if valid_lens is None: return nn.functional.softmax(X, dim=-1) else: shape = X.shape if valid_lens.dim() == 1: valid_lens = torch.repeat_interleave(valid_lens, shape[1]) else: valid_lens = valid_lens.reshape(-1) # 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0 X = d2l.sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6) return nn.functional.softmax(X.reshape(shape), dim=-1) class DotProductAttention(nn.Module): &quot;&quot;&quot;缩放点积注意力&quot;&quot;&quot; def __init__(self, dropout, **kwargs): super(DotProductAttention, self).__init__(**kwargs) self.dropout = nn.Dropout(dropout) # queries的形状：(batch_size，查询的个数，d) # keys的形状：(batch_size，“键－值”对的个数，d) # values的形状：(batch_size，“键－值”对的个数，值的维度) # valid_lens的形状:(batch_size，)或者(batch_size，查询的个数) def forward(self, queries, keys, values, valid_lens=None): d = queries.shape[-1] # 设置transpose_b=True为了交换keys的最后两个维度 scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d) self.attention_weights = masked_softmax(scores, valid_lens) return torch.bmm(self.dropout(self.attention_weights), values) 并行运算的张量分解和合并 1234567891011121314151617181920212223242526272829303132333435363738394041424344def transpose_qkv(X, num_heads): &quot;&quot;&quot;为了多注意力头的并行计算而变换形状&quot;&quot;&quot; # 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens) # 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，num_hiddens/num_heads) X = X.reshape(X.shape[0], X.shape[1], num_heads, -1) # 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数, num_hiddens/num_heads) X = X.permute(0, 2, 1, 3) # 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数, # num_hiddens/num_heads) return X.reshape(-1, X.shape[2], X.shape[3])#@savedef transpose_output(X, num_heads): &quot;&quot;&quot;逆转transpose_qkv函数的操作&quot;&quot;&quot; X = X.reshape(-1, num_heads, X.shape[1], X.shape[2]) X = X.permute(0, 2, 1, 3) return X.reshape(X.shape[0], X.shape[1], -1) 测试：#为了加强多个头的矩阵并行计算#首先对于X，batch_size=2, 每个batch有3个query，每个query经过转换有num_hidden=8个特征。#transpose_qky，会针对每个注意力头，有个batch，但是特征数目会除以head数目，比如2两个头，则每个query分出4个特征给头1，和头2.#综上所述, 所以对于一个输入的无论是Q,K,V, 形状为(batch_size, 查询个数&amp;kv对数目，特征数目)#会转为(batch_size*num_head, 查询个数&amp;kv对数目，特征数目/num_head)X=torch.normal(0,1,(2,3,8))print(X.shape)Xt=transpose_qkv(X, 2)print(Xt.shape)#在进行完多头并行计算后，得到多个头的数据，这个时候需要合并多个头，进行最后的线性映射。#(batch_size*num_head, 查询个数&amp;kv对数目，特征数目/num_head) 通过transpose_outputOutput=transpose_output(Xt, 2)print(Output.shape)结果：torch.Size([2, 3, 8])torch.Size([4, 3, 4])torch.Size([2, 3, 8]) 多头注意力 1234567891011121314151617181920212223class MultiHeadAttention(nn.Module): def __init__(self,key_size,query_size,value_size,num_hiddens,num_heads,dropout,bias=False,**kwargs): super(MultiHeadAttention,self).__init__(**kwargs) self.num_heads=num_heads self.attention=DotProductAttention(dropout) self.W_q=nn.Linear(query_size,num_hiddens,bias=False) self.W_k=nn.Linear(key_size,num_hiddens,bias=False) self.W_v=nn.Linear(value_size,num_hiddens,bias=False) self.W_o=nn.Linear(num_hiddens,num_hiddens,bias=False) def forward(self,queries,keys,values,valid_lens): queriesT=transpose_qkv(self.W_q(queries), self.num_heads) keysT=transpose_qkv(self.W_k(keys), self.num_heads) valuesT=transpose_qkv(self.W_v(values), self.num_heads) if valid_lens is not None: # 在轴0，将第一项（标量或者矢量）复制num_heads次， # 然后如此复制第二项，然后诸如此类。 valid_lens = torch.repeat_interleave( valid_lens, repeats=self.num_heads, dim=0) output=self.attention(queriesT,keysT,valuesT,valid_lens) output_concat=transpose_output(output, self.num_heads) return self.W_o(output_concat) 运行看看 12345678910111213num_hiddens, num_heads = 100, 5attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens, num_hiddens, num_heads, 0.5)attention.eval()batch_size, num_queries = 2, 4num_kvpairs, valid_lens = 6, torch.tensor([3, 2])X = torch.ones((batch_size, num_queries, num_hiddens))Y = torch.ones((batch_size, num_kvpairs, num_hiddens))attention(X, Y, Y, valid_lens).shape结果：torch.Size([2, 4, 100])","link":"/2024/04/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80/%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B/"},{"title":"GRU(门控循环单元)","text":"门控rnn是一个现代的rnn的变体。在rnn计算梯度的时候，往往出现了消失爆炸的情况。 1 早期观测值对预测所有未来观测值具有非常重要的意义，需要给早期值一个大梯度，这句话无法理解。2 情绪分析中，出现的h5之类的词元和观测无相关性，需要机制来跳过隐状态表示这类词元。3 中间出现连续中断，一本书中间的章节，还有牛市和熊市之间的中断，需要机制重置内部状态表示。 总结上述可能导致梯度的问题，和机制就是，1需要跳过无关词元机制2需要重置内部状态3需要给早期值一个大梯度 总共两个方式来解决，LSTM(长短记忆），门控循环单元(gate recurrent unit)GRU比较简单，它可以在观测不相关词元，则不更新隐状态，对应机制1， 也可以在必要的时候重置隐状态，对应机制2，对于机制3，可以在观测第一个词元后不更新隐状态，对应机制1。 对于这个机制，可以理解的是，需要通过两个小组件来决定是否要更新，是否需要重置？所以这里才命名成，更新门，和重置门。 这里可能需要放如何实现更新门，和重置门的公式。 简单的说，可以通过进化和退化，来说明重置门，和候选隐状态的关系。公式1再通过更新门来说明候选隐状态，和下一个真正的隐状态的关系。公式2 图 然后就是demo的实现了。","link":"/2024/04/09/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80/GRU(%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83)/"},{"title":"LSTM","text":"隐变量模型，隐藏变量存在一个问题，就是长期信息的保存，和短期输入的缺失。 长期信息的保存可以理解，一系列的序列预测后，最开始的序列信息早已经在模型里失去了。短期信息的输入缺失，这倒是难以理解了。设计灵感来自计算机的逻辑门，逻辑门。 对于信息的保存，lstm可能引入了记忆单元一样的小组件，然后对于这个记忆单元的使用，则设计了3个门来控制这个记忆单元。这三个门分别是，输出门，控制记忆单元输出，输入门，控制记忆单元的输入，遗忘门，重置单元的内容。 这三个门的公式和GRU的差不都都是公式1：Ot = activate(XtWxo + Ht-1Who + Bo)It = activate(XtWxi + Ht-1Whi + Bi)F = activate(XtWxf + Ht-1Whf + Bf)也就是Xt和Ht-1的输入，作为门的输入控制。 还有候选记忆单元C公式2Ct-=tanh(XtWxc + Ht-1Whc + Bc) 对于新的记忆实体公式3Ct = Ft O Ct-1 + It O Ct-其中这里的遗忘门控制着需要忘记多少过去的记忆，It则控制要接受多少新的记忆。 公式4Ht = Ot O Ct这里输出门发挥作用，就i是将有效的记忆传递给预测部分的隐状态，但是！输出门这里可以为0，或者接近0，这样的可以保留记忆信息，但是不需要更新隐状态。 图 后面则是实现的部分。not gonna implement it","link":"/2024/04/09/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80/LSTM/"},{"title":"多层rnn","text":"两个例子 GRU，LSTM，但是层数都没有什么增加。 在线性模型内，添加层数很简单，但是对于一个rnn如何添加层数。没有期待一个简单的单层rnn及其变种能够捕捉股市的非常复杂的信息。 以前是 X1-&gt;H1-&gt;O1, (X2,H1)-&gt;H2-&gt;O2, (X3,H2)-&gt;H3-&gt;O3现在是 X1-&gt;H1(1), H1(2) …H1(n)-&gt;O1, (X3, H1(1), H1(2), H1(3)…H1(n)) -&gt;H2(1), H2(2) … H2(n)-&gt;O3 公式：Ht(l) = activate( Ht(l-1)Wxh(l) + Ht-1(l)Whh(l) + Bh(l) )一个隐藏层，需要除了时间步t，还有层数l，输入来自H(t)(l-1），H(t-1)(l) 再torch里面：lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers) 这里可以指定num_hiddens超参数。 图","link":"/2024/04/09/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80/%E5%A4%9A%E5%B1%82rnn/"},{"title":"编码器-解码器架构","text":"一些编码器-解码器架构的思考 基本ai问题都可以归类为文字生文字（语言大模型），文字生图（midjourney），文字生视频（sora)。输入和输出都是长度可变的序列，机器翻译语种问题就是这个架构的一个简单的应用。 图？ 数学原理就是基本就是把输入序列映射为形状固定的编码状态S，然后映射成长度可变的序列。编码器应该就是最基本的一个继承自nn.module的具有forward方法的类解码器有点特殊，是将一个编码状态S，和前一个时间步生成的词元，映射成当前时间步的输出词元，所以初始化的时候需要把编码器的output state存起来 for example：S(I love you !) 【】 -&gt; Decoder -&gt; 【我】 , t1时间步S(I love you !) 【我】-&gt; Decoder -&gt; 【爱】，t2时间步S(I love you!) 【我爱】-&gt; Decoder -&gt; 【我爱你】, t3时间步S(I love you!) 【我爱你】-&gt; Decoder -&gt; 【我爱你 !】, t4时间步 Demo代码： 12345678910111213141516171819202122232425from torch import nnclass Encoder(nn.Module): def __init__(self, **kwargs): super(Encoder, self).__init__(**kwargs) def forward(self, X, *args): raise NotImplementedErrorclass Decoder(nn.Module): def __init__(self,**kwargs): super(Decoder, self).__init__(**kwargs) def init_state(self, enc_output, *args): raise NotImplementedError def forward(self, X, state): raise NotImplementedErrorclass EncoderDecoder(nn.Module): def __init__(self, encoder, decoder, **kwargs): super(EncoderDecoder, self).__init__(**kwargs) self.encoder=encoder self.decoder=decoder def forward(self, enc_X, dec_X, **kwargs): enc_output=self.encoder(enc_X,*args) dec_state=self.decoder.init_state(enc_output, *args) return self.decoder(dec_X, dec_state) 落地encoder实现，就是序列的开始是以特殊词元 bos，和结束词元 eos 来完成。 1 输入的X：示例：X = torch.zeros((4, 7), dtype=torch.long)可以理解为批量为4，长度为7的，数字组成的序列。【【3,4,5,1,5,7,8】【3,4,5,1,5,7,8】【3,4,5,1,5,7,8】【3,4,5,1,5,7,8】】 2 经给embedding的X_emb会变成示例：【【【1，2，3，0，5，6】【1，3，4，0，5，6】…】【…】【…】【…】】 3 经给 X_rnn = X = X_emb.permute(1, 0, 2)这里更换成形状 num_steps, batch_size, emb_size，因为始终要按照时间步 t1,t2,t3-&gt;…这样进行forward的，所以每个时间步可以处理多个不同批量的单个词元，这就是变换目的。说到底，rnn和其变种接受的X形状是(时间步，批量，词表大小） 4 output, state=self.rnn(X_rnn)这个时候就是rnn的表演了。由于state的一开始在rnn里面规定的形状就是nxh，由于layer是多层，所以有多个隐藏状态，所以给出的形状(layer_num, n, h)而output这里没有添加dens线性层去映射到vocab范围，所以也是nxh，由于每个时间步都有output，所以给出的形状是(n_steps, n, h) demo代码： 12345678910111213141516171819class Seq2SeqEncoder(d2l.Encoder): &quot;&quot;&quot;用于序列到序列学习的循环神经网络编码器&quot;&quot;&quot; def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs): super(Seq2SeqEncoder, self).__init__(**kwargs) # 嵌入层 self.embedding = nn.Embedding(vocab_size, embed_size) self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout=dropout) def forward(self, X, *args): # 输出'X'的形状：(batch_size,num_steps,embed_size) X = self.embedding(X) # 在循环神经网络模型中，第一个轴对应于时间步 X = X.permute(1, 0, 2) # 如果未提及状态，则默认为0 output, state = self.rnn(X) # output的形状:(num_steps,batch_size,num_hiddens) # state的形状:(num_layers,batch_size,num_hiddens) return output, state 1 上下文变量是如何来？通过取最后一个隐藏层(batch_size, num_hiddens)，即然后赋值n份，以匹配n的批量值。 2 怎么拼接？以下是模拟拼接的代码： 12345678state=torch.ones(2,4,3) #模拟一个state， num_layer=2, batch_size=4, hiddens=3 context=state[-1].repeat(5,1,1) #取最后一个layer，repeat steps次，变成(5,4,3) print(context) X=torch.rand(4,5,5) #本来是batch_size=4, steps=5, emb=5 X=X.permute(1,0,2) #变成(step=5,batch=4,emb=5) X_and_context = torch.cat((X, context), 2)#所以对于X变成steps次，和batch，然后emb，而隐藏层本来就是batch x h，最后复制steps次，然后就是emb+h print(X) X_and_context 可以认为隐藏层的nxh，对每个时间步都跟着X进去了。落地decoder的实现 123456789101112131415161718192021222324class Seq2SeqDecoder(d2l.Decoder):&quot;&quot;&quot;用于序列到序列学习的循环神经网络解码器&quot;&quot;&quot;def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,dropout=0, **kwargs):super(Seq2SeqDecoder, self).__init__(**kwargs)self.embedding = nn.Embedding(vocab_size, embed_size)self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,dropout=dropout)self.dense = nn.Linear(num_hiddens, vocab_size) def init_state(self, enc_outputs, *args): return enc_outputs[1] def forward(self, X, state): # 输出'X'的形状：(batch_size,num_steps,embed_size) X = self.embedding(X).permute(1, 0, 2) # 广播context，使其具有与X相同的num_steps context = state[-1].repeat(X.shape[0], 1, 1) X_and_context = torch.cat((X, context), 2) output, state = self.rnn(X_and_context, state) output = self.dense(output).permute(1, 0, 2) # output的形状:(batch_size,num_steps,vocab_size) # state的形状:(num_layers,batch_size,num_hiddens) return output, state 后面的loss构建，训练，和评估虽然很重要，但是我感觉现在我无法搞定。","link":"/2024/03/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80/%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E6%9E%B6%E6%9E%84/"},{"title":"数据及其分布的偏移","text":"这里说了很多我觉得是数据的偏移，然后给了许多例子，然后如何纠正，主要是关于清洗数据的一些思考。 interesting example， 一个穿牛津鞋的和一个穿梅杜斯邦特运动鞋的人，在放贷模型的训练集里面，会发生什么？ 分布偏移的类型:协变量偏移，标签偏移，还有概念偏移。还有大量的例子, 通常情况下大量的例子，会帮助更好了解这些偏移的背后的数学原理。 大学生的血液样本和病人。猫狗分类器以及tom猫和jerry鼠自动驾驶游戏仿真与真实路况广告模型不知道apple vision垃圾邮件spam，但是新型的垃圾邮件出现产品推荐，但是圣诞节过去了还在推荐圣诞帽子人脸分类器，但是对于人脸特写无法识别美国各州对软饮称呼的变化 再者有些有趣的学习方式也出现批量学习在线学习强化学习ect 还有提到的一个 反馈循环失控的问题。","link":"/2024/04/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E5%8F%8A%E5%88%86%E5%B8%83%E7%9A%84%E5%81%8F%E7%A7%BB/"},{"title":"编码器-解码器架构","text":"训练seq2seq代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#!/usr/bin/python# -*- coding: UTF-8 -*-import mathimport torchfrom torch import nnfrom d2l import torch as d2limport pandas as pdfrom DotProductAttention import *from EncoderDecoder import *from MultiHeadAttention import *from PositionalEncoding import *def sequence_mask(X, valid_len, value=0): &quot;&quot;&quot;在序列中屏蔽不相关的项&quot;&quot;&quot; maxlen = X.size(1) mask = torch.arange((maxlen), dtype=torch.float32, device=X.device)[None, :] &lt; valid_len[:, None] X[~mask] = value return X#@saveclass MaskedSoftmaxCELoss(nn.CrossEntropyLoss): &quot;&quot;&quot;带遮蔽的softmax交叉熵损失函数&quot;&quot;&quot; # pred的形状：(batch_size,num_steps,vocab_size) # label的形状：(batch_size,num_steps) # valid_len的形状：(batch_size,) def forward(self, pred, label, valid_len): weights = torch.ones_like(label) weights = sequence_mask(weights, valid_len) self.reduction='none' unweighted_loss = super(MaskedSoftmaxCELoss, self).forward( pred.permute(0, 2, 1), label) weighted_loss = (unweighted_loss * weights).mean(dim=1) return weighted_lossdef train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device): &quot;&quot;&quot;训练序列到序列模型&quot;&quot;&quot; def xavier_init_weights(m): if type(m) == nn.Linear: nn.init.xavier_uniform_(m.weight) if type(m) == nn.GRU: for param in m._flat_weights_names: if &quot;weight&quot; in param: nn.init.xavier_uniform_(m._parameters[param]) net.apply(xavier_init_weights) net.to(device) optimizer = torch.optim.Adam(net.parameters(), lr=lr) loss = MaskedSoftmaxCELoss() net.train() animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[10, num_epochs]) for epoch in range(num_epochs): timer = d2l.Timer() metric = d2l.Accumulator(2) # 训练损失总和，词元数量 for batch in data_iter: optimizer.zero_grad() X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch] bos = torch.tensor([tgt_vocab['&lt;bos&gt;']] * Y.shape[0], device=device).reshape(-1, 1) dec_input = torch.cat([bos, Y[:, :-1]], 1) # 强制教学 Y_hat, _ = net(X, dec_input, X_valid_len) l = loss(Y_hat, Y, Y_valid_len) l.sum().backward() # 损失函数的标量进行“反向传播” d2l.grad_clipping(net, 1) num_tokens = Y_valid_len.sum() optimizer.step() with torch.no_grad(): metric.add(l.sum(), num_tokens) if (epoch + 1) % 10 == 0: animator.add(epoch + 1, (metric[0] / metric[1],)) print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} ' f'tokens/sec on {str(device)}')","link":"/2024/04/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80/%E8%AE%AD%E7%BB%83seq2seq/"},{"title":"注意力评分函数","text":"加性注意力评分函数从上述例子中，知道a函数，注意力评分主要是用来衡量，query和key值得相似度。 加性注意力评分的公式：a(q , k) = tranpose( W(v) ) * tanh( W(q) * q + W(k) * k ) 本质上q是形状为 （batch_size, 查询次数，特征值）本质上k是形状为 (batch_size, kv对数，特征值） 123456789101112131415def masked_softmax(X, valid_lens): &quot;&quot;&quot;通过在最后一个轴上掩蔽元素来执行softmax操作&quot;&quot;&quot; # X:3D张量，valid_lens:1D或2D张量 if valid_lens is None: return nn.functional.softmax(X, dim=-1) else: shape = X.shape if valid_lens.dim() == 1: valid_lens = torch.repeat_interleave(valid_lens, shape[1]) else: valid_lens = valid_lens.reshape(-1) # 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0 X = d2l.sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6) return nn.functional.softmax(X.reshape(shape), dim=-1) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364class AdditiveAttention(nn.Module): &quot;&quot;&quot;加性注意力&quot;&quot;&quot; def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs): super(AdditiveAttention, self).__init__(**kwargs) self.W_k = nn.Linear(key_size, num_hiddens, bias=False) self.W_q = nn.Linear(query_size, num_hiddens, bias=False) self.w_v = nn.Linear(num_hiddens, 1, bias=False) self.dropout = nn.Dropout(dropout) def forward(self, queries, keys, values, valid_lens): print(&quot;1 进入的queries和keys的shape&quot;) # queries的形状：(batch_size，查询的个数，num_hidden) # key的形状：(batch_size，“键－值”对的个数，num_hiddens) print(queries.shape) print(keys.shape) # queries的形状：(batch_size，查询的个数，1，num_hidden) # key的形状：(batch_size，1，“键－值”对的个数，num_hiddens) queries, keys = self.W_q(queries), self.W_k(keys) print(&quot;2 经给linear后，queries和keys的size&quot;) print(queries.shape) print(keys.shape) ## 维度拓展： # queries的形状：(batch_size，查询的个数，1，num_hidden) # key的形状：(batch_size，1，“键－值”对的个数，num_hiddens) print(&quot;3 经给维度扩展，queries和keys的size&quot;) queries_unsq = queries.unsqueeze(2) keys_unsq = keys.unsqueeze(1) print(queries_unsq.shape) print(keys_unsq.shape) # 使用广播方式进行求和 features = queries_unsq + keys_unsq # features形状 print(&quot;4 queries和keys相加得到的features的size&quot;) print(features.shape) features = torch.tanh(features) print(&quot;5 激活features之后的size&quot;) print(features.shape) scores = self.w_v(features) print(&quot;6 经给wv之后的scores&quot;) print(scores.shape) scores = scores.squeeze(-1) print(&quot;7 squeeze(-1)之后的scores&quot;) print(scores.shape) self.attention_weights = masked_softmax(scores, valid_lens) print(&quot;8 masked_softmax之后的attention_weights&quot;) print(self.attention_weights.shape) res = torch.bmm(self.dropout(self.attention_weights), values) print(&quot;9 value的shape&quot;) print(values.shape) print(&quot;10 res的shape&quot;) print(res.shape) return res 123456789queries = torch.normal(0, 1, (2, 4, 20))keys = torch.ones((2, 10, 2))values = torch.arange(40, dtype=torch.float32).reshape(1, 10, 4).repeat(2, 1, 1)valid_lens = torch.tensor([2, 6])attention = AdditiveAttention(key_size=2, query_size=20, num_hiddens=8, dropout=0.1)attention.eval()attention(queries, keys, values, valid_lens) 得到的结果是 12345678910111213141516171819202122231 进入的queries和keys的shapetorch.Size([2, 4, 20])torch.Size([2, 10, 2])2 经给linear后，queries和keys的sizetorch.Size([2, 4, 8])torch.Size([2, 10, 8])3 经给维度扩展，queries和keys的sizetorch.Size([2, 4, 1, 8])torch.Size([2, 1, 10, 8])4 queries和keys相加得到的features的sizetorch.Size([2, 4, 10, 8])5 激活features之后的sizetorch.Size([2, 4, 10, 8])6 经给wv之后的scorestorch.Size([2, 4, 10, 1])7 squeeze(-1)之后的scorestorch.Size([2, 4, 10])8 masked_softmax之后的attention_weightstorch.Size([2, 4, 10])9 value的shapetorch.Size([2, 10, 4])10 res的shapetorch.Size([2, 4, 4]) 可以看到最后的atteion_weight是2, 4, 10，相当于两个批量，4个查询，10个kv对的权重。最后进行相乘，可以看到value的形状，（batch size, kv对的数目，特征数），其中kv对数目是需要事先确定好的。 本质上就是一个就是一个mlp，多层感知机。我也不知道为什么这样的mlp可以怎么衡量query和key的相似程度？只能认为是mlp具有学习功能，可以把相似的query和key联系起来。我也不知道的理解是对还是错的。 缩放点积注意力评分函数A点乘B的结果表示 A在B 方向上的投影与|B|的乘积，反映了两个向量在方向上的相似度，结果越大越相似。这种方式衡量相似度，非常科学，且非常的快速。 Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice,since it can be implemented using highly optimized matrix multiplication code意思就是缩放点积注意力评分函数比加性更快，更省内存。 缩放点积注意力公式：a(q, k) = ( transpose(q) * k ) / sqrt(d) 12345678910111213141516171819202122232425262728293031323334353637383940class DotProductAttention(nn.Module): &quot;&quot;&quot;缩放点积注意力&quot;&quot;&quot; def __init__(self, dropout, **kwargs): super(DotProductAttention, self).__init__(**kwargs) self.dropout = nn.Dropout(dropout) # queries的形状：(batch_size，查询的个数，d) # keys的形状：(batch_size，“键－值”对的个数，d) # values的形状：(batch_size，“键－值”对的个数，值的维度) # valid_lens的形状:(batch_size，)或者(batch_size，查询的个数) def forward(self, queries, keys, values, valid_lens=None): # queries的形状：(batch_size，查询的个数，num_hidden) # key的形状：(batch_size，“键－值”对的个数，num_hiddens) print(&quot;1 进入的queries和keys, value的shape&quot;) print(queries.shape) print(keys.shape) print(values.shape) d = queries.shape[-1] print(&quot;2 d的长度&quot;) print(d) keys_t = keys.transpose(1,2) print(&quot;3 transpose keys之后&quot;) print(keys_t.shape) # 设置transpose_b=True为了交换keys的最后两个维度 scores = torch.bmm(queries, keys_t) / math.sqrt(d) print(&quot;4 bmm之后的scores的shape&quot;) print(scores.shape) self.attention_weights = masked_softmax(scores, valid_lens) print(&quot;5 attention_weights的shape&quot;) print(self.attention_weights.shape) res = torch.bmm(self.dropout(self.attention_weights), values) print(&quot;6 res的shape&quot;) print(res.shape) return res 1234queries = torch.normal(0, 1, (2, 1, 2))attention = DotProductAttention(dropout=0.5)attention.eval()attention(queries, keys, values, valid_lens) 得到是一个res，( batch size, 查询次数，value特征个数），本质上就是得到了query个value 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293#!/usr/bin/python# -*- coding: UTF-8 -*-import mathimport torchfrom torch import nnfrom d2l import torch as d2limport pandas as pd# skipdef masked_softmax(X, valid_lens): &quot;&quot;&quot;通过在最后一个轴上掩蔽元素来执行softmax操作&quot;&quot;&quot; # X:3D张量，valid_lens:1D或2D张量 if valid_lens is None: return nn.functional.softmax(X, dim=-1) else: shape = X.shape if valid_lens.dim() == 1: valid_lens = torch.repeat_interleave(valid_lens, shape[1]) else: valid_lens = valid_lens.reshape(-1) # 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0 X = d2l.sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6) return nn.functional.softmax(X.reshape(shape), dim=-1)# ok，self attention，输入q,k,v（query数目，和kv数目一致，d维度也一样）# 输出得score是q对应每个v的特征加权和class DotProductAttention(nn.Module): &quot;&quot;&quot;缩放点积注意力&quot;&quot;&quot; def __init__(self, dropout, **kwargs): super(DotProductAttention, self).__init__(**kwargs) self.dropout = nn.Dropout(dropout) def forward(self, queries, keys, values, valid_lens=None): d = queries.shape[-1] # 设置transpose_b=True为了交换keys的最后两个维度 scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d) self.attention_weights = masked_softmax(scores, valid_lens) attention_weights_dropout = self.dropout(self.attention_weights) ## dropout 保持均值不变的情况下，把个别特征值设置为0 # 最关键是Q*K_T是什么意义？ # 设置1个batch，每个batch2个query，一个query有4个特征值, (1,2,4) # 设置1个batch，每个bacth有2个kv值，每个kv值由4个特征值, (1,2,4) # query: # q11,q12,q13,q14 # q21,q22,q23,q24 # key: # k11,k12,k13,k14 # k21,k22,k23,k24 # key_T # k11,k21 # k12,k22 # k13,k23 # k14,k24 # Q*K_T/sqrt(d) = (1,2,4) * (1,4,2) = (1,2,2), 也就是每个query对应的key的相似度 # a11=q11*k11 + q12*k12 + q13*k13 + q14*k14 q1和k1 # a12=q11*k21 + q12*k22 + q13*k23 + q14*k24 q1和k2 # a21=q21*k11 + q22*k12 + q23*k13 + q24*k14 q2和k1 # a22=q21*k21 + q22*k22 + q23*k23 + q24*k24 q2和k2 # a11，a12 # a21，a22 # a11是query1 对应 key1的相似度 # AT * Val = (1,2,2) * (1,2,4) = (1, 2, 4) # value： # v11, v12, v13, v14 # v21, v22, v23, v24 # a11*v11+a12*v21, a11*v12+a12*v22, a11*v13+a12*v23, a11*v14+a12*v24 # 解读： # score11 = q1和k1相似度*v1的特征1 + q1和k2相似度*v2的特征1 # score12 = q1和k1相似度*v1的特征2 + q1和k2相似度*v2的特征2 # score13 = q1和k1相似度*v1的特征3 + q1和k2相似度*v2的特征3 # score14 = q1和k1相似度*v1的特征4 + q1和k2相似度*v2的特征4 # a21*v11+a22*v21, a21*v12+a22*v22, a21*v13+a22*v23, a21*v14+a22*v24 # 解读： # score21 = q2和k1相似度*v1的特征1 + q2和k2相似度*v2的特征1 # score22 = q2和k1相似度*v1的特征2 + q2和k2相似度*v2的特征2 # score23 = q2和k1相似度*v1的特征3 + q2和k2相似度*v2的特征3 # score24 = q2和k1相似度*v1的特征4 + q2和k2相似度*v2的特征4 # 也就是一个score(i,j)第一个索引是代表query(i)，第二个索引是代表所有的value在j特征的加权和， res = torch.bmm(attention_weights_dropout, values) return res","link":"/2024/04/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80/%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%AF%84%E5%88%86%E5%87%BD%E6%95%B0/"},{"title":"数据集合下载和预处理","text":"12345678def read_data_nmt(): &quot;&quot;&quot;载入“英语－法语”数据集&quot;&quot;&quot; data_dir = d2l.download_extract('fra-eng') with open(os.path.join(data_dir, 'fra.txt'), 'r', encoding='utf-8') as f: return f.read() raw_text = read_data_nmt() print(raw_text[:75]) 预处理数据集：这里是很简单的，把多空格替换为一个空格，使用小写字母替代大写字母，再单词和标点符号插入空格 1234567891011121314def preprocess_nmt(text): &quot;&quot;&quot;预处理“英语－法语”数据集&quot;&quot;&quot; def no_space(char, prev_char): return char in set(',.!?') and prev_char != ' ' # 使用空格替换不间断空格 # 使用小写字母替换大写字母 text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower() # 在单词和标点符号之间插入空格 out = [' ' + char if i &gt; 0 and no_space(char, text[i - 1]) else char for i, char in enumerate(text)] return ''.join(out) text = preprocess_nmt(raw_text) print(text[:80]) 然后词元化，tokenize？ 123456789101112131415#@savedef tokenize_nmt(text, num_examples=None): &quot;&quot;&quot;词元化“英语－法语”数据数据集&quot;&quot;&quot; source, target = [], [] for i, line in enumerate(text.split('\\n')): if num_examples and i &gt; num_examples: break parts = line.split('\\t') if len(parts) == 2: source.append(parts[0].split(' ')) target.append(parts[1].split(' ')) return source, target source, target = tokenize_nmt(text) source[:6], target[:6] 这个tokenizer函数就是最简单通过空格把单词分成一个一个token 词表：数据集合里所有的唯一单词构成了词表。 加载数据集?对于不够时间步的使用词表里的pad填充，多出的则使用截断。当出现一批量的张量对子input, output就是数据集合准备完成了。","link":"/2024/04/09/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E4%B8%8B%E8%BD%BD%E5%92%8C%E9%A2%84%E5%A4%84%E7%90%86/"}],"tags":[{"name":"c++","slug":"c","link":"/tags/c/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"Language(c++)","slug":"Language-c","link":"/tags/Language-c/"},{"name":"unfinish","slug":"unfinish","link":"/tags/unfinish/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"操作系统","slug":"操作系统","link":"/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"内存","slug":"内存","link":"/tags/%E5%86%85%E5%AD%98/"},{"name":"ai","slug":"ai","link":"/tags/ai/"},{"name":"llm","slug":"llm","link":"/tags/llm/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"向量数据库","slug":"向量数据库","link":"/tags/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"数据库","slug":"数据库","link":"/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"agent","slug":"agent","link":"/tags/agent/"},{"name":"dify","slug":"dify","link":"/tags/dify/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"数据","slug":"数据","link":"/tags/%E6%95%B0%E6%8D%AE/"}],"categories":[],"pages":[]}